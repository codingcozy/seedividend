---
title: "RAG 모델 구축하기  Databrick으로 쉽게 만들기"
description: ""
coverImage: "/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png"
date: 2024-05-15 15:48
ogImage: 
  url: /assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png
tag: Tech
originalTitle: "RAG your model — Simplified with Databrick"
link: "https://medium.com/@yashodhannn/rag-your-model-simplified-with-databricks-9c8daa02ddd7"
---


# RAG 이론 (간단 버전)

언어 모델이 지식을 배우는 몇 가지 방법이 있습니다. 전통적으로는 모델을 처음부터 훈련하거나 기존 모델을 세밀 조정하는 방법이 있습니다. 이는 모델 가중치를 업데이트하여 모델을 더 훈련시키는 것을 의미합니다. 다른 방법은 상대적으로 새롭고 직접적으로 프롬프트 공학과 연관이 있습니다. 여기서는 지식을 모델 입력값으로 전달합니다. 모델은 이를 문맥으로 편입하여 지식을 통합합니다. 왜 모델에 처음부터 문맥을 전달하는 걸까요? 모델에게 문맥을 전달하는 것은 모델에게 오픈 노트로 시험을 보는 것과 같습니다. 모델은 참고할 사실을 손에 넣게 됩니다. 다만 전달할 수 있는 문맥의 크기에는 제한이 있습니다. 보통 5 페이지이며, 이는 대부분의 산업용 사례에 부족할 정도입니다. 그래서 모델 문맥 문제용 새로운 모델 구조 - 'RAG'가 등장했습니다!

# RAG 아키텍처—

Context 데이터/지식을 임베드된 벡터로 변환하고 이를 저장하는 모델(임베딩 모델)을 추가(Vector 스토어). 쿼리(프롬프트)가 전달되면 프롬프트를 캡처하여 임베드된 벡터로 변환하고, 저장소에서 유사한 벡터를 찾아 메인 모델에게 컨텍스트 강화된 프롬프트(쿼리 벡터 + 저장소로부터의 유사 벡터)를 전달합니다. 이것이 여러분이 제공한 문맥을 통합하여 정확한 응답을 생성하는 RAG입니다.



아래는 Markdown 형식으로 변경된 표입니다.


![이미지](/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png)

# RAG의 세부 내용:

RAG는 다음과 같은 네 가지 주요 구성 요소로 구성됩니다:-

1. 벡터 검색 - 주의할 점은 우리가 아무런 검색을 하기 전에 문맥/지식을 임베딩 벡터로 변환한다는 것입니다. 모든 데이터 객체인 오디오, 비디오 또는 텍스트는 임베드 벡터로 변환되어 벡터 저장소에 저장될 수 있습니다.
  



두 가지 종류의 검색이 있습니다. 정확한 검색과 근사 검색. 이름에서 알 수 있듯이 정확한 검색은 가장 가까운 방법을 찾는 무차별 대입 방법입니다. 전통적인 KNN과 유사합니다. 반면에 근사 검색은 가장 가까운 이웃을 찾는 데 정확도가 낮지만 속도가 빠릅니다. 가장 인기 있는 벡터 검색 알고리즘은 ANN을 사용합니다.

공통의 인덱싱 알고리즘은 몇 가지 있습니다 — Spotify의 ANNOY(트리 기반), Facebook의 FAISS(클러스터링), LSH(해싱), 그리고 Google의 SCaNN(벡터 압축). 이러한 알고리즘들은 모두 효율적인 검색을 수행하기 위한 모든 필요한 정보를 보유한 vector Index라는 데이터 구조를 반환합니다.

2. Vector Store: 벡터를 저장하는 데 사용하는 두 가지 전략이 있습니다. 하나는 가벼운 벡터 라이브러리를 사용하는 것이고, 다른 하나는 고급 기능을 제공하는 벡터 데이터베이스를 사용하는 것입니다.

- 벡터 라이브러리: 벡터 인덱스를 생성하고 이 인덱스는 메모리에 저장됩니다. 라이브러리는 일반적으로 작고 정적인 데이터에 대해 충분합니다. 저장된 벡터에 대한 CRUD 작업을 지원하지 않습니다. 즉, 데이터가 변경될 때마다 인덱스를 다시 만들어야 합니다. 데이터 복제가 없습니다.



b. 벡터 데이터베이스 — 다른 한편으로는 구조화되지 않은 데이터를 저장하는 데 특화된 데이터베이스입니다. 데이터베이스의 CRUD 속성을 상속하며 오프라인에서 색인을 사전 처리하고 나서 벡터를 데이터베이스에 저장하여 모델에 온라인으로 제공할 수 있도록 합니다.

3. 필터링 — 필터링은 생성 프로세스에 통합하기 전에 검색된 지식 베이스에서 관련 정보를 선택하고 우선순위를 정하는 메커니즘을 참조합니다. 이를 통해 검색된 컨텍스트가 정보를 제공할 뿐만 아니라 일관성 있고 맥락에 부합하는 응답을 생성하는 데 도움이 됩니다.

세 가지 전략 — Pre-Query, In-Query, Post-Query 필터링.

4. 프롬프트 엔지니어링 — LLM에 응답 생성이나 작업 완료를 요청하는 텍스트입니다. 원하는 출력을 생성하도록 모델을 안내하는 명확하고 구체적이며 잘 구조화된 입력을 작성하는 것을 포함합니다. 모델이 사실을 찾지 못했을 때 억지로 내용을 만들지 않도록 안내하는 지침을 제공하는 것, 예시와 데모 사용, 그리고 더 중요한 것은 작업을 더 잘 이해할 수 있도록 맥락을 제공하는 것을 모두 포함합니다. 위에서 설명한 RAG 워크플로우에 주목하세요. 마지막으로, 우리는 맥락 문서를 프롬프트를 통해 모델에 전달하고 있기 때문에 RAG가 프롬프트 엔지니어링과 밀접하게 연관되어 있다고 할 수 있습니다.



# RAG Nvidia/Llama3 model with Databricks

## Assets -

Model - nvidia/Llama3-ChatQA-1.5-8B ([링크](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B))

Vector Database - ChromaDb



Databricks ML 런타임 -13.3.x-cpu-ml-scala2.12 / 13.3.x-gpu-ml-scala2.12

이 두 Databricks 런타임은 호환됩니다. 그러나이 모델은 8B 매개변수를 가지고 있어 노트북에서 추론을 위해 로드하기에 너무 큽니다. 충분한 메모리와 코어가 있는 CPU 머신에서 응답을 생성하는 데 오랜 시간이 걸렸습니다. 당신의 요구에 적합하고 예산에 맞는 런타임을 선택하세요.

## 데모 -

감사합니다.