---
title: "ChatGPT-4o, 오픈AI의 새로운 주력 모델 전체 리뷰"
description: ""
coverImage: "/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_0.png"
date: 2024-05-15 11:35
ogImage: 
  url: /assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_0.png
tag: Tech
originalTitle: "ChatGPT-4o, OpenAI’s New Flagship Model’s Full Review"
link: "https://medium.com/@ignacio.de.gregorio.noblejas/chatgpt-4o-openais-new-flagship-model-s-full-review-6d7ac44f49c9"
---


일년 만에 OpenAI의 새로운 모델, 그들의 트랜스포머 패밀리의 최신 버전인 GPT-4o ("omnimodal")가 드디어 나왔어요.

이 모델은 텍스트, 오디오, 이미지, 비디오 처리 및 이미지 생성에서 엄청나게 빠르며, 코딩 및 멀티모달 추론 개선을 보여주고 있어요. 또한 3D 렌더링과 같은 새로운 형태의 모달리티를 가능하게 합니다.

그리고 lmsys.org의 챗봇 아레나에 따르면, 약 두 주 전에 우리가 논의한 유명한 gpt2-chatbot의 프록시 모델로부터 얻은 결과를 기반으로 이미 최고의 올라운드 모델이라고 해요.

하지만 이번에 모델을 공개한 이유는 Sam Altman이 말한 대로 "무지의 가리개를 밀어내는 것"이 아니라, 최첨단 인공지능을 수십억 명의 손에 무료로 전달하는 것에 있어요.



여기 ChatGPT-4o에 대해 알아야 할 모든 정보가 있어요.

# 다중성의 저주

다중 모닥과 대형 언어 모델, 즉 MLLM은 상당한 시간 동안 존재해 왔지만, GPT-4o는 오디오, 비디오, 이미지 및 텍스트 네 가지 다른 모드에 걸친 진정한 다중 모달리티를 보여주는 첫 번째 모델로 보입니다.

- 네, Gemini 1.5와 같은 모델은 후자 세 가지에 대해 진정으로 다중 모단에 보였지만, 오디오에 대해서는 아니었어요.
- 실제로, GPT-4V는 오디오 처리/생성 및 이미지 생성을 허용했지만, Whisper, OpenAI TTO 및 Dall-e3와 별도의 모델들과 통합하여 해당 기능들을 가능하게 했어요.



안녕하세요! ChatGPT-4o는 모든 모달리티에 기본적으로 작동하는 하나의 단일 모델을 의미하는 올인원 모델입니다.

그런데 이게 무슨 의미일까요?

## 다중 모달리티 입력, 다중 모달리티 출력

이에 대해 더 자세히 이야기할 것이며, 이번 주 목요일에 무료 뉴스레터를 통해 더 많은 내용을 알려드리겠습니다 (위 참조).

ChatGPT-4o는 이제 더 이상 "그저 큰 언어 모델"이 아니라는 아이디어입니다.



대형 언어 모델(LLMs)은 시퀀스 대 시퀀스 모델입니다(입력과 출력이 모두 시퀀스인 모델). 보통 텍스트를 입력으로 받아 다른 텍스트를 출력합니다.

이 이미지 인코더와 같은 구성 요소와 결합하면 이미지도 처리할 수 있으며, 다른 모달리티에도 동일하게 적용됩니다.

그러나 많은 경우에 이러한 구성 요소는 내적 요소가 아닙니다. 따라서 LLM은 입력을 사용하여 다른 데이터 유형을 처리할 수는 있지만 cross-modal 추론을 수행할 수는 없습니다.

그렇다면 그것이 무슨 의미일까요?



민아 무라티가 공식 발표에서 강조한 대로, 말은 단어 이상의 것을 포함합니다. 톤, 감정, 일시정지 및 다른 여러 단서들도 포함되어 발언자가 무엇을 전달하려는지에 대한 추가 정보를 전달합니다.

예를 들어, "널 죽일 거야!"라는 문장은 말하는 사람이 명백한 의도를 보여주거나 문장 중간에 웃음을 보여주는 경우 매우 다른 해석을 가질 수 있습니다.

그러나 지금까지 ChatGPT의 이전 버전이 실제로 수신한 것은 단지 음성 전사뿐이었기 때문에 다른 모든 단서들이 손실되었습니다. 따라서 모델은 음성을 해석할 때 매우 제한되었으며 이전의 두 가지 예제는 모두 그것과 동일했습니다.

그러나 이제 ChatGPT-4o는 텍스트, 이미지, 오디오 및 비디오 (비디오 생성 제외)를 처리하고 생성하는 데 필요한 모든 구성 요소를 포함하고 있습니다. 다시 말해, GPT-4o는 첫 번째 모델로, 인간처럼 모든 모달리티와 그 이유를 결합합니다.



그것을 알았다면, 어제 새롭고 흥미로운 능력이 제시되었나요?

## 다재다능한 "야수"

두 시간반의 짧은 발표에도 불구하고, 언급할 가치가 있는 많은 것이 보여졌습니다.

사실, ChatGPT-4o에는 수십억 명이 사용하는 제품에서 수십억 명이 사용하는 제품으로 변화시키는 데 필요한 많은 특성이 있습니다.



## 인상적인 쇼케이스

첫 번째로, 내가 본 중에서 가장 인상적인 두 가지 중 하나인 것은 ChatGPT가 실시간 비디오 인식을 수행한다는 것입니다. 구글은 Gemini이 그랬지만 실제로 그렇지 않았습니다.

다른 비디오에서, OpenAI의 X 관객 중 한 사람이 실시간 번역을 제안했고, ChatGPT-4o는 다른 큰 개선 사항이 있어서 완벽하게 실행되었습니다: 인간 수준의 대기 시간.

ChatGPT-4o와 같은 음성 비서가 사회에 미칠 수 있는 흥미로운 사용 사례 중 하나는 교육입니다. 항상 인내심 있는 AI 모델은 학생들이 복잡한 작업을 배울 때 도움이 될 수 있습니다.



메모리는 비디오 시연 중에 감지되지 않은 매우 흥미로운 기능이었습니다. 아래 비디오에서 OpenAI의 대표이사 그렉 브록먼이 모델이 처음에는 무시하는 비디오 프레임의 '침입자'를 가지고 있습니다.

그러나 그렉이 이에 반응하도록 모델에 요청하자, 모델은 이전에 발생한 정확한 상호 작용을 다시 호출합니다. 이는 두 가지를 의미합니다:

- 모델은 이전 이벤트를 기억할 수 있는 것으로 보입니다.
- 모델이 특정 작업에 집중하고 나머지를 무시하는 메커니즘이 있는 것으로 보입니다. 이는 OpenAI가 계층화된 주의를 가진 매우 능률적인 비디오 인코딩 메커니즘을 개발했을 수 있다는 것을 의미할 수 있습니다.

물론, 이 모든 것에 대해 X는 열광했고, 매우 흥미로운 스레드가 나타났습니다. 아마도 가장 인상적으로 느꼈던 것 중 하나는 OpenAI의 윌 데퓨가 제작한 것인데, 이것은 많은 예제를 보여주어 GPT-4o의 기본 다중 모달성을 증명했습니다.



모델은 제어 넷 유형 이미지 조건화가 없어도 여러 세대에 걸쳐 문자 일관성을 유지하는 것으로 보입니다:

![이미지1](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_0.png)

모델은 사진을 가져와 대체 3D 뷰를 생성하고 이를 실제 3D 렌더링으로 조립할 수도 있습니다.

![이미지2](https://miro.medium.com/v2/resize:fit:1000/1*KTEVeb4ty--ihrJ-HiGnYg.gif)



시위와 관계없이, 해당 모델은 특히 새로운 벤치마킹의 왕이 되었습니다.

## 더 똑똑해졌지만 AGI는 아님

이전부터 의심되었던 바와 같이, lmsys.org의 X 페이지와 OpenAI 연구원들의 확인에 따르면, 'im-also-a-good-gpt2-chatbot'인 'gpt2-chatbot' 라인업의 일원이 사실 ChatGPT-4o였습니다.

전자가 공유한 이미지에서 gpt2-chatbots 또는 GPT-4o 챗봇은 GPT-4 및 Claude 3 Opus 모델에 비해 전반적인 ELO(품질 측정) 면에서 앞서 있습니다.



![image](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_1.png)

또 다른 뚜렷한 개선 사항은 코딩에서 볼 수 있습니다. 개선 정도가 미친 듯이 100 ELO 점수가 상승했어요. 참고로, 두 모델 간의 100 점 차이는 패배 모델이 선호되는 경우가 1/3만 된다는 걸 의미합니다.

특히 코딩에 대해 이야기하자면, 가장 주목할 만 한 공지 사항 중 하나는 ChatGPT 데스크톱 앱이었어요. 이 앱은 디버깅과 같은 작업에서 모델을 완전한 노트북 화면에서 활용할 수 있게 도와줄 겁니다. 이 비디오에서 확인할 수 있어요.

그리고 이 공지에는 강력한 언어 개선도 포함되어 있었습니다.



## 전 세계 인구의 97%가 서비스되었습니다

제 모델의 토크나이저를 크게 개선했을 것 같아요, 특히 비영어권 언어를 고려할 때 (전 세계 인구의 97%까지 제공할 수 있다고 주장합니다. 상당한 주장이네요).

이를 증명하기 위해, 해당 모델이 언어당 토큰을 상당히 줄였다고 주장하는 테이블을 공개했습니다.

![이미지](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_2.png)



압축이 왜 중요한지 궁금하신가요? 그들이 주장하는 바는 빠르고 더 효율적인 버전 뿐만 아니라 보다 뛰어난 "언어 지능"을 보여줍니다. 간단히 말해, 언어가 가지는 토큰이 적을수록 모델이 언어를 생성하는 방법을 더 잘 알게 됩니다.

하지만 ChatGPT-4가 정말 그만큼 우수하고 지능적인 발전이라고 할 수 있을까요? 

음, 아닙니다.



## 차분해지세요, AGI가 아닙니다

OpenAI가 공유한 그래프에 따르면, 이 모델은 현재 가장 우수한 것으로 분명하지만, 나머지에 비해 지능적인 개선은 미미합니다.

![이미지](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_3.png)

'지능' 개선을 보면, 본 릴리스는 소박해 보일 수 있습니다. 그러나 저는 완전히 동의하지 않습니다. 왜냐하면 이 릴리스는 다음 중요한 새로운 영역에 대한 것이 아니라 다른 어떤 것에 대한 것이었기 때문입니다.



그러나 모델이 더 똑똑해지지 않았다면 그 의미가 뭡니까?

# OpenAI의 진짜 의도

내가 보기에, 이 릴리스에는 세 가지 구성 요소가 있습니다:

- 다음 프론티어인 이른바 'GPT-5'의 대규모 릴리스를 위한 시간 확보
- 오늘 열리는 Google의 I/O 컨퍼런스를 사전에 저질러놓기
- Apple 승리



하나씩 해보자.

## 다가오는 다음 단계는 가깝지만 완전하게는 아니다

OpenAI의 CTO 미나 무라티는 공개적으로 이 점에 대해 언급했습니다. GPT-4o는 지능적인 크게 발전이 있는 것이 아니며, 사실 그들은 명시적으로 "GPT-4 수준의 지능"이라고 밝혔습니다.

또한 그들은 곧 '다가오는' 다음 단계에 대한 소식과 업데이트를 받게 될 것이며, 그것에 어떤 이름을 붙일지에 대해 언급했습니다.



## 구글의 최악의 악몽

이 시점에서 OpenAI가 무엇을 공개할지 예측하고 싶다면, 그냥 구글이 무엇을 할지 보세요.

예를 들어, 구글이 Gemini 1.5를 위해 백만 컨텍스트 윈도우를 출시하면서 MLLM이 한 번에 처리할 수 있는 데이터 양을 크게 늘린 것을 보면, OpenAI는 전혀 다른 이야기로 넘어가서 Sora라는 비디오 생성 모델을 출시했습니다.

오늘 고대로 예상되는 Google I/O 컨퍼런스가 온라인으로 열리기 전에, OpenAI는 그보다 하루 전에 자체 컨퍼런스를 열어 전자에 대한 분석가들에 대한 기대치를 매우 높였습니다.



간단히 말하면, 이제 구글이 새로운 AI 기능을 제공하는 것이 아닙니다. OpenAI의 발표에 기반해 구글이 어떻게 응답하는지 확인해보는 경우가 되었습니다.

그리고 마지막으로, 현재 경매 중인 Siri 왕관을 가져가기 위해 구글과 OpenAI 간에 매우 소문난 '전투'를 고려할 때, 우리는 애플에 대해 이야기해야 합니다.

## 이러한 목표가 계속되는 이유?

애플과의 협업의 잠재적 이익을 고려할 때, Siri 계약을 따내는 것이 OpenAI의 목표였을 수도 있습니다.



지연 시간이 뛰어나고 애정 어린 음성 행동, 여러 데이터 유형에서의 뛰어난 기능, 그리고 중요하게도 좋은 화면 시각 기능을 자랑하는 OpenAI가 프리미티브 시리를 개선하기 위해 Apple과 파트너십을 맺고 싶어하는 것은 비밀이 아닙니다.

사실, Apple이 놀라운 기기 내 모델을 내놓지 않는 이상, 사용자들은 무관심하게 될 것이며 즉시 최첨단 기술과 비교할 것입니다.

결국, 이는 분명히 Apple에게는 좋지 않은 PR 전망이지만, 자본이 풍부하여 자본주주 매입으로 자본주가 역사상 최대의 1310억 달러를 보유한 회사인 Apple은 여전히 시리를 해결하지 못했습니다. Apple은 이에 대한 여지가 거의 없습니다.

그러므로 내부 'AI 난국'을 정리하고 좋은 AI 제품을 제공하기 시작할 때까지, GPT-4o(또는 Google이 오늘 자랑하는 것)에 베팅하고자 하는 유혹이 높아질 것입니다.



그렇다면, 그 제휴가 어떻게 구체화될지 알기 위해 추측으로 나아가야 할 것입니다.

애플은 사용자의 개인정보 보호에 매우 열정적이라고 알려져 있습니다. 이는 명백히 저작권과 보안 규정을 위반하여 모델을 훈련시킨 회사로부터 Siri를 위한 클라우드 기반 LLM 솔루션을 갖는 것과 호환되지 않는 것으로 보입니다.

그러나 윤리가 돈에 방해를 받을 때, 기업들이 어떤 선택을 하는지 우리는 압니다. 의심스러운 윤리적 측면을 떠나서라도 돈이 관련된 상황에서 기업들이 선택하는 것을 말이죠.

## 수십억 달러로부터 수백억 달러까지



모든 것을 종합하면, OpenAI는 항상 실망시키지 않습니다. 그러나 이번에는 과거와는 다르게 그들의 의도가 명확하지 않을 수도 있습니다.

GenAI 제품은 약속을 지키지 못하는 것으로 알려져 있으며, 이는 ChatGPT와 같은 경우에도 사실입니다. 지연 시간과 부족한 교차 모달 추론 등 여러 가지 이유로 이러한 상황이 발생합니다.

지금 OpenAI는 인터넷 이후로 가장 큰 발견로 여겨지는 기대치에 부응하는 AI를 마침내 제공하는 제품을 갖고 있다고 생각합니다.

이게 바로 그런 경우인지 여전히 이른 것 같지만, 이러한 가능성은 구글을 겁나게 할 민감한 지점을 만들고, AI의 다음 지평으로 자신들의 가장 큰 릴리스에 충분한 시간을 확보할 기회를 제공합니다.



그러나 GPT-4o는 여전히 제한이 있으며 GPT-4보다 AGI에 더 가까워지는 것이 아니라는 점에서 명백합니다.

하지만 이 제품은 강력한 AI를 널리 이용할 수 있게 함으로써 Generative AI를 사회에 훨씬 가깝게 만들어줍니다(제품은 무료로 제공될 예정임). 실제로 수십억 명에게까지 Siri를 통해 제공된다면 AI가 약속한 것을 실현하는 데 필요한 것이 바로 이런 접근방법이죠.