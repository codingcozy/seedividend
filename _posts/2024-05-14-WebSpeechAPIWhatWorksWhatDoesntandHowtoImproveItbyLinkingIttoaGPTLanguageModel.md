---
title: "웹 음성 API 무엇이 동작하고, 무엇이 동작하지 않으며, GPT 언어 모델과 연결하여 향상시키는 방법"
description: ""
coverImage: "/assets/img/2024-05-14-WebSpeechAPIWhatWorksWhatDoesntandHowtoImproveItbyLinkingIttoaGPTLanguageModel_0.png"
date: 2024-05-14 13:30
ogImage: 
  url: /assets/img/2024-05-14-WebSpeechAPIWhatWorksWhatDoesntandHowtoImproveItbyLinkingIttoaGPTLanguageModel_0.png
tag: Tech
originalTitle: "Web Speech API: What Works, What Doesn’t, and How to Improve It by Linking It to a GPT Language Model"
link: "https://medium.com/towards-data-science/web-speech-api-what-works-what-doesnt-and-how-to-improve-it-by-linking-it-to-a-gpt-language-dc1afde54ced"
---


![웹 스피치 API가 작동하는 방식이 무엇이고, 어떻게 개선할 수 있는지에 대한 리스트와 GPT 언어 모델과 연결하는 방법](/assets/img/2024-05-14-WebSpeechAPIWhatWorksWhatDoesntandHowtoImproveItbyLinkingIttoaGPTLanguageModel_0.png)

현대 기술들은 현재 소프트웨어가 제안하는 것보다 훨씬 간단하고 자연스러운 인간-컴퓨터 상호작용을 가능케 한다는 생각을 가지고 있어요. 사실, 저는 기술들이 충분히 성숙해져서 전통적인 인터페이스 없이도 사용자 경험에 혁명을 가져갈 수 있다고 생각해요.

대형 언어 모델은 특히 정보 요청하는 방식에서 이러한 혁명의 한 단계를 촉발했습니다. 하지만, 제 생각에는 기술들이 더 많은 것을 제공할 수 있다고 봐요. 예를 들어, VR 헤드셋 비용이 점차 낮아지더라도 아직도 평평한 화면에 크게 의존하고 있고; 눈금, 음성 인식, 신체 부위 추적과 같은 기술들이 발전함에도 마우스, 키보드 및 터치 동작을 사용하고 있고; 음성 합성이 크게 발전했음에도 여전히 많은 것을 읽는 것에 그친 채 있어요.

본 기사를 통해 저는 이미 잘 작동하는 현대 기술들로 인해 인간-컴퓨터 상호작용이 영원히 변할 수 있다는 것에 헌신하는 짧은 시리즈를 시작하려고 해요. 제가 공유할 코드 조각과 예시 앱을 통해 직접 테스트해보실 수 있을 거예요.



저는 내 스타일을 유지하며, 이 모든 현대 기술의 웹 기반 구현에 대해 구체적으로 이야기하겠습니다. 그리고 여기서 웹 브라우저에 통합된 웹 음성 API로 시작하여 그 강점을 논의하고 일부 사용 사례를 보여주며 한계를 강조하고 일부 한계를 커버하는 방법에 대해 구체적으로 설명해 보겠습니다.

# (현대) 음성 인식과 음성 합성

특히 두 가지 기술, 음성 인식과 음성 합성은 거의 20년 동안 존재해 오며 더 자연스러운 인간과 컴퓨터 간 상호 작용을 도와줄 많은 것을 제공합니다.

음성 인식 또는 ASR(Automatic Speech Recognition) 또는 STT(Speech-To-Text)는 구어를 쓰여진 텍스트로 변환합니다. 주요 넓은 적용 분야가 두 개 있습니다:



- 명령어를 자연스럽게 발화하여 스마트폰이나 컴퓨터와 같은 하드웨어를 제어합니다. 스마트폰이나 Alexa 또는 Siri를 음성 명령을 통해 사용하는 방식을 생각해보세요.
- 대화 내용을 필사하고(그리고 아마도 그 후에 표시하거나 저장하거나 분석함)하는 작업입니다. 회의 필사, 비디오 자막 등을 생각해보세요.

두 응용 프로그램은 이 블로그 글의 주제와 직접적으로 관련이 있으며, 웹 프로그래머를 위해 Web Speech API를 통해 구현되었습니다.

음성 합성 또는 TTS(텍스트를 음성으로 변환)는 쓰여진 텍스트를 말로 변환합니다. 이를 통해 컴퓨터 및 다른 장치가 인간과 유사한 말을 생성하여 정보를 청각적으로 액세스할 수 있게 되었습니다. 여기 Medium 기사에서 보는 것처럼 음성으로도 정보를 듣을 수 있게 해줍니다.

음성 합성은 시각 장애인을 위한 접근성 기능, 대화형 음성 응답 시스템 및 멀티미디어 콘텐츠 보강을 포함한 다양한 시나리오에 적용됩니다.



STT와 TTS를 함께 사용하면 디지털 콘텐츠와의 상호작용이 더 포괄적이고 다양해지며, 무손실 및 화면 없이 작동하는 기회와 맞춤형 사용자 경험을 열어줍니다.

## 음성 인식 및 합성 기술의 역사 개관

음성 인식과 합성 기술은 특히 최신 AI 모델의 통합과 기계 학습 기술의 발전으로 지속적인 발전을 이루었습니다. 초기 시도가 컴퓨터를 언어 처리에 사용하려는 20세기 중반으로 거슬러 올라갑니다. 초기 모델은 발음, 방언, 동음이의어 및 언어적 미묘함에 대한 도전을 겪었으며, 음성 생성 및 이해 모두에서 직면했습니다. 통계 모델과 상징적 자연 언어 처리의 발전으로 ASR 시스템이 점차 향상되었지만, 2010년대 후반에 트랜스포머의 등장으로 ASR에서 특히 기반을 둔 획기적인 발전이 이루어졌습니다. 

STT는 TTS보다 변수가 많아 다소 복잡하지만, 가장 현대적인 방법은 매우 잘 작동합니다. 이에 대해 자세히 다루지는 않겠습니다. 현대 STT는 여러 단계와 AI 모델이 협력하는 복잡한 프로세스를 포함합니다. 주요 단계는 오디오 입력의 전처리, 특징 추출, 음소 추출, 언어 모델 의사 결정 및 단어 시퀀스로의 디코딩이 포함됩니다. 현대 ASR 모델은 종종 모든 단계에서 트랜스포머를 사용하여 정보의 장거리 결합을 보존하여 정확도와 일관성을 향상시킵니다. 가장 선진적인 ASR 시스템에는 핵심 모듈에 직접 내장된 언어 모델 요소가 포함되어 있습니다. 이러한 내장 통합은 높은 전사 및 인식 정확도에 중요한 역할을 하였습니다.



현대 ASR 및 TTS 기술은 실제로 단순한 음성인식 또는 합성 이상으로 한 걸음 더 나아가 "말하는 언어의 이해"에 근접할 수 있습니다. 가장 고급 스피치 투 텍스트(STT) 및 텍스트 투 스피치(TTS) 모델은 여러 언어로 오디오를 텍스트로 전사하거나 텍스트에서 오디오를 합성할 수 있으며, 일부는 언어를 자동으로 식별하고 맥락에 적응하며, 다양한 화자를 감지하거나 시뮬레이션할 수 있으며, 전사된 단어에 타임스탬프를 달거나 구두점 및 비언어적 발화를 처리하며, 사용자 정의 사전을 허용하는 등의 기능도 제공합니다.

웹 Speech API를 통해 웹 앱에서 이러한 기능을 활용할 수 있는 가능성에 흥분한다면, 알아야 할 몇 가지 제약 사항이 있습니다. 브라우저가 지원하는 것이 최첨단 기술이 아니기 때문에 조금 실망할 수도 있습니다. 다행히도 Web Speech API를 대규모 언어 모델과 결합하면 이러한 문제 일부를 완화할 수 있으며, 많은 현대 ASR 및 TTS 기술이 수행하는 것처럼 GPT-3.5-turbo 또는 GPT-4를 프로그래밍적으로 사용하여 여기에서 보여드리겠습니다. 그것만으로 충분하지 않다면, 최첨단에서 ASR 및 TTS를 수행하기 위해 자사의 (유료) API를 제공하는 회사들도 있음을 알게 될 것입니다.

# Web Speech API, 그 구성 요소 및 이용 가능성

Web Speech API는 웹 애플리케이션이 음성 데이터를 기능에 통합할 수 있도록 하는 웹 표준입니다. SpeechRecognition(음성 입력 및 인식을 허용하는 구성요소)와 SpeechSynthesis(음성 출력 및 합성을 허용하는 구성요소)로 구성되어 있습니다.



Web Speech API은 구글에서 2010년에 처음 제안되었고, 2013년 Chrome 25에서 구현되었습니다. 그 이후로 다른 브라우저에서도 지원되고 있지만 호환성과 기능에는 차이가 있습니다. 2023년 12월 기준으로 caniuse.com에서 보고한대로 매우 다양한 지원이 있습니다, 특히 음성 인식에 대해 말이죠:

![음성인식](/assets/img/2024-05-14-WebSpeechAPIWhatWorksWhatDoesntandHowtoImproveItbyLinkingIttoaGPTLanguageModel_1.png)

제 경험상 API의 음성 인식 모듈은 사파리와 크롬에서만 잘 작동했습니다. 각각의 iOS 및 Android 버전도 지원하고 있죠. Brave나 Oculus Browser와 같은 크로미움 기반 브라우저(메타의 퀘스트 브라우저용 가상현실 웹 브라우저)는 음성 인식을 지원하지 않습니다. 이는 구글의 음성 인식 서비스가 독점적이며 라이선스가 필요하기 때문에 발생하는 문제입니다. 구글은 다른 브라우저에게 라이선스를 부여하지 않습니다. 소리내어 나타내자면, 크롬의 음성 인식은 클라우드 컴퓨팅을 기반으로하기 때문에 Brave와 같은 사용자 프라이버시에 중점을 둔 브라우저가 구글이 자원을 이용하여 음성 인식을 실행할 수 있게 하도록 허용한다면 어려운 결정을 내려야 할 것입니다. 그리고 프라이버시에 대해 언급하자면, Web Speech API를 통해 이루어지는 모든 음성 인식은 호출하는 웹페이지가 https를 통해 제공되어야 합니다!

음성 합성은 음성 인식과는 달리 컴퓨터의 모든 주요 브라우저에서 잘 처리되며, 스마트폰을 포함한 대부분의 브라우저에서도 지원됩니다. TTS의 경우, 현재 https는 필수가 아닙니다.



다음에는 구체적으로 Web Speech API에 초점을 맞추어 Chrome에서 해당 API를 사용하는 방법에 대해 알아볼 것입니다. 원칙적으로는 API를 지원하는 모든 다른 브라우저에서도 핵심 요소가 동일하게 작동해야 합니다.

# Chrome의 Web Speech API 사용하기

프로그래밍적으로 Web Speech API를 사용하려면 SpeechRecognition 또는 SpeechSynthesis 인터페이스의 인스턴스를 생성해야 합니다. 음성 입력 또는 출력을 사용하려는 경우에 따라 두 가지를 모두 생성하고 동일한 앱에서 사용할 수도 있습니다!

## 음성 인식



예를 들어, 음성 인식 객체를 생성하려면 이렇게 간단히 할 수 있어요:

```js
var recognition = new SpeechRecognition();
```

하지만, 브라우저에서 실제로 ASR(음성 인식)이 활성화되어 있는지 확인해야 해요. 그러면 최소한의 코드 블록은 다음과 같아요:

```js
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

window.onload = function () {
  if (typeof SpeechRecognition === “undefined") {  // 브라우저가 음성 인식을 할 수 있는지 확인
    console.log(“Speech api failed");
  } else {
    console.log(“Speech api ok");
    const recognition = new SpeechRecognition();  // 음성 인식 객체 생성
    recognition.continuous = true;                // 동작 및 속성 정의, 예를 들어 지속적인 ASR을 요청하는 경우,
    recognition.interimResults = true;            // ASR의 일부 결과 표시 (즉, 결과가 아직 최종이 아닌 경우에 발화가 계속되는 경우)
    recognition.lang = "en-US";                   // 언어는 미리 정의되어야 해요
    recognition.onresult = (event) => {           // 단어나 문장이 인식되었을 때 트리거
      const res = event.results[last];            // 여러 인식된 단어/문장 목록이 있는 경우, 마지막 것을 가져와요
      var text = res[0].transcript.trim();        // 텍스트 추출 및 정리
      … // 텍스트 처리
    }
    recognition.start();            // 모든 설정이 준비된 상태로 인식 시작
  }
}
```



위에 표시된 대로 객체의 메서드와 속성을 사용하여 음성 인식 서비스를 제어하고 최상의 방법으로 활용할 수 있어요. 가장 많이 사용되는 명령어와 속성은 start, stop, abort, lang, interimResults 등이에요. 객체가 발생시키는 이벤트를 처리하기 위해 onstart, onend, onresult, onerror 등과 같은 이벤트 리스너를 추가할 수도 있어요.

특히 주목할만한 프로퍼티 중 하나는, 약간의 부정적인 이유로 매우 주목할 가치가 있는 것은 "grammars"입니다. 음성 인식 "grammars"는 ASR을 수행하는 코드가 일반적으로 이해하지 못하거나 순위가 낮게 채점할 수 있는 단어를 이해할 수 있게 해줘야 한다고 합니다. 이 기능은 발음을 교정하고 이름, 전문용어, 지역 표현 등의 사용에 특히 중요하다고 해요. 기능에 대한 일부 정보는 여기서 찾아볼 수 있지만... 자세한 내용은 다루지 않겠습니다. 왜냐하면... 그냥 제대로 작동하지 않아서 많은 사람들이 이에 대해 불평하고 있거든요. 더구나, 현재 어떤 브라우저도 잘 지원하지 않기 때문에 API에서 gramamrs를 삭제할 계획이 있는데요 (여기를 참조하세요).

SpeechRecognition 객체의 속성 및 이벤트에 대해 더 알고 싶다면, 여기를 확인해보세요.

그리고 웹 Speech API를 음성 인식에 사용하는 전역적이고 완벽한 예제로, 웹 페이지의 배경색을 음성 명령어를 통해 변경하는 방법을 보여주는 이 공식 예제를 확인해보세요.



## 음성 합성

또한, 음성 합성 객체를 만들기 위해 다음과 같이 작성할 수 있습니다:

```js
var synthesis = window.speechSynthesis;
```

그런 다음, SpeechSynthesisUtterance 인터페이스의 인스턴스를 생성할 수 있습니다. 이 인스턴스는 합성하고자 하는 구체적인 음성 요청을 나타냅니다. 텍스트, 음성, 속도, 음높이, 볼륨 등과 같은 utterance의 속성을 설정할 수 있습니다. 객체의 메서드와 속성을 사용하여 음성 합성 서비스를 제어할 수 있습니다. speak, pause, resume, cancel, getVoices 등과 같은 작업을 수행할 수도 있습니다. 또한, utterance가 발생시키는 이벤트를 처리하기 위해 이벤트 리스너를 추가할 수도 있습니다. onstart, onend, onerror 등과 같이 발생하는 이벤트를 처리하기 위한 예시로, 텍스트를 합성하고 시작과 종료시에 로그를 남기는 방법은 다음과 같습니다:



웹 앱에서 정말 좋아하는 것 중 하나는 문자열을 받아서 음성으로 출력하는 함수를 가지고 있는 것입니다. 이 함수를 speakUp()이라고 부르고 다음과 같이 보입니다:

```js
function speakup(TextToSpeak) {
  if (“speechSynthesis" in window) {                            //TTS가 지원되는지 확인
    const toSpeak = new SpeechSynthesisUtterance(TextToSpeak);  //음성 생성
    toSpeak.lang = “en-US";                                     //언어 설정
    window.speechSynthesis.speak(toSpeak);                      //음성 출력!
  } else {
    console.log(“브라우저에서 음성 합성을 지원하지 않습니다.");
  }
}
```

# 웹 Speech API를 통한 음성 인식 및 합성의 문제와 대형 언어 모델을 사용하여 그 일부를 수정하는 방법



웹 Speech API는 매우 쉽게 사용할 수 있어요. 무료이며 API 키도 필요하지 않고 얼마나 자주 호출해도 제한이 없어요.

하지만 계속 시도해보니 시스템이 자주 다운되는 경향이 보이는데, 특히 ASR(자동 음성 인식)에 대해서요. 게다가 API는 때때로 자체적으로 꺼지는 것처럼 보이며, 많은 사람들이 명확한 해결책 없이 질문을 합니다. 더 나쁜 점은 무료 API가 어떤 응용 프로그램에는 괜찮을 수 있지만, 음성 인식 및 합성 측면에서는 최신 기술 수준과는 거리가 멀어요. 일반적으로 설명한 현대 ASR(자동 음성 인식) 시스템에 비해, Chrome의 ASR 서비스는 매우 제한적이에요!

음성 인식은 매우 정확하거나 신뢰성이 높지 않으며, 사투리나 방언에도 민감하지 않아요. 심지어 약간의 소음이 있어도 거의 쓸모가 없어요. 언어를 자동으로 감지하지 않고, 같은 대화에서 여러 사람이 말할 때도 인식하지 못하며, 구두점은 괜찮지만 발성과 혼동되기도 해요.

음성 인식 서비스는 개인 정보 보호 및 보안 문제가 있어요. 음성 데이터가 사용자 동의나 지식 없이 외부 서버나 제3자에 전송됩니다.



음성 합성 서비스는 특히 영어 이외의 언어에 대해서는 매우 자연스럽거나 표현력이 부족합니다.

또한 위에서 본 것처럼 음성 인식 및 합성 서비스는 브라우저와 장치에 매우 의존적이며, 다른 플랫폼 및 지역에서 사용 가능하지 않거나 일관성이 없을 수 있습니다.

그리고 그 한계로 인해 편견, 차별, 속임수, 조작 또는 가장 등 윤리적 및 사회적 영향이 발생할 수 있습니다.

개발자로서 그리고 사용자로서, 이러한 제한사항과 도전에 대해 인식하고 Web Speech API를 책임 있게 사용해야 합니다.



## 대형 언어 모델을 사용하여 음성 인식 문제 완화하기

위 문제 중에서도 음성 인식(ASR)과 관련된 문제가 가장 중요합니다. 이러한 문제 중 많은 것들을 해결하기 위해서는 인식이 진행되는 가장 기본적인 수준에서 조치가 필요합니다. 그러나 전사 정확성과 전문 용어 완성과 관련된 가장 중요한 문제들은 대형 언어 모델을 사용해서 해결할 수 있습니다. 제가 GPT-3.5-turbo 또는 GPT-4를 통해 직접 JavaScript 내부의 웹 앱에서 프로그래밍을 통해 해결하고 있습니다.

작동 방식
기본적으로 언어 모델을 호출하여 입력이 어떻게 보일지와 해당 입력에 대한 출력이 어떻게 되어야 하는지 설명하는 프롬프트를 제공한 다음, API에서 인식한 음성으로부터 나오는 실제 (원시) 전사를 따릅니다.

이 모든 작업을 일으킨 웹 앱에서는 음성 인식을 사용하여 명령을 트리거합니다. 예를 들어 사용자가 "확대", "줌 인" 또는 비슷한 명령을 하면 시각화(분자 그래픽을 위한 앱)가 확대되어야 합니다. 이 앱의 프롬프트는 다음과 같습니다:



```js
let theprompt = [];         //프롬프트가 저장될 배열을 초기화합니다

theprompt.push({
  role: "system",           //모드에게 수행할 작업을 알립니다
  content: "음성인식으로 텍스트를 받고 주어진 예제처럼 명령을 트리거하여 적절히 대응합니다. 요청을 이해하지 못하거나 목록에 없는 경우 didntUnderstand() 명령을 실행합니다.",
  });

theprompt.push(                                      //여기서부터 예제 제공
 { role: “user", content: “분자 크게 만들기" },
 { role: “assistant", content: “scale(+)" }
);
theprompt.push(
 { role: "user", content: "확대" },
 { role: "assistant", content: "scale(+)" }
);
theprompt.push(
 { role: "user", content: "크게 만들기" },
 { role: "assistant", content: "scale(+)" }
);
theprompt.push(
 { role: "user", content: "축소" },
 { role: "assistant", content: "scale(-)" }
);
theprompt.push(
 { role: "user", content: "작게 만들기" },
 { role: "assistant", content: "scale(-)" }
);
```

이를 자세히 살펴보겠습니다. 여기에 프롬프트의 시스템 요소가 있습니다:

그런 다음 사용자/어시스턴트 쌍이 동일한 또는 관련된 출력을 생성하는 가능한 입력의 예제를 제공하는 방법을 살펴보십시오. 예를 들어 "확대"라고 말하면 "크게 만들기" 모두 scale(+) 호출을 결과로 지니며, "축소"는 scale(-) 호출을 합니다.

코드의 다른 부분에서는 프로그램이 크래시하지 않는 방식으로 입력을 수정하도록 언어 모델을 가르치는 항목을 찾을 수 있습니다. 동시에 올바른 출력을 생성할 가능성을 높입니다. 예를 들어, 제가 말하는 "최소화"는 종종 "미니 마우스"로 인식되고 "ANI"는 표준 영어 단어가 아닌 "애니"로 인식됩니다. 그런 다음 이러한 예제로 언어 모델을 지시할 수 있습니다:




```js
theprompt.push(
 { role: "사용자", content: "ANI로 최소화하기" },
 { role: "조수", content: "minimize(ANI)" }
);
theprompt.push(
 { role: "사용자", content: "애니로 최소화하기" },
 { role: "조수", content: "minimize(ANI)" }
);
theprompt.push(
 { role: "사용자", content: "애니로 미니 쥐들을 최소화하기" },
 { role: "조수", content: "minimize(ANI)" }
);
...등등.
```

이 요령이 굉장히 잘 작동하는 것을 알 수 있습니다. 이는 최신 언어 모델인 Whisper와 같은 모던한 언어 모델의 핵심입니다. 이 모델은 언어 모델을 직접 음성 인식 절차에 포함시켜서 가능한 것입니다!

이 섹션을 마치면서, 물론 전체 프롬프트는 저의 경우에는 주로 GPT-3.5-turbo나 OpenAI의 GPT-4로부터의 응답을 포함하는 프로미스 내부에 있어야 합니다.

```js
fetch(`https://api.openai.com/v1/chat/completions`, {    // 음성 API를 호출하는 주요 부분
  body: JSON.stringify({
  model: "gpt-3.5-turbo",                     // 또는 2023년 12월 기준 GPT-4
  messages: theprompt,                    // 방금 생성된 프롬프트
  temperature: 0,                         // 환각을 피하기 위해 이를 낮추세요
  max_tokens: 20,                         // 출력물은 작은 명령어여야 하므로 이 값도 낮추세요
}),
  method: "POST",
  headers: {
    "content-type": "application/json",
    Authorization: "Bearer " + 빈칸에 당신의 API 키와 문자열을 넣으세요,    // OpenAI의 GPT 모델용 API 키!
  },
}).then((response) => {
  if (response.ok) {
    response.json().then((json) => {
    var command = json.choices[0].message.content.trim();    // 명령어가 담긴 텍스트를 추출합니다
    console.log(command);
    if (command == ....                 // 작업 목록을 나열하고 실행합니다                  
```



# Chrome Web Speech API를 사용하는 몇 가지 사례

이 블로그 글을 트리거한 앱 이외에도 음성 인식 및/또는 합성 API를 사용하는 이 다른 프로젝트들을 공유할 수 있어요:

이 예시에서, 나는 Chrome의 음성 인식 기능을 GPT-3과 함께 사용하여 음성으로 입력된 노트와 가이드라인에서 이메일을 작성하는 웹 앱을 만들었어요.

여기에서, 나는 당신에게 Chrome의 음성 인식 및 합성 API를 사용하여 GPT-3으로 구동된 언어 모델이 채팅봇의 '뇌'로 작용하는 채팅GPT와 유사한 봇을 만드는 방법을 보여줄 거예요.



이 다른 예제에서는 Chrome의 음성 인식 기능을 사용하여 웹 앱을 제어합니다. 이 경우, GPT-3를 사용하여 발언된 요청을 명령으로 변환합니다.

# 웹 음성 API를 넘어서

위에서 설명했듯이 일부 웹 브라우저는 음성 합성을 지원하지 않을 뿐 아니라 음성 인식도 지원하지 않습니다. 또한 Chrome의 내장 음성 API가 좋지 않다고 설명했고, 특히 음성 인식에 있어서는 그 제한사항이 극복되는 경우도 있지만, 이 전략으로 해결할 수 없는 문제도 있으며, 따라서 현대 음성 인식 시스템이 가지고 있는 많은 기능이 완전히 부족한 경우가 많습니다.

이러한 맥락에서, 브라우저가 대안 서비스나 솔루션을 사용하여 음성 인식 및 합성을 지원할 수 있다는 것을 알아두는 것이 중요합니다 - Chrome의 무료 API와 대조적으로 유료이지만 일반적으로 더 강력한 원칙입니다.



일부 이 API 서비스를 제공하는 회사들을 언급해보자면 Gladia, Speechly, AssemblyAI, Deepgram, Spechmatics(이 회사는 여기에서 멋진 예제를 자랑해요) 등이 있습니다. Google도 Chrome에서 무료로 제공하는 것과 별도로 ASR 시스템을 보유하고 있으며, 이는 분명히 더 잘 작동합니다. 또한 Microsoft와 AWS와 같은 기술 거인들도 API를 통해 제공하는 제품을 갖고 있습니다. OpenAI의 오픈 소스 Whisper를 다운로드해서 특정 서비스로 실행할 수도 있습니다. 이 경우에는 본인의 요구에 따라 커스터마이징하여 사용할 수도 있는데, 그런 경우에는 Gladia.io와 같이 API를 제공하는 회사와 함께 하는 편이 더욱 편리할 수 있습니다.

물론, Whisper를 로컬 서버에서 사용하듯이, 닫힌 환경에서 구현하지 않는 한, 이러한 서비스 대부분은 오디오를 서버로 전송해야 하므로 개인 정보를 잠재적으로 노출할 수 있습니다. 그러나 민감하지 않은 작업에 대해서는 매우 저렴한 비용으로도 완벽한 해결책이 될 수 있습니다.

# 더 많은 읽을거리

www.lucianoabriata.com 나는 내 관심 분야인 자연, 과학, 기술, 프로그래밍 등에 관한 모든 것에 대해 쓰고 있습니다. 새로운 이야기를 이메일로 받으시려면 구독하세요. 소규모 작업에 대해 상담하려면 여기에서 내 서비스 페이지를 확인하세요. 저에게 연락하려면 여기에서 연락하세요. 팁을 보내고 싶으시다면 여기에서 가능합니다.