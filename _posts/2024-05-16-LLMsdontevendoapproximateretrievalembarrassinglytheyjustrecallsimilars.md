---
title: "LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요"
description: ""
coverImage: "/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png"
date: 2024-05-16 04:15
ogImage: 
  url: /assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png
tag: Tech
originalTitle: "LLMs don’t even do ‘approximate retrieval’ — embarrassingly, they just recall ‘similars’"
link: "https://medium.com/ontologik/llms-dont-even-do-approximate-retrieval-embarrassingly-they-try-to-recall-some-similars-59147f9f9cce"
---


![이미지](/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png)

새로운 우수한 게시물에서 Melanie Mitchell은 대형 언어 모델(Large Language Models, LLMs)과 관련된 중요한 문제에 대해 다루었습니다. 바로 LLMs 내의 ‘지능’에 대한 본질(그런 게 있는 경우)과 LLMs이 전혀 ‘추론’하는지에 대한 문제입니다. 이 질문은 일부 LLMs의 결과물이 어떤 메모된 콘텐츠를 ‘회상’하는지 테스트하여 가장 잘 대답할 수 있습니다. 이 콘텐츠가 똑똑한 방식으로 이어붙여진 것인지 아니면 진정한 추론과 이해의 결과물인지를 확인하는 것입니다.

이 맥락에서 반사적 작업은 LLMs를 테스트하기에 이상적인 작업이며, 기본적으로 LLM에게 ‘훈련’ 데이터에서 볼 가능성이 매우 높은 입력을 제공하는 대신, 훈련 중에 본 적이 없는 데이터에 노출시킵니다(‘반사적’ 데이터). 이 기술은 다른 사람들이 시도한 바 있다고 보고되었고, 그 결과로 LLMs의 훈련 ‘템플릿’을 방해하면 LLMs의 성능이 거의 무작위 선택으로 떨어진다고 합니다.

이러한 결과에 영감을 받아(전혀 놀랍지 않게), 예전에 한 몇 가지 테스트를 새롭고 더 나은 GPT 4o(AGI에 가까워지고 있는 것이죠 — 네, 제가 굉장히 비꼬는 중입니다)에서 다시 시행했습니다. 제가 수행한 실험은 ‘반사적’ 개념과 유사하며, 다른 코딩 체계로 특정 현실을 표현하는 가능한 세계를 만듭니다. 단, 의미론적으로는 현실 자체는 여전히 동일합니다. 예를 들어, 영어 언어의 알파벳을 어떤 방식으로든 혼합한다고 가정해봅시다. 이제 ‘a’는 ‘b’이고 ‘b’는 ‘c’이런 식이죠. ‘dbu’의 의미는 여전히 우리 모두가 알고 있는 모찌 고양이들을 가리켜야 합니다. 왜냐하면 바뀐 것은 단순히 ‘고양이’가 ‘dbu’로 바뀌었을 뿐이고, 고양이 자체는 전혀 바뀌지 않았기 때문입니다. 물론, LLMs는 현실에 대해 아무것도 알지 않으므로 — 고양이에 대해서도 마찬가지이죠, 그들의 표면적인 메모리(수백만 개의 가중치를 통해)는 이 ‘반사적’ 요령에 즉시 노출될 것입니다.



저는 이 테스트로 LLM의 세계가 뒤죽박죽될 것이라는 것을 시험해보기도 전에 알았다고 말해야겠어요. 왜냐하면 저는 LLM과 모든 딥 뉴럴 네트워크가 거대한 흐릿한 해시 테이블을 지나지 않는다고 확신했었거든! 그리고 실망하지 않았어요. 놀랍다고 느낄 만한 점은 성능이 얼마나 이상하게 나왔는지였어요. 사실, 몇몇 실험에서 LLM은 한두 구절과 '일치하는' 텍스트를 가져오기도 했는데, 그 텍스트는 전혀 관련이 없는 내용이었어요. 어떤 경우에는 LLM이 이전에 대화에서 세 개나 네 개 쿼리 전에 저장해 둔 텍스트를 끌어오기도 했어요. 완전히 무작위로 말뿐인 것들 — 코사인 유사도 함수가 엉뚱한 텐서를 다룰 방법을 모르고 있었죠.

재미있게 놀아보고 싶다면, 다양한 쿼리와 코딩 방식을 시도해보세요. LLM이 기억해 둔 것을 어떻게 방해할 수 있는 "대상 이론"을 만들어보세요 — 다만 의미(현실)를 바꾸지 않고요. 그리고... 수십억 달러를 들여서 딱 '대략적인 검색'을 수행하는 거대하고 지능이 없는 기계와 함께 재미를 느껴보세요. 그래서 '거대한 흐릿한 해시 테이블'이라는 용어를 사용하는 거지요.



나무에서 내려와서 우리가 40년 전보다 알고 있던 것을 인정할 때가 언제일까요? 순수히 행동주의적이고 연관적이며 통계적인 패러다임만으로는 인지를 설명할 수 없다는 것을요. 데이터에서 패턴을 찾아낸 몇몇 소과일을 수확하는 데에 유용했더라도, 이 패러다임으로는 언어나 추론, 이해력, 그리고 마음에 대해 (과학적으로) 아무것도 알려주지 않을 겁니다.