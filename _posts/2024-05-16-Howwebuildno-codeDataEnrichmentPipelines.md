---
title: "무코드 데이터 인리치먼트 파이프라인을 어떻게 구축하는지"
description: ""
coverImage: "/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png"
date: 2024-05-16 04:01
ogImage: 
  url: /assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png
tag: Tech
originalTitle: "How we build no-code Data Enrichment Pipelines"
link: "https://medium.com/@yotam.aharony/how-we-build-no-code-data-enrichment-pipelines-f5815b9d0fe4"
---


Explorium은 특정 산업 및 비즈니스 요구에 맞게 맞춤 데이터 시그널을 제작하는 것에 특화되어 있습니다. 이것이 바로 우리가 데이터 파이프라인을 만드는 책임을 지고 있는 데이터 제품 개발자(데이터 엔지니어)로 구성된 전담 팀을 모았던 이유입니다.

![image](/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png)

# 데이터 풍부화 파이프라인이란 무엇인가요?

데이터 풍부화는 비즈니스 의사 결정을 위한 유틸리티와 가치를 향상시키기 위해 데이터를 향상하고 정제하는 프로세스입니다. 이 프로세스에는 일반적으로 다양한 소스에서 외부 데이터를 사용자의 입력과 병합하는 것이 포함됩니다. 데이터 풍부화는 CRM 강화, 리드 점수 매기기, 리스크 분석 및 타겟 마케팅 이니셔티브를 포함한 다양한 도메인에 적용될 수 있습니다.



예를 들어, 회사의 기본 세부정보 — 이름과 주소 같은 것들 —을 매출, 직원 수, 그리고 NAICS 코드와 같은 기업 데이터로 보강함으로써 CRM 내에서 리드 점수 산정 프로세스가 크게 향상됩니다. 더불어, 이러한 보강된 데이터는 유료 고객으로 전환될 가망이 있는 잠재고객을 평가하기 위한 예측 모델 학습에 활용될 수 있으며, 마케팅 전략을 최적화하는 데도 도움이 됩니다.

금융 집계에 관심이 있는 사용자를 가정했을 때, 예를 들어 Yahoo Finance 보강 파이프라인을 살펴봅시다: 사용자가 회사 이름을 입력하면 이는 대응하는 티커 심볼로 변환됩니다 (AAPL, AMZN) — 정확한 데이터 검색에 있어서 중요한 단계입니다. 그런 다음, Yahoo Finance API를 통해 금융 데이터에 접근하여 주식 성능 지표를 가져옵니다. 주요 금융 통계는 이를 이용해 월별 주식 가격의 최저, 최고, 평균가 및 거래량을 집계하여 추출됩니다. 이러한 집계는 정보를 제공하는 요약을 제공하여 특정 기간 내 주가 추세와 시장 행동에 대한 통찰을 제공합니다.

더불어, 해당 파이프라인에는 주기적인 데이터 가져오기 프로세스 주기가 포함될 수 있습니다. 이러한 맥락은 LLMs를 활용하여 통찰을 얻는 데 활용될 수 있습니다. 예를 들어, 회사의 주식이 상승 추세인가 하강 추세인가?

## 단계별 기능 요구사항과 도전 과제



Pre Transform

목표: 파이프라인의 입력을 확인하고 정리하며 정규화하는 것입니다.

데이터 클렌징: 입력 데이터를 정규화하고 수정하여 공통 언어로 번역함으로써 향상된 데이터의 정확도, 범위 및 신뢰성을 얻습니다. 사용자가 제시한 데이터 유형에 따라 입력을 구조화하고자 합니다. 따라서 사용자의 데이터 모델에 따른 내부 Transformer를 실행합니다:

![How web build no-code Data Enrichment Pipelines](/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_1.png)



- TextToOrganizationName: 회사 이름을 더 구조화된 레이블로 표준화하여 일반 접미사를 제거하고 소문자로 바꾸며 정리합니다.
- CountryToCountryCodeAlpha2 — 미국, united states, United States of America는 모두 동일한 개체를 나타내는 서로 다른 레이블입니다: 'us'.

데이터 정확도 향상: 주소, 전화 번호, 이메일과 같은 데이터 요소를 확인하여 현재 및 올바른지 확인합니다. 예: 잘못된 이메일로 레코드를 보완하지 않도록 합니다.

도전 과제: 관련 트랜스포머가 올바르게 사용되고 있는지 어떻게 확인하나요? 모든 보강 파이프라인을 통해 트랜스포머 버전을 어떻게 업데이트하나요?

검색중



목표: 내부 또는 외부 소스에서 데이터 신호를 검색하는 것입니다.

- 내부 데이터베이스: PostgreSQL, DynamoDB, Elasticsearch 또는 기타 프로덕션 데이터베이스에서 데이터를 가져옵니다. 오프라인 ETL(추출, 변환, 로드) 프로세스가 정기적으로이러한 데이터베이스를 업데이트합니다.
- REST API: 다양한 공급업체에 쿼리하여 풍부화된 레코드의 데이터를 수집합니다. 공급업체에는 서로 다른 특성이 있습니다. 몇몇은 엄격한 동시성 제한, 다른 쿼리 방법 또는 일괄 처리 크기가 있을 수 있습니다.

도전 과제: 백만 개의 레코드를 효율적으로 검색할 때 데이터베이스 연결 관리는 복잡할 수 있습니다. 외부 REST API 호출의 경우 성능 최적화가 중요합니다. 확장 가능한 동시 API 요청을 구현하는 것이 핵심입니다. 재시도 메커니즘 및 속도 제한기를 사용하면 API 상호 작용의 견고성을 향상시킬 수 있습니다. 캐싱 시스템을 통합함으로써 성능을 향상시킬 수도 있습니다.

인메모리 처리



목표: 데이터 처리 및 보충

예를 들어, 매장 수와 총 매출이 주어졌을 때 매장 당 매출을 계산하거나, 회사 유형과 지점 수와 같은 필드를 기반으로 기고 학습 모델을 활용하여 회사 규모와 같은 누락된 세부 정보를 보충합니다.

도전: 노코드 유연한 프로세스 엔진을 어떻게 구현할까요?

변환 후



목표: 모든 신호가 유효한지 확인하고 올바른 데이터 유형으로 변환하며, 부동 소수점을 정수로 변환하고 유효하지 않은 데이터를 삭제합니다.

## 일반 인프라

각 단계마다 도전 과제가 있습니다. 특히 파이프라인의 수십 개의 다양한 부분 부가를 처리할 때는 보다 견고한 인프라가 필요합니다. 또한 다양한 부가물의 방대한 양을 사용하려면 전략적인 접근 방식이 필요합니다. 내부 패키지 버전이 업데이트되면 모든 관련 부가물을 재배포해야 하는 경우가 많습니다. 이러한 이유로 모든 파이프라인에 걸쳐 향상된 모니터링 기능을 강화하기 위해 표준 로깅 프로토콜의 구현이 중요합니다. 시스템 효율 유지에 중요한 캐시 히트율, API 쿼리 수, 파이프라인 처리량 및 지연 시간 등의 주요 성능 지표를 모니터링하는 것이 필수적입니다.

## 이전 인프라



No-code 인프라가 등장하기 전에는 일반적인 Python 추상화 클래스가 있었습니다. 이 클래스에는 가져오기(fetch)와 처리(process) 단계가 포함되어 있었죠. 각 데이터 엔지니어마다 고유한 파이프라인 Python 코드를 구현했습니다. API 쿼리, 다양한 데이터베이스 쿼리 또는 변환기(transformers) 실행을 위한 공유 패키지가 있었지만 코드는 난잡했습니다.

- 중복된 노력과 비효율성: 다른 사람의 작업을 명확히 파악할 방법이 없어 엔지니어들은 종종 다른 방식으로 동일한 문제를 해결하여 노력을 중복했습니다. 이는 시간과 자원을 낭비하고 문제 해결과 코드 유지보수에서 비효율성을 야기했습니다.
- 확장성 문제: 비즈니스 규모가 커지고 데이터 양이 증가함에 따라 각 엔지니어의 고유한 파이프라인의 본질은 확장성 문제를 야기했습니다. 개별화된 솔루션은 확장성을 고려하지 않고 설계되어, 증가된 부하를 처리하기 위해 상당한 재작업이 필요했습니다.
- 지식 이전의 어려움: 표준 코딩 관행이나 공유 지식 저장소가 없었기 때문에 새 엔지니어가 새로 합류하고 기존 파이프라인을 이해하는 것이 어려웠습니다. 이는 팀원 사이에서 최선의 사례를 공유하는 것을 방해하여 혁신과 개선의 속도를 늦추었습니다.
- 데이터 품질이 손상된 우선순위에 따른 타협: 데이터 엔지니어들이 주로 성능, 확장성 및 오류 처리에 집중할 때 데이터 품질이 손상될 수 있습니다. 이 소홀함으로 인해 데이터 문제의 감지와 교정이 지연될 수 있으며, 의사 결정에 영향을 주는 일관성 없는 데이터 출력을 초래하여 데이터의 전반적 가치를 감소시킬 수 있습니다.
- 복잡하고 비효율적인 배포 흐름: 배포 프로세스가 길고 복잡했으며 종종 충돌을 피하고 서비스 간 일관성을 보장하기 위해 주의 깊은 조정이 필요했습니다. 게다가, 확장 작업은 주로 파이프라인 pod에만 집중되어 비효율적인 자원 사용을 야기했습니다. 이 접근 방식은 개발 주기가 몇 일(심지어 몇 주)에 걸쳐 확장되도록 이끌었습니다.

이러한 점들은 분투와 비효율성을 강조하며, 이러한 문제를 효과적으로 해결하기 위해 No-code 인프라의 중요성을 강조합니다.

# 새 No-Code 인프라를 위한 안내 지침



해당 병목 현상을 해결하기 위해 새로운 노코드 인프라 프로젝트를 시작하기로 결정했습니다:

- 빠른 개발 주기: 노코드 인프라는 개발 프로세스를 혁신적으로 가속화합니다. 새로운 향상 파이프라인 개발은 이제 하루 이상이 아닌 기존 코딩 방법의 오랜 기간보다 단 하루 또는 두 날만 소요됩니다. 게다가 버그 수정 및 기능 추가는 분에서 몇 시간 안에 완료되어 본운에 배포될 수 있습니다. 이러한 신속한 개발 주기는 비즈니스 요구사항 및 기술적 변화에 대한 더 민첩한 대응을 가능하게 합니다.
- 유지 관리 및 모니터링: 새로운 시스템은 유지 관리를 간소화하고 모니터링 기능을 향상시킵니다. 모든 파이프라인 간에 공통 로깅 및 추적 언어가 있으며 파이프라인 모두 간 공유 요소 및 속성이 있습니다.
- 설정으로써의 향상 파이프라인: 새로운 인프라는 향상 파이프라인을 코드 중심 프로젝트가 아닌 설정으로 취급하여 개별 구성 요소 관리와 관련된 복잡성을 최소화합니다. 이를 통해 데이터 파이프라인의 개발 및 유지 관리가 간소화되며 엔지니어가 기술적인 세부 사항을 탐색하지 않고 데이터 품질과 기능성의 최적화에만 집중할 수 있도록 합니다. 이를 통해 제공되는 데이터 제품이 고품질이고 비즈니스 요구를 충족하는 것이 보장됩니다.
- 확장성 및 유연성: 새로운 인프라는 특정 파이프라인에 중점을 두지 않고 수요에 따라 자동으로 확장되도록 설계되었습니다. 이러한 탄성은 자원의 효율적인 사용을 가능하게 하며 지속적인 조정이 필요하지 않으면서 변화하는 부하에 맞춰 적응할 수 있습니다.

# 좋은 추상화의 구성 요소

- 가독성: 구성은 YAML 기반으로 작성되어 JSON보다 사용자 친화적입니다. 또한, “include"와 같은 기능을 지원하는 사용자 정의 YAML 파서를 통합하여 모듈화 구성을 가능하게 했습니다. 이 모듈성은 복잡한 구성 관리를 도와 재사용 가능하고 관리하기 쉬운 구성 요소로 분해함으로써 유지 보수성과 가독성을 향상시킵니다.
- 구조화 및 모듈식 설계: 각 향상 파이프라인은 명확하고 논리적인 구조를 따르는 정의된 단계의 시퀀스를 통해 구축됩니다. 각 단계는 특정 리소스를 활용하도록 설계되었습니다. REST API 기반 리소스, PostgreSQL, DynamoDB, DuckDB 또는 추가 구현된 다른 리소스 등 어떤 리소스든 사용할 수 있습니다. 이러한 구조화된 접근 방식은 파이프라인이 확장 가능하고 적응 가능하도록 하여 비즈니스 요구사항이 변할 때 리소스의 통합 또는 수정이 쉽게 가능해집니다.
- 효율적인 개발 및 테스트: 저희는 전통적인 코드 배포보다 구성에 중점을 둔 접근 방식을 채택하여 개발 및 테스트 단계를 크게 효율화했습니다. 엔지니어들은 코드를 배포할 필요 없이 구성을 직접로드하고 테스트할 수 있어 더 빠른 개발 반복을 이끌어냅니다. 현재 환경 전체에 가장 최신의 구성이 동기화되도록 보장하는 CI 시스템과 통합된 YAML 구성 전용 저장소를 유지합니다.



# 인프라 솔루션

다중 통합: 우리 시스템 내에서 다중 통합을 허용하기 위해 'Resource'라는 추가 추상화를 만들었습니다. 리소스는 데이터 소스를 정의하는 방법입니다. 각 리소스는 고유한 이름을 가지며 사용 가능한 커넥터 중 하나를 사용합니다. 한 번 정의하면 여러 단계나 풍부화 파이프라인에서 사용할 수 있습니다. 저희가 보유한 일부 사용 가능한 리소스는 PostgreSQL, DynamoDB, Elasticsearch, DuckDB 또는 모든 REST API 제공 업체 등이 있습니다.

```js
name: google_geocoding
connector:
  type: REST@v1
  base_url: "https://maps.googleapis.com/maps/api/geocode/json"
  auth:
    type: api_key
    parameters:
      key: <my_api_key>
    add_to: query_parameters
  timeout_seconds: 10
retry_policy:
  type: exponential
  max_tries: 3
  max_time: 60
batch_size: 1
concurrency: 1000
```

REST API: REST API를 위한 강력한 추상화를 구현하는 것은 재시도 메커니즘과 실패한 요청 처리 정책과 같은 좀 더 고급 기능을 포함할 수 있습니다. 각 쿼리에 관련 매개변수를 추가하는 일반적인 방법(메서드 유형 - GET/POST, 전용 본문, 쿼리 매개변수, 헤더 또는 인증)이 포함될 수 있습니다. 일부 제공 업체는 동일한 쿼리 내에서 여러 레코드를 허용하고 '맵/리듀스' 메커니즘을 지원해야 했습니다. 일부 제공 업체는 HTTP 성공 응답(200)을 반환하지만 실패 정보가 응답 내에 캡슐화되어 있어 상태 해결자를 구현해야 했습니다.



DuckDB를 사용하여 데이터 처리 유연성 강화: 이전 인프라에서는 데이터 엔지니어들이 Python 코드를 작성하여 객체 처리, 데이터 병합, 외부 소스 가져오기, 집계, 예외 처리와 같은 작업에 필요한 유연성을 즐겼습니다. 이 유연성을 유지할 효과적인 대안을 찾기 위해 DuckDB를 우리의 인메모리 처리 프레임워크에 통합하기로 결정했습니다. DuckDB는 온라인 분석 처리 (OLAP) 시나리오에서 흔히 발생하는 분석 쿼리 워크로드에 특히 최적화되어 있습니다. 저희 운영에서 일반적인 Python 사용 사례를 평가한 결과, DuckDB의 능력은 lambda 및 유틸리티 함수, 패턴 매칭, 고급 텍스트 함수와 같은 기능으로 확장되어 대부분의 Python 기능을 SQL 쿼리를 사용하여 재현할 수 있다는 것을 확인했습니다. 이러한 적응은 우리의 데이터 처리가 견고하고 다용도로 유지되도록 보장합니다.

DuckDB 능력을 확인해보겠습니다. 다음은 추가 처리가 필요한 중첩 JSON인 Google Geocoding API 응답의 예제입니다:

```js
{
    "results": [
        {
            "address_components": [
                ...
            ],
            "formatted_address": "1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA",
            "geometry": {
                "location": {
                    "lat": 37.4224428,
                    "lng": -122.0842467
                },
                "location_type": "ROOFTOP",
            },
            ...
        }
    ],
    "status": "OK"
}
```

우리가 관련 필드만 추출하고 싶다고 가정할 때, DuckDB 엔진을 활용하여 이 응답을 평평하게 만드는 SQL 쿼리를 구축할 수 있습니다.



```js
select 
    results ->> '$[0].geometry.location_type' as "위치 유형",
    results ->> '$[0].geometry.location.lat' as "위도",
    results ->> '$[0].geometry.location.lng' as "경도",
from {_.data}
```

청크 크기: 성능을 최적화하기 위해 요청된 데이터를 청크와 배치로 나눠 빠르고 확장 가능한 패칭을 할 수 있습니다. 50만 레코드의 파이프라인을 순차적으로 실행하는 대신, 데이터가 작은 병렬 청크로 나눠집니다. 

템플릿 엔진: 시스템을 개발할 때, 간단한 설정 방식만으로는 개발자들의 요구를 충족시키기에는 부족하다고 판단했습니다. 우리는 유연성을 향상시키기 위해 Jinja 템플릿 엔진을 설정 프레임워크에 통합하기로 결정했습니다. Jinja 템플릿 엔진을 사용한 SQL 예제:

```js
-- 소셜 링크
{-
  set social_platforms = ['twitter', 'facebook', 'linkedin', 'pinterest', 'youtube', 'instagram']
-}
select
  { for platform in social_platforms -}
    trim({ platform }_link, '"') as { platform }_link,
  { endfor },
  len(social_links) as number_of_social_networks
from { _.data }
```



# No-Code 예제를 통한 Enrichment 파이프라인

마침내, 완전한 Google Geocoding Enrichment 파이프라인에 대해 살펴보겠습니다. 이 파이프라인은 PostalAddress 유형의 입력을 받아 다양한 지리적 위치 속성으로 보강합니다. 이 파이프라인은 Google API에서 데이터를 가져오는 단계와 DuckDB 엔진을 사용하여 데이터를 후처리하는 두 단계로 구성되어 있습니다.

```js
name: google_geocoding_api
pipelines:
- input_schema:
    fields:
    - name: PostalAddress
      type: PostalAddress
  steps:
  - name: fetch
    action:
      type: rest@v1
      resource_name: google_geocoding
      method: GET
      # Jinja 템플릿 엔진을 활용하여 쿼리 매개변수에 입력을 추가합니다
      parameters:
        address: "{_.data[0].PostalAddress }"
      # 'include' 기능을 보여주는 데모
      status_resolver: !include 'google_geocoding_api/status_resolver.sql'
  - name: post_duck
    action:
      type: duckdb@v1
      query: !include 'google_geocoding_api/post_transform.sql'
signals:
- Location Type
- Latitude
- Longitude
- Country
- Region
- Sub-Region
- US State
- County
- City
- Neighborhood
- Subpremise
- Street
- House Number
- Zip Code
```

새로운 인프라는 데이터 엔지니어들에게 진보된 보편적인 도구를 제공하여 조직적 요구에 맞춘 견고하고 유연한 데이터 파이프라인을 설계하고 구현할 수 있도록 도와줍니다. 이 접근 방식은 루틴적인 성능 문제에서 전략적 데이터 품질 관리로 초점을 전환합니다.



# 요약

이 기사에서는 복잡한 데이터 제품을 개발하기 위한 구성 기반 접근 방식을 소개했습니다. 기존의 코드 중심 방법에서 혁신적인 노코드 인프라로의 전환을 강조했습니다. 데이터 엔지니어들은 이제 고급 도구를 이용하여 빠른 데이터 파이프라인 개발 및 배포가 가능해졌으며, 컨셉에서 제품화까지 걸리는 시간을 몇 시간으로 대폭 줄였습니다. 유지 관리를 간소화하고 모니터링을 강화하며 동적 확장이 가능하게 했습니다.

이 접근 방식은 운영을 최적화할 뿐만 아니라 엔지니어들이 인프라를 관리하는 대신 데이터 품질에 중점을 둘 수 있도록 돕습니다. 효율이 발전하는 환경을 조성합니다.