---
title: "íŒŒì´í† ì¹˜ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë§Œë“¤ì–´ë³´ê¸° GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ í¬í•¨"
description: ""
coverImage: "/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png"
date: 2024-05-15 10:33
ogImage: 
  url: /assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png
tag: Tech
originalTitle: "Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)"
link: "https://medium.com/towards-data-science/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc"
---


## C/C++, CUDA, ë° Pythonì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ìœ ì˜ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•´ ë³´ì„¸ìš”. GPU ì§€ì›ê³¼ ìë™ ë¯¸ë¶„ì„ ì œê³µí•©ë‹ˆë‹¤

![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png)

# ì†Œê°œ

ì—¬ëŸ¬ í•´ ë™ì•ˆ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•´ ì™”ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ê·¸ ë¬¸ë²•ê³¼ ê·œì¹™ì„ ìµíˆê³ ë„, ì œ ê¶ê¸ˆì¦ì„ ìê·¹í•˜ë˜ ê²ƒì´ ìˆì—ˆìŠµë‹ˆë‹¤: ì´ëŸ¬í•œ ì‘ì—… ì¤‘ì— ë‚´ë¶€ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ” ê±¸ê¹Œìš”? ì´ ëª¨ë“  ê²ƒì´ ì–´ë–»ê²Œ ì‘ë™í• ê¹Œìš”?



ì—¬ê¸°ê¹Œì§€ ì˜¤ì…¨ë‹¤ë©´, ì•„ë§ˆë„ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ ê°€ì§€ê³  ê³„ì‹¤ ê²ƒì…ë‹ˆë‹¤. íŒŒì´í† ì¹˜(PyTorch)ì—ì„œ ëª¨ë¸ì„ ìƒì„±í•˜ê³  í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ë¬¼ì–´ë³¸ë‹¤ë©´ ì•„ë§ˆë„ ì•„ë˜ ì½”ë“œì™€ ë¹„ìŠ·í•œ ê²ƒì„ ìƒê°í•´ë³¼ ê²ƒì…ë‹ˆë‹¤:

```js
import torch
import torch.nn as nn
import torch.optim as optim

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(1, 10)
        self.sigmoid = nn.Sigmoid()
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.sigmoid(out)
        out = self.fc2(out)
        
        return out

...

model = MyModel().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)

for epoch in range(epochs):
    for x, y in ...
        
        x = x.to(device)
        y = y.to(device)

        outputs = model(x)
        loss = criterion(outputs, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

í•˜ì§€ë§Œ ì´ë²ˆì— ì—­ì „íŒŒ(backward) ë‹¨ê³„ê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë¬¼ì–´ë³¸ë‹¤ë©´ ì–´ë–¨ê¹Œìš”? ë˜ëŠ” ì˜ˆë¥¼ ë“¤ì–´, í…ì„œë¥¼ ì¬êµ¬ì„±í•  ë•Œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ìš”? ë‚´ë¶€ì—ì„œ ë°ì´í„°ê°€ ì¬ë°°ì¹˜ë˜ë‚˜ìš”? ê·¸ëŸ° ì¼ì´ ì–´ë–»ê²Œ ì¼ì–´ë‚˜ë‚˜ìš”? ì™œ PyTorchëŠ” ë¹ ë¥¸ê°€ìš”? PyTorchê°€ GPU ì—°ì‚°ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ìš”? ì´ëŸ° ì§ˆë¬¸ë“¤ì´ í•­ìƒ ì €ë¥¼ í˜¸ê¸°ì‹¬ ê°€ë“í•˜ê²Œ ë§Œë“¤ì—ˆê³ , ì—¬ëŸ¬ë¶„ë„ ë§ˆì°¬ê°€ì§€ë¡œ í˜¸ê¸°ì‹¬ì´ ë“œì‹¤ ê²ƒì´ë¼ê³  ìƒìƒí•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ëŸ¬í•œ ê°œë…ì„ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì²˜ìŒë¶€í„° êµ¬ì¶•í•´ë³´ëŠ” ê²ƒì´ ë¬´ì—‡ë³´ë‹¤ ì¢‹ì„ê¹Œìš”? ì´ ê¸€ì—ì„œ ì—¬ëŸ¬ë¶„ì´ ë°°ìš°ê²Œ ë  ê²ƒì´ ë°”ë¡œ ê·¸ê²ë‹ˆë‹¤!

## #1 â€” í…ì„œ



í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € ì•Œì•„ì•¼ í•  ê°œë…ì€ ë¬´ì—‡ì´ í…ì„œì¸ì§€ì— ëŒ€í•œ ëª…ë°±í•œ ê°œë…ì…ë‹ˆë‹¤.

í…ì„œëŠ” ëª‡ ê°€ì§€ ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ” nì°¨ì› ë°ì´í„° êµ¬ì¡°ì˜ ìˆ˜í•™ì  ê°œë…ì´ë¼ëŠ” ì§ê´€ì ì¸ ìƒê°ì„ ê°€ì§€ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ê¸°ì„œëŠ” ì´ ë°ì´í„° êµ¬ì¡°ë¥¼ ê³„ì‚°ì  ê´€ì ì—ì„œ ì–´ë–»ê²Œ ëª¨ë¸ë§í• ì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. í…ì„œëŠ” ë°ì´í„° ìì²´ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ì–‘ì´ë‚˜ í…ì„œê°€ ìˆëŠ” ì¥ì¹˜(ì˜ˆ: CPU ë©”ëª¨ë¦¬, GPU ë©”ëª¨ë¦¬)ì™€ ê°™ì€ ì¸¡ë©´ì„ ì„¤ëª…í•˜ëŠ” ë©”íƒ€ë°ì´í„°ë¡œ êµ¬ì„±ëœë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í…ì„œì˜ ë‚´ë¶€ë¥¼ ì´í•´í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ì¸ strideë¼ëŠ” ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ ë©”íƒ€ë°ì´í„°ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í…ì„œ ë°ì´í„° ì¬ë°°ì—´ì˜ ë‚´ë¶€ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì•½ê°„ ë” ì´ì— ëŒ€í•´ ë…¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.



2-D í…ì„œì˜ ëª¨ì–‘ì´ [4, 8]ì¸ ê²½ìš°ë¥¼ ìƒìƒí•´ë³´ì„¸ìš”.

![í…ì„œ](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_2.png)

í…ì„œì˜ ë°ì´í„°(ì¦‰, ë¶€ë™ ì†Œìˆ˜ì  ìˆ˜)ëŠ” ì‹¤ì œë¡œ ë©”ëª¨ë¦¬ì— 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥ë©ë‹ˆë‹¤.

![ë°ì´í„°](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_3.png)



ê·¸ëŸ¬ë©´ ì´ 1ì°¨ì› ë°°ì—´ì„ Nì°¨ì› í…ì„œë¡œ ë‚˜íƒ€ë‚´ë ¤ë©´ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

4í–‰ 8ì—´ì˜ í–‰ë ¬ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ í–‰ë ¬ì˜ ëª¨ë“  ì›ì†Œê°€ 1ì°¨ì› ë°°ì—´ì˜ í–‰ì— ì˜í•´ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•  ë•Œ, ìœ„ì¹˜ [2, 3]ì˜ ê°’ì„ ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´ 2í–‰(ê° í–‰ì— 8ê°œì˜ ìš”ì†Œ)ì„ íš¡ë‹¨í•´ì•¼ í•˜ë©° ì¶”ê°€ë¡œ 3ê°œì˜ ìœ„ì¹˜ë¥¼ ì§€ë‚˜ì•¼ í•©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´ 1ì°¨ì› ë°°ì—´ì—ì„œ 3 + 2 * 8 ìš”ì†Œë¥¼ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ, '8'ì€ ë‘ ë²ˆì§¸ ì°¨ì›ì˜ ìŠ¤íŠ¸ë¼ì´ë“œì…ë‹ˆë‹¤. ì´ ê²½ìš°, ë°°ì—´ì—ì„œ ë‹¤ë¥¸ ìœ„ì¹˜ë¡œ "ì í”„"í•˜ê¸° ìœ„í•´ ëª‡ ê°œì˜ ìš”ì†Œë¥¼ íš¡ë‹¨í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´ì…ë‹ˆë‹¤.



ë”°ë¼ì„œ, ëª¨ì–‘ì´ [shape_0, shape_1]ì¸ 2ì°¨ì› í…ì„œì˜ ìš”ì†Œ [i, j]ì— ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ j + i * shape_1 ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ì œ 3ì°¨ì› í…ì„œë¥¼ ìƒìƒí•´ë³´ê² ìŠµë‹ˆë‹¤:

![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_5.png)

ì´ 3ì°¨ì› í…ì„œë¥¼ í–‰ë ¬ì˜ ì‹œí€€ìŠ¤ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ [5, 4, 8] í…ì„œë¥¼ [4, 8] ëª¨ì–‘ì˜ 5ê°œ í–‰ë ¬ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



ì´ì œ [1, 3, 7] ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ê¸° ìœ„í•´ [4,8] í˜•íƒœì˜ í–‰ë ¬ì„ 1ê°œ ì™„ì „íˆ íš¡ë‹¨í•˜ê³ , [8] í˜•íƒœì˜ í–‰ì„ 2ê°œ, [1] í˜•íƒœì˜ ì—´ì„ 7ê°œ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ 1ì°¨ì› ë°°ì—´ì—ì„œ (1 * 4 * 8) + (2 * 8) + (7 * 1) ìœ„ì¹˜ë¥¼ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.

![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_6.png)

ë”°ë¼ì„œ, [shape_0, shape_1, shape_2] ëª¨ì–‘ì˜ 3ì°¨ì› í…ì„œì—ì„œ 1ì°¨ì› ë°ì´í„° ë°°ì—´ì—ì„œ [i][j][k] ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_7.png)



ì´ shape_1 * shape_2ê°€ ì²« ë²ˆì§¸ ì°¨ì›ì˜ strideì´ê³ , shape_2ëŠ” ë‘ ë²ˆì§¸ ì°¨ì›ì˜ strideì´ë©° 1ì€ ì„¸ ë²ˆì§¸ ì°¨ì›ì˜ strideì…ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ, ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ì„œëŠ”:

![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_8.png)

ê° ì°¨ì›ì˜ strideëŠ” ë‹¤ìŒ ì°¨ì› í…ì„œ ëª¨ì–‘ì˜ ê³±ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:



<img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_9.png" />

ê·¸ëŸ° ë‹¤ìŒ stride[n-1] = 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

ìš°ë¦¬ì˜ í˜•íƒœì˜ í…ì„œ ì˜ˆì œ [5, 4, 8]ì—ì„œ strides = [4*8, 8, 1] = [32, 8, 1]ì¼ ê²ƒì…ë‹ˆë‹¤.

ì—¬ëŸ¬ë¶„ë“¤ë„ ì§ì ‘ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆì–´ìš”:



```js
import torch

torch.rand([5, 4, 8]).stride()
#(32, 8, 1)
```

ì•Œê² ì–´ìš”, ê·¸ëŸ°ë° ì™œ ëª¨ì–‘ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œê°€ í•„ìš”í•œ ê±´ê°€ìš”? Nì°¨ì› í…ì„œì˜ ìš”ì†Œì— ì ‘ê·¼í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ì´ ê°œë…ì€ í…ì„œ ë°°ì—´ì„ ë§¤ìš° ì‰½ê²Œ ì¡°ì‘í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆì–´ìš”.

ì˜ˆë¥¼ ë“¤ì–´, í…ì„œë¥¼ ì¬êµ¬ì„±í•˜ë ¤ë©´ ìƒˆë¡œìš´ ëª¨ì–‘ì„ ì„¤ì •í•˜ê³  ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤! (ìƒˆë¡œìš´ ëª¨ì–‘ì€ ë™ì¼í•œ ìš”ì†Œ ìˆ˜ë¥¼ ë³´ì¥í•˜ë¯€ë¡œ)

```js
import torch

t = torch.rand([5, 4, 8])

print(t.shape)
# [5, 4, 8]

print(t.stride())
# [32, 8, 1]

new_t = t.reshape([4, 5, 2, 2, 2])

print(new_t.shape)
# [4, 5, 2, 2, 2]

print(new_t.stride())
# [40, 8, 4, 2, 1]
``` 




í…ì„œ ë‚´ë¶€ì—ì„œëŠ” ì—¬ì „íˆ ë™ì¼í•œ 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥ë©ë‹ˆë‹¤. reshape ë©”ì„œë“œëŠ” ë°°ì—´ ë‚´ ìš”ì†Œì˜ ìˆœì„œë¥¼ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ëŒ€ë‹¨í•˜ì§€ ì•Šë‚˜ìš”? ğŸ˜

ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì—ì„œ ë‚´ë¶€ 1ì°¨ì› ë°°ì—´ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
import ctypes

def print_internal(t: torch.Tensor):
    print(
        torch.frombuffer(
            ctypes.string_at(t.data_ptr(), t.storage().nbytes()), dtype=t.dtype
        )
    )

print_internal(t)
# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...

print_internal(new_t)
# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...
```

ì˜ˆë¥¼ ë“¤ì–´ ë‘ ì¶•ì„ ì „ì¹˜í•˜ë ¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œ í•´ë‹¹ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ë‹¨ìˆœíˆ ë°”ê¾¸ì–´ ì£¼ë©´ ë©ë‹ˆë‹¤!



```js
t = torch.arange(0, 24).reshape(2, 3, 4)
print(t)
# [[[ 0,  1,  2,  3],
#   [ 4,  5,  6,  7],
#   [ 8,  9, 10, 11]],
 
#  [[12, 13, 14, 15],
#   [16, 17, 18, 19],
#   [20, 21, 22, 23]]]

print(t.shape)
# [2, 3, 4]

print(t.stride())
# [12, 4, 1]

new_t = t.transpose(0, 1)
print(new_t)
# [[[ 0,  1,  2,  3],
#   [12, 13, 14, 15]],

#  [[ 4,  5,  6,  7],
#   [16, 17, 18, 19]],

#  [[ 8,  9, 10, 11],
#   [20, 21, 22, 23]]]

print(new_t.shape)
# [3, 2, 4]

print(new_t.stride())
# [4, 12, 1]
```

ë‚´ë¶€ ë°°ì—´ì„ ì¶œë ¥í•˜ë©´ ë‘ ê°’ ëª¨ë‘ ë™ì¼í•©ë‹ˆë‹¤:

```js
print_internal(t)
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]

print_internal(new_t)
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
```

ê·¸ëŸ¬ë‚˜ new_tì˜ ìŠ¤íŠ¸ë¼ì´ë“œëŠ” ì´ì œ ìœ„ì˜ ì‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ê²ƒì€ í…ì„œê°€ ì´ì œ ì—°ì†ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤. ì¦‰, ë‚´ë¶€ ë°°ì—´ì€ ë™ì¼í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ë‚´ì˜ ê°’ì˜ ìˆœì„œê°€ í…ì„œì˜ ì‹¤ì œ ìˆœì„œì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.



```js
t.is_contiguous()
# True

new_t.is_contiguous()
# False
```

ì´ëŠ” ì—°ì†ë˜ì§€ ì•ŠëŠ” ìš”ì†Œì— ì—°ì†ì ìœ¼ë¡œ ì•¡ì„¸ìŠ¤í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤ (ì‹¤ì œ í…ì„œ ìš”ì†ŒëŠ” ë©”ëª¨ë¦¬ ìƒì—ì„œ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤). ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
new_t_contiguous = new_t.contiguous()

print(new_t_contiguous.is_contiguous())
# True
```

ë‚´ë¶€ ë°°ì—´ì„ ë¶„ì„í•˜ë©´ ì´ì œ ìˆœì„œê°€ ì‹¤ì œ í…ì„œ ìˆœì„œì™€ ì¼ì¹˜í•˜ì—¬ ë” ë‚˜ì€ ë©”ëª¨ë¦¬ ì•¡ì„¸ìŠ¤ íš¨ìœ¨ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:



```js
print(new_t)
# [[[ 0,  1,  2,  3],
#   [12, 13, 14, 15]],

#  [[ 4,  5,  6,  7],
#   [16, 17, 18, 19]],

#  [[ 8,  9, 10, 11],
#   [20, 21, 22, 23]]]

print_internal(new_t)
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]

print_internal(new_t_contiguous)
# [ 0,  1,  2,  3, 12, 13, 14, 15,  4,  5,  6,  7, 16, 17, 18, 19,  8,  9, 10, 11, 20, 21, 22, 23]
```

ì´ì œ ìš°ë¦¬ëŠ” í…ì„œê°€ ì–´ë–»ê²Œ ëª¨ë¸ë§ë˜ëŠ”ì§€ ì´í•´í–ˆìœ¼ë‹ˆ, ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±ì„ ì‹œì‘í•´ ë´…ì‹œë‹¤!

ë‚´ê°€ ë§Œë“¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ë¦„ì€ Norchì…ë‹ˆë‹¤. PyTorchê°€ ì•„ë‹Œ (NOT PyTorch)ì„ ì˜ë¯¸í•˜ë©°, ì„±(Nogueira)ì„ ì•”ì‹œí•˜ê¸°ë„ í•©ë‹ˆë‹¤. ğŸ˜

ì²« ë²ˆì§¸ë¡œ ì•Œì•„ì•¼ í•  ê²ƒì€ PyTorchê°€ Pythonì„ í†µí•´ ì‚¬ìš©ë˜ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œëŠ” C/C++ë¡œ ì‹¤í–‰ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë¨¼ì € ë‚´ë¶€ C/C++ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.




ë¨¼ì € í…ì„œë¥¼ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” êµ¬ì¡°ì²´ë¡œ ì •ì˜í•˜ê³  ì´ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
//norch/csrc/tensor.cpp

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

typedef struct {
    float* data;
    int* strides;
    int* shape;
    int ndim;
    int size;
    char* device;
} Tensor;

Tensor* create_tensor(float* data, int* shape, int ndim) {
    
    Tensor* tensor = (Tensor*)malloc(sizeof(Tensor));
    if (tensor == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }
    tensor->data = data;
    tensor->shape = shape;
    tensor->ndim = ndim;

    tensor->size = 1;
    for (int i = 0; i < ndim; i++) {
        tensor->size *= shape[i];
    }

    tensor->strides = (int*)malloc(ndim * sizeof(int));
    if (tensor->strides == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }
    int stride = 1;
    for (int i = ndim - 1; i >= 0; i--) {
        tensor->strides[i] = stride;
        stride *= shape[i];
    }
    
    return tensor;
}
```

ì¼ë¶€ ìš”ì†Œì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œëŠ” ì•ì„œ ë°°ì› ë˜ ìŠ¤íŠ¸ë¼ì´ë“œ(strides)ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
//norch/csrc/tensor.cpp

float get_item(Tensor* tensor, int* indices) {
    int index = 0;
    for (int i = 0; i < tensor->ndim; i++) {
        index += indices[i] * tensor->strides[i];
    }

    float result;
    result = tensor->data[index];

    return result;
}
```



ì´ì œ í…ì„œ ì‘ì—…ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ ë³´ì—¬ë“œë¦¬ê² ê³ , ì´ ê¸€ ëì— ë§í¬ëœ ì €ì¥ì†Œì—ì„œ ì™„ì „í•œ ë²„ì „ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
//norch/csrc/cpu.cpp

void add_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {
    
    for (int i = 0; i < tensor1->size; i++) {
        result_data[i] = tensor1->data[i] + tensor2->data[i];
    }
}

void sub_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {
    
    for (int i = 0; i < tensor1->size; i++) {
        result_data[i] = tensor1->data[i] - tensor2->data[i];
    }
}

void elementwise_mul_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {
    
    for (int i = 0; i < tensor1->size; i++) {
        result_data[i] = tensor1->data[i] * tensor2->data[i];
    }
}

void assign_tensor_cpu(Tensor* tensor, float* result_data) {

    for (int i = 0; i < tensor->size; i++) {
        result_data[i] = tensor->data[i];
    }
}

...
```

ê·¸ ë‹¤ìŒì—, ì´ëŸ¬í•œ ì‘ì—…ë“¤ì„ í˜¸ì¶œí•  í…ì„œ ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
//norch/csrc/tensor.cpp

Tensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {
    if (tensor1->ndim != tensor2->ndim) {
        fprintf(stderr, "ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œëŠ” ë™ì¼í•œ ì°¨ì› ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ %d ì™€ %d\n", tensor1->ndim, tensor2->ndim);
        exit(1);
    }

    int ndim = tensor1->ndim;
    int* shape = (int*)malloc(ndim * sizeof(int));
    if (shape == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }

    for (int i = 0; i < ndim; i++) {
        if (tensor1->shape[i] != tensor2->shape[i]) {
            fprintf(stderr, "ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œëŠ” ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤ %d ì™€ %d ì¸ë±ìŠ¤ %dì—ì„œ\n", tensor1->shape[i], tensor2->shape[i], i);
            exit(1);
        }
        shape[i] = tensor1->shape[i];
    }        
    float* result_data = (float*)malloc(tensor1->size * sizeof(float));
    if (result_data == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }
    add_tensor_cpu(tensor1, tensor2, result_data);
    
    return create_tensor(result_data, shape, ndim, device);
}
```



ì´ì „ì— ì–¸ê¸‰í•œ ëŒ€ë¡œ, í…ì„œ ì¬êµ¬ì„±ì€ ë‚´ë¶€ ë°ì´í„° ë°°ì—´ì„ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```js
//norch/csrc/tensor.cpp

Tensor* reshape_tensor(Tensor* tensor, int* new_shape, int new_ndim) {

    int ndim = new_ndim;
    int* shape = (int*)malloc(ndim * sizeof(int));
    if (shape == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }

    for (int i = 0; i < ndim; i++) {
        shape[i] = new_shape[i];
    }

    // ìƒˆ ëª¨ì–‘ì˜ ìš”ì†Œ ì´ ìˆ˜ ê³„ì‚°
    int size = 1;
    for (int i = 0; i < new_ndim; i++) {
        size *= shape[i];
    }

    // ì´ ìš”ì†Œ ìˆ˜ê°€ í˜„ì¬ í…ì„œì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    if (size != tensor->size) {
        fprintf(stderr, "í…ì„œë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ì–‘ì˜ ìš”ì†Œ ì´ ìˆ˜ê°€ í˜„ì¬ í…ì„œì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n");
        exit(1);
    }

    float* result_data = (float*)malloc(tensor->size * sizeof(float));
    if (result_data == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }
    assign_tensor_cpu(tensor, result_data);
    return create_tensor(result_data, shape, ndim, device);
}
```

ì´ì œ ì¼ë¶€ í…ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ, ëˆ„êµ¬ë‚˜ C/C++ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì´ì œ Python ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!

Pythonì„ ì‚¬ìš©í•˜ì—¬ C/C++ ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤. Pybind11ê³¼ Cython ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œëŠ” ctypesë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.



ì•„ë˜ëŠ” ctypesì˜ ê¸°ë³¸ì ì¸ êµ¬ì¡°ì…ë‹ˆë‹¤:

```js
//C ì½”ë“œ
#include <stdio.h>

float add_floats(float a, float b) {
    return a + b;
}
```

```js
# ì»´íŒŒì¼
gcc -shared -o add_floats.so -fPIC add_floats.c
```

```js
# Python ì½”ë“œ
import ctypes

# ê³µìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
lib = ctypes.CDLL('./add_floats.so')

# í•¨ìˆ˜ì˜ ì¸ìì™€ ë°˜í™˜ ìœ í˜• ì •ì˜
lib.add_floats.argtypes = [ctypes.c_float, ctypes.c_float]
lib.add_floats.restype = ctypes.c_float

# íŒŒì´ì¬ float ê°’ì„ c_float ìœ í˜•ìœ¼ë¡œ ë³€í™˜
a = ctypes.c_float(3.5)
b = ctypes.c_float(2.2)

# C í•¨ìˆ˜ í˜¸ì¶œ
result = lib.add_floats(a, b)
print(result)
# 5.7
```



ë³´ì‹œë‹¤ì‹œí”¼ ë§¤ìš° ì§ê´€ì ì…ë‹ˆë‹¤. C/C++ ì½”ë“œë¥¼ ì»´íŒŒì¼í•œ í›„ Pythonì—ì„œ ctypesë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ ë° ë°˜í™˜ c_typesë¥¼ ì •ì˜í•˜ê³ , ë³€ìˆ˜ë¥¼ í•´ë‹¹ c_typesë¡œ ë³€í™˜í•˜ê³  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ë°°ì—´(ë¶€ë™ ì†Œìˆ˜ì  ëª©ë¡)ê³¼ ê°™ì€ ë³´ë‹¤ ë³µì¡í•œ ìœ í˜•ì˜ ê²½ìš° í¬ì¸í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
data = [1.0, 2.0, 3.0]
data_ctype = (ctypes.c_float * len(data))(*data)

lib.some_array_func.argstypes = [ctypes.POINTER(ctypes.c_float)]

...

lib.some_array_func(data)
```

ê·¸ë¦¬ê³  êµ¬ì¡°ì²´ ìœ í˜•ì˜ ê²½ìš° ì§ì ‘ c_typeì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
class CustomType(ctypes.Structure):
    _fields_ = [
        ('field1', ctypes.POINTER(ctypes.c_float)),
        ('field2', ctypes.POINTER(ctypes.c_int)),
        ('field3', ctypes.c_int),
    ]

# ctypes.POINTER(CustomType)ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```



ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³ , í…ì„œ C/C++ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìœ„í•œ Python ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤!

```js
# norch/tensor.py

import ctypes

class CTensor(ctypes.Structure):
    _fields_ = [
        ('data', ctypes.POINTER(ctypes.c_float)),
        ('strides', ctypes.POINTER(ctypes.c_int)),
        ('shape', ctypes.POINTER(ctypes.c_int)),
        ('ndim', ctypes.c_int),
        ('size', ctypes.c_int),
    ]

class Tensor:
    os.path.abspath(os.curdir)
    _C = ctypes.CDLL("COMPILED_LIB.so")

    def __init__(self):
        
        data, shape = self.flatten(data)
        self.data_ctype = (ctypes.c_float * len(data))(*data)
        self.shape_ctype = (ctypes.c_int * len(shape))(*shape)
        self.ndim_ctype = ctypes.c_int(len(shape))
       
        self.shape = shape
        self.ndim = len(shape)

        Tensor._C.create_tensor.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_int), ctypes.c_int]
        Tensor._C.create_tensor.restype = ctypes.POINTER(CTensor)

        self.tensor = Tensor._C.create_tensor(
            self.data_ctype,
            self.shape_ctype,
            self.ndim_ctype,
        )
        
    def flatten(self, nested_list):
        """
        This method simply convert a list type tensor to a flatten tensor with its shape
        
        Example:
        
        Arguments:  
            nested_list: [[1, 2, 3], [-5, 2, 0]]
        Return:
            flat_data: [1, 2, 3, -5, 2, 0]
            shape: [2, 3]
        """
        def flatten_recursively(nested_list):
            flat_data = []
            shape = []
            if isinstance(nested_list, list):
                for sublist in nested_list:
                    inner_data, inner_shape = flatten_recursively(sublist)
                    flat_data.extend(inner_data)
                shape.append(len(nested_list))
                shape.extend(inner_shape)
            else:
                flat_data.append(nested_list)
            return flat_data, shape
        
        flat_data, shape = flatten_recursively(nested_list)
        return flat_data, shape
```

ì´ì œ Python í…ì„œ ì‘ì—…ì„ í¬í•¨í•˜ì—¬ C/C++ ì‘ì—…ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
# norch/tensor.py

def __getitem__(self, indices):
    """
    index í…ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ì„œì— ì•¡ì„¸ìŠ¤ tensor[i, j, k...]
    """

    if len(indices) != self.ndim:
        raise ValueError("ì¸ë±ìŠ¤ ìˆ˜ê°€ ì°¨ì› ìˆ˜ì™€ ì¼ì¹˜í•´ì•¼ í•¨")
    
    Tensor._C.get_item.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int)]
    Tensor._C.get_item.restype = ctypes.c_float
                                       
    indices = (ctypes.c_int * len(indices))(*indices)
    value = Tensor._C.get_item(self.tensor, indices)  
    
    return value

def reshape(self, new_shape):
    """
    í…ì„œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤
    result = tensor.reshape([1,2])
    """
    new_shape_ctype = (ctypes.c_int * len(new_shape))(*new_shape)
    new_ndim_ctype = ctypes.c_int(len(new_shape))
    
    Tensor._C.reshape_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int), ctypes.c_int]
    Tensor._C.reshape_tensor.restype = ctypes.POINTER(CTensor)
    result_tensor_ptr = Tensor._C.reshape_tensor(self.tensor, new_shape_ctype, new_ndim_ctype)   

    result_data = Tensor()
    result_data.tensor = result_tensor_ptr
    result_data.shape = new_shape.copy()
    result_data.ndim = len(new_shape)
    result_data.device = self.device

    return result_data

def __add__(self, other):
    """
    í…ì„œë¥¼ ë”í•©ë‹ˆë‹¤
    result = tensor1 + tensor2
    """
  
    if self.shape != other.shape:
        raise ValueError("ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œë“¤ì€ ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•¨")
    
    Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]
    Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)

    result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)

    result_data = Tensor()
    result_data.tensor = result_tensor_ptr
    result_data.shape = self.shape.copy()
    result_data.ndim = self.ndim
    result_data.device = self.device

    return result_data

# ê¸°íƒ€ ì—°ì‚° í¬í•¨:
# __str__
# __sub__ (-)
# __mul__ (*)
# __matmul__ (@)
# __pow__ (**)
# __truediv__ (/)
# log
# ...
```



ì—¬ê¸°ê¹Œì§€ ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ì œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  í…ì„œ ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ìƒê²¼ìŠµë‹ˆë‹¤!

```js
import norch

tensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]])
tensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]])

result = tensor1 + tensor2
print(result[0, 0])
# 4 
```

# #2 â€” GPU ì§€ì›

ìš°ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë§Œë“  í›„, ì´ì œ ìƒˆë¡œìš´ ìˆ˜ì¤€ìœ¼ë¡œ ëŒì–´ì˜¬ë¦´ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ GPUë¡œ ì „ì†¡í•˜ê³  ìˆ˜í•™ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ `.to("cuda")`ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ì˜ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. CUDAê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê¸°ë³¸ ì§€ì‹ì´ ìˆì„ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ë‹¤ë¥¸ ê¸°ì‚¬ì¸ 'CUDA íŠœí† ë¦¬ì–¼'ì„ ì½ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ê²Œìš”. ğŸ˜Š



...

ê¸‰í•œ ì‚¬ëŒë“¤ì„ ìœ„í•´, ê°„ë‹¨í•œ ì†Œê°œê°€ ì—¬ê¸° ìˆì–´ìš”:

ê¸°ë³¸ì ìœ¼ë¡œ, ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ì½”ë“œëŠ” CPU ë©”ëª¨ë¦¬ì—ì„œ ì‹¤í–‰ë˜ê³  ìˆì–´ìš”. í•˜ë‚˜ì˜ ì‘ì—…ì— ëŒ€í•´ì„œëŠ” CPUê°€ ë¹ ë¥´ì§€ë§Œ, GPUì˜ ì¥ì ì€ ë³‘ë ¬í™” ëŠ¥ë ¥ì— ìˆì–´ìš”. CPU ë””ìì¸ì€ ì—°ì‚°(ìŠ¤ë ˆë“œ)ì„ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ë„ë¡ ëª©í‘œë¥¼ í•œ ë°˜ë©´, GPU ë””ìì¸ì€ ìˆ˜ë°±ë§Œ ê°œì˜ ì—°ì‚°ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ë„ë¡ ëª©í‘œë¥¼ í•´ìš” (ê°œë³„ ìŠ¤ë ˆë“œì˜ ì„±ëŠ¥ì„ í¬ìƒí•˜ë©°).

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì´ ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë³‘ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ë°±ë§Œ ê°œì˜ ìš”ì†Œë¡œ êµ¬ì„±ëœ í…ì„œë¥¼ ì¶”ê°€í•  ë•Œ, ë°˜ë³µë¬¸ ë‚´ì—ì„œ ê° ìƒ‰ì¸ì˜ ìš”ì†Œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹ , GPUë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêº¼ë²ˆì— ëª¨ë‘ë¥¼ ë³‘ë ¬ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”. ì´ë¥¼ ìœ„í•´ NVIDIAì—ì„œ ê°œë°œí•œ ê°œë°œìë“¤ì´ GPU ì§€ì›ì„ ì†Œí”„íŠ¸ì›¨ì–´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í”Œë«í¼ì¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”.



ê·¸ê±¸ í•˜ë ¤ë©´, íŠ¹ì • GPU ì‘ì—…(ì˜ˆ: CPU ë©”ëª¨ë¦¬ì—ì„œ GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ë³µì‚¬)ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ê°„ë‹¨í•œ C/C++ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤ ì¸ CUDA C/C++ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPUì—ì„œ GPUë¡œ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ê³  ë°°ì—´ì˜ ê° ìš”ì†Œë¥¼ ì¶”ê°€í•˜ëŠ” AddTwoArrays í•¨ìˆ˜(ì»¤ë„ì´ë¼ê³ ë„ í•¨)ë¥¼ Nê°œì˜ GPU ìŠ¤ë ˆë“œì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ëª‡ ê°€ì§€ CUDA C/C++ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```c
#include <stdio.h>

// CPU ë²„ì „(ë¹„êµìš©)
void AddTwoArrays_CPU(flaot A[], float B[], float C[]) {
    for (int i = 0; i < N; i++) {
        C[i] = A[i] + B[i];
    }
}

// ì»¤ë„ ì •ì˜
__global__ void AddTwoArrays_GPU(float A[], float B[], float C[]) {
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main() {

    int N = 1000; // ë°°ì—´ í¬ê¸°
    float A[N], B[N], C[N]; // ë°°ì—´ A, B, C

    ...

    float *d_A, *d_B, *d_C; // ë°°ì—´ A, B, Cì˜ ì¥ì¹˜ í¬ì¸í„°

    // ë°°ì—´ A, B, Cì— ëŒ€í•œ ì¥ì¹˜ì—ì„œì˜ ë©”ëª¨ë¦¬ í• ë‹¹
    cudaMalloc((void **)&d_A, N * sizeof(float));
    cudaMalloc((void **)&d_B, N * sizeof(float));
    cudaMalloc((void **)&d_C, N * sizeof(float));

    // í˜¸ìŠ¤íŠ¸ì—ì„œ ì¥ì¹˜ë¡œ ë°°ì—´ A ë° B ë³µì‚¬
    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);

    // Nê°œì˜ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ë„ í˜¸ì¶œ
    AddTwoArrays_GPU<<<1, N>>>(d_A, d_B, d_C);
    
    // ì¥ì¹˜ì—ì„œ í˜¸ìŠ¤íŠ¸ë¡œ ë²¡í„° C ë³µì‚¬
    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);

}
```

ì£¼ëª©í•  ì ì€ ê° ìš”ì†Œ ìŒì„ ê°ê° ì¶”ê°€í•˜ëŠ” ëŒ€ì‹  ëª¨ë“  ë§ì…ˆ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë£¨í”„ ëª…ë ¹ì„ ì œê±°í•œ ê²ƒì…ë‹ˆë‹¤.



ê°„ë‹¨í•œ ì†Œê°œ ì´í›„ì—, í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆì–´ìš”.

ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” CPUì—ì„œ GPUë¡œ í…ì„œ ë°ì´í„°ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

```js
//norch/csrc/tensor.cpp

void to_device(Tensor* tensor, char* target_device) {
    if ((strcmp(target_device, "cuda") == 0) && (strcmp(tensor->device, "cpu") == 0)) {
        cpu_to_cuda(tensor);
    }

    else if ((strcmp(target_device, "cpu") == 0) && (strcmp(tensor->device, "cuda") == 0)) {
        cuda_to_cpu(tensor);
    }
}
```

```js
//norch/csrc/cuda.cu

__host__ void cpu_to_cuda(Tensor* tensor) {
    
    float* data_tmp;
    cudaMalloc((void **)&data_tmp, tensor->size * sizeof(float));
    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyHostToDevice);

    tensor->data = data_tmp;

    const char* device_str = "cuda";
    tensor->device = (char*)malloc(strlen(device_str) + 1);
    strcpy(tensor->device, device_str); 

    printf("í…ì„œê°€ ì„±ê³µì ìœ¼ë¡œ %së¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.\n", tensor->device);
}

__host__ void cuda_to_cpu(Tensor* tensor) {
    float* data_tmp = (float*)malloc(tensor->size * sizeof(float));

    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyDeviceToHost);
    cudaFree(tensor->data);

    tensor->data = data_tmp;

    const char* device_str = "cpu";
    tensor->device = (char*)malloc(strlen(device_str) + 1);
    strcpy(tensor->device, device_str); 

    printf("í…ì„œê°€ ì„±ê³µì ìœ¼ë¡œ %së¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.\n", tensor->device);
}
```



íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„ëœ ë˜í¼:

```js
# norch/tensor.py

def to(self, device):
    self.device = device
    self.device_ctype = self.device.encode('utf-8')
  
    Tensor._C.to_device.argtypes = [ctypes.POINTER(CTensor), ctypes.c_char_p]
    Tensor._C.to_device.restype = None
    Tensor._C.to_device(self.tensor, self.device_ctype)
  
    return self
```

ë‹¤ìŒìœ¼ë¡œ, ëª¨ë“  í…ì„œ ì—°ì‚°ì— ëŒ€í•´ GPU ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ì…ˆê³¼ ëº„ì…ˆì— ëŒ€í•œ ì˜ˆì œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤:

```js
//norch/csrc/cuda.cu

#define THREADS_PER_BLOCK 128

__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {
    
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < size) {
        result_data[i] = data1[i] + data2[i];
    }
}

__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {
    
    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    add_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);

    cudaError_t error = cudaGetLastError();
    if (error != cudaSuccess) {
        printf("CUDA error: %s\n", cudaGetErrorString(error));
        exit(-1);
    }

    cudaDeviceSynchronize();
}

__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {
   
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < size) {
        result_data[i] = data1[i] - data2[i];
    }
}

__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {
    
    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    sub_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);

    cudaError_t error = cudaGetLastError();
    if (error != cudaSuccess) {
        printf("CUDA error: %s\n", cudaGetErrorString(error));
        exit(-1);
    }

    cudaDeviceSynchronize();
}

...
```



ê·¸ëŸ° ë‹¤ìŒ, í…ì„œ.cppì— ìƒˆë¡œìš´ í…ì„œ ì†ì„± char* deviceë¥¼ ì¶”ê°€í•˜ê³  ì‘ì—…ì„ ì‹¤í–‰í•  ìœ„ì¹˜(CPU ë˜ëŠ” GPU)ë¥¼ ì„ íƒí•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
//norch/csrc/tensor.cpp

Tensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {
    if (tensor1->ndim != tensor2->ndim) {
        fprintf(stderr, "ë§ì…ˆì„ ìœ„í•´ í…ì„œê°€ ë™ì¼í•œ ì°¨ì› ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ %d and %d\n", tensor1->ndim, tensor2->ndim);
        exit(1);
    }

    if (strcmp(tensor1->device, tensor2->device) != 0) {
        fprintf(stderr, "í…ì„œëŠ” ë™ì¼í•œ ì¥ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤: %s and %s\n", tensor1->device, tensor2->device);
        exit(1);
    }

    char* device = (char*)malloc(strlen(tensor1->device) + 1);
    if (device != NULL) {
        strcpy(device, tensor1->device);
    } else {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(-1);
    }
    int ndim = tensor1->ndim;
    int* shape = (int*)malloc(ndim * sizeof(int));
    if (shape == NULL) {
        fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
        exit(1);
    }

    for (int i = 0; i < ndim; i++) {
        if (tensor1->shape[i] != tensor2->shape[i]) {
            fprintf(stderr, "ë§ì…ˆì„ ìœ„í•´ í…ì„œë“¤ì€ ìƒ‰ì¸ %dì—ì„œ ë™ì¼í•œ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤ %d and %d\n", i, tensor1->shape[i], tensor2->shape[i]);
            exit(1);
        }
        shape[i] = tensor1->shape[i];
    }        

    if (strcmp(tensor1->device, "cuda") == 0) {

        float* result_data;
        cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));
        add_tensor_cuda(tensor1, tensor2, result_data);
        return create_tensor(result_data, shape, ndim, device);
    } 
    else {
        float* result_data = (float*)malloc(tensor1->size * sizeof(float));
        if (result_data == NULL) {
            fprintf(stderr, "ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\n");
            exit(1);
        }
        add_tensor_cpu(tensor1, tensor2, result_data);
        return create_tensor(result_data, shape, ndim, device);
    }     
}
```

ì´ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ GPU ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤!

```js
import norch

tensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]]).to("cuda")
tensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]]).to("cuda")

result = tensor1 + tensor2
```



# #3 â€” Automatic Differentiation (Autograd)

íŒŒì´í† ì¹˜ê°€ ì¸ê¸°ë¥¼ ì–»ê²Œ ëœ ì£¼ìš” ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” Autograd ëª¨ë“ˆ ë•Œë¬¸ì…ë‹ˆë‹¤. Autograd ëª¨ë“ˆì€ ìë™ ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤ (ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤). .backward()ë¼ëŠ” ë‹¨ì¼ ë©”ì„œë“œ í˜¸ì¶œë¡œ ì´ì „ í…ì„œ ì—°ì‚°ì—ì„œ ëª¨ë“  ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:

```js
x = torch.tensor([[1., 2, 3], [3., 2, 1]], requires_grad=True)
# [[1,  2,  3],
#  [3,  2., 1]]

y = torch.tensor([[3., 2, 1], [1., 2, 3]], requires_grad=True)
# [[3,  2, 1],
#  [1,  2, 3]]

L = ((x - y) ** 3).sum()

L.backward()

# xì™€ yì˜ ê¸°ìš¸ê¸°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
print(x.grad)
# [[12, 0, 12],
#  [12, 0, 12]]

print(y.grad)
# [[-12, 0, -12],
#  [-12, 0, -12]]

# zë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
# x = x - í•™ìŠµë¥  * x.grad
# y = y - í•™ìŠµë¥  * y.grad
```

ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ ì´í•´í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ì ˆì°¨ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•´ë³´ê² ìŠµë‹ˆë‹¤:



<img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_10.png" />

ìš°ì„  ê³„ì‚°í•´ ë´…ì‹œë‹¤:

<img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_11.png" />

xê°€ í–‰ë ¬ì´ë¼ëŠ” ê²ƒì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê° ìš”ì†Œì— ëŒ€í•œ Lì˜ ë¯¸ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. ê²Œë‹¤ê°€, Lì€ ëª¨ë“  ìš”ì†Œì— ëŒ€í•œ í•©ì´ì§€ë§Œ ê° ìš”ì†Œì— ëŒ€í•œ ë¯¸ë¶„ì—ì„œ ë‹¤ë¥¸ ìš”ì†Œë“¤ì€ ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•­ì„ ì–»ìŠµë‹ˆë‹¤:




![ì´ë¯¸ì§€](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_12.png)

ê° í•­ì— ëŒ€í•´ ì—°ì‡„ ë²•ì¹™ì„ ì ìš©í•˜ì—¬ ì™¸ë¶€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ê³  ë‚´ë¶€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•œ ê°’ì„ ê³±í•©ë‹ˆë‹¤:

![ì´ë¯¸ì§€](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_13.png)

Where:




ë§ˆì¹¨ë‚´:

![ì´ë¯¸ì§€](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_14.png)

ê·¸ëŸ¬ë¯€ë¡œ, xì— ê´€í•œ Lì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ëŠ” ìµœì¢… ë°©ì •ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:



ì•„ë˜ëŠ” Markdown í˜•ì‹ìœ¼ë¡œ ë³€ê²½ëœ ë‚´ìš©ì…ë‹ˆë‹¤.


![Image 1](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_16.png)

Substituting the values into the equation:

![Image 2](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_17.png)

Calculating the result, we get the same values we obtained with PyTorch:





![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_18.png)

Now, letâ€™s analyze what we just did:

Basically, we observed all the operations involved in reverse order: a summation, a power of 3, and a subtraction. Then, we applied the chain rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:

For addition:





![Image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_19.png)

```js
# norch/autograd/functions.py

class AddBackward:
    def __init__(self, x, y):
        self.input = [x, y]

    def backward(self, gradient):
        return [gradient, gradient]
```

For sin:

![Image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_20.png)




```js
# norch/autograd/functions.py

class SinBackward:
    def __init__(self, x):
        self.input = [x]

    def backward(self, gradient):
        x = self.input[0]
        return [x.cos() * gradient]
```

ì½”ì‚¬ì¸ì— ëŒ€í•´:

![2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21.png)

```js
# norch/autograd/functions.py

class CosBackward:
    def __init__(self, x):
        self.input = [x]

    def backward(self, gradient):
        x = self.input[0]
        return [- x.sin() * gradient]
```



ìš”ì†Œë³„ ê³±ì…ˆì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”:

![element-wise multiplication](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_22.png)

```python
# norch/autograd/functions.py

class ElementwiseMulBackward:
    def __init__(self, x, y):
        self.input = [x, y]

    def backward(self, gradient):
        x = self.input[0]
        y = self.input[1]
        return [y * gradient, x * gradient]
```

í•©ì‚°ì— ëŒ€í•´ì„œ:




# norch/autograd/functions.py

```python
class SumBackward:
    def __init__(self, x):
        self.input = [x]

    def backward(self, gradient):
        # sum í•¨ìˆ˜ëŠ” í…ì„œë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ì¤„ì´ë¯€ë¡œ ê¸°ìš¸ê¸°ë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ ë¸Œë¡œë“œìºìŠ¤íŠ¸ë©ë‹ˆë‹¤.
        return [float(gradient.tensor.contents.data[0]) * self.input[0].ones_like()]
```

ë‹¤ë¥¸ ì—°ì‚°ì„ ì‚´í´ë³¼ ìˆ˜ ìˆëŠ” GitHub ì €ì¥ì†Œ ë§í¬ë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ê° ì‘ì—…ì— ëŒ€í•œ ë„í•¨ìˆ˜ ì‹ì„ ê°€ì¡Œìœ¼ë‹ˆ, ì¬ê·€ì ìœ¼ë¡œ ì—­ì „íŒŒ ì²´ì¸ ê·œì¹™ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ì„œì— requires_grad ì¸ìë¥¼ ì„¤ì •í•˜ì—¬ ì´ í…ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•˜ë ¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Trueì´ë©´ ê° í…ì„œ ì‘ì—…ì˜ ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

```python
# norch/tensor.py

def __add__(self, other):

  if self.shape != other.shape:
      raise ValueError("ë§ì…ˆì„ ìœ„í•´ í…ì„œëŠ” ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
  
  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]
  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)
  
  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)
  
  result_data = Tensor()
  result_data.tensor = result_tensor_ptr
  result_data.shape = self.shape.copy()
  result_data.ndim = self.ndim
  result_data.device = self.device
  
  result_data.requires_grad = self.requires_grad or other.requires_grad
  if result_data.requires_grad:
      result_data.grad_fn = AddBackward(self, other)
```



ê·¸ëŸ¼, `.backward()` ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”:

```python
# norch/tensor.py

def backward(self, gradient=None):
    if not self.requires_grad:
        return
    
    if gradient is None:
        if self.shape == [1]:
            gradient = Tensor([1]) # dx/dx = 1 case
        else:
            raise RuntimeError("Gradient argument must be specified for non-scalar tensors.")

    if self.grad is None:
        self.grad = gradient

    else:
        self.grad += gradient

    if self.grad_fn is not None: # not a leaf
        grads = self.grad_fn.backward(gradient) # call the operation backward
        for tensor, grad in zip(self.grad_fn.input, grads):
            if isinstance(tensor, Tensor):
                tensor.backward(grad) # recursively call the backward again for the gradient expression (chain rule)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, í…ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì œë¡œí™”í•˜ëŠ” `.zero_grad()`ì™€ í…ì„œì˜ ì˜¤í† ê·¸ë˜ë“œ íˆìŠ¤í† ë¦¬ë¥¼ ì œê±°í•˜ëŠ” `.detach()`ë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”:

```python
# norch/tensor.py

def zero_grad(self):
    self.grad = None

def detach(self):
    self.grad = None
    self.grad_fn = None
```



ì¶•í•˜í•©ë‹ˆë‹¤! GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ì´ ìˆëŠ” ì™„ì „í•œ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§Œë“œì…¨êµ°ìš”! ì´ì œ nn ë° optim ëª¨ë“ˆì„ ë§Œë“¤ì–´ ëª‡ ê°€ì§€ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ ë” ì‰½ê²Œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## #4 â€” nn ë° optim ëª¨ë“ˆ

nnì€ ì‹ ê²½ë§ ë° ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ëª¨ë“ˆì´ë©°, optimì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì„ ì¬í˜„í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” Parameterë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ParameterëŠ” ê°„ë‹¨íˆ ë§í•´ í•­ìƒ Trueë¡œ ì„¤ì •ëœ requires_grad ì†ì„±ì„ ê°–ëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ í…ì„œë¡œ, ì¼ë¶€ ì„ì˜ì˜ ì´ˆê¸°í™” ê¸°ë²•ì„ ì‚¬ìš©í•´ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```js
# norch/nn/parameter.py

from norch.tensor import Tensor
from norch.utils import utils
import random

class Parameter(Tensor):
    """
    A parameter is a trainable tensor.
    """
    def __init__(self, shape):
        data = utils.generate_random_list(shape=shape)
        super().__init__(data, requires_grad=True)
```



```js
# norch/utisl/utils.py

def generate_random_list(shape):
    """
    ëœë¤í•œ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ 'shape' í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
    [4, 2] --> [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]
    """
    if len(shape) == 0:
        return []
    else:
        inner_shape = shape[1:]
        if len(inner_shape) == 0:
            return [random.uniform(-1, 1) for _ in range(shape[0])]
        else:
            return [generate_random_list(inner_shape) for _ in range(shape[0])]
```

íŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•˜ë©´ ëª¨ë“ˆì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
# norch/nn/module.py

from .parameter import Parameter
from collections import OrderedDict
from abc import ABC
import inspect

class Module(ABC):
    """
    ëª¨ë“ˆì„ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤
    """
    def __init__(self):
        self._modules = OrderedDict()
        self._params = OrderedDict()
        self._grads = OrderedDict()
        self.training = True

    def forward(self, *inputs, **kwargs):
        raise NotImplementedError

    def __call__(self, *inputs, **kwargs):
        return self.forward(*inputs, **kwargs)

    def train(self):
        self.training = True
        for param in self.parameters():
            param.requires_grad = True

    def eval(self):
        self.training = False
        for param in self.parameters():
            param.requires_grad = False

    def parameters(self):
        for name, value in inspect.getmembers(self):
            if isinstance(value, Parameter):
                yield self, name, value
            elif isinstance(value, Module):
                yield from value.parameters()

    def modules(self):
        yield from self._modules.values()

    def gradients(self):
        for module in self.modules():
            yield module._grads

    def zero_grad(self):
        for _, _, parameter in self.parameters():
            parameter.zero_grad()

    def to(self, device):
        for _, _, parameter in self.parameters():
            parameter.to(device)

        return self
    
    def inner_repr(self):
        return ""

    def __repr__(self):
        string = f"{self.get_name()}("
        tab = "   "
        modules = self._modules
        if modules == {}:
            string += f'\n{tab}(parameters): {self.inner_repr()}'
        else:
            for key, module in modules.items():
                string += f"\n{tab}({key}): {module.get_name()}({module.inner_repr()})"
        return f'{string}\n)'
    
    def get_name(self):
        return self.__class__.__name__
    
    def __setattr__(self, key, value):
        self.__dict__[key] = value

        if isinstance(value, Module):
            self._modules[key] = value
        elif isinstance(value, Parameter):
            self._params[key] = value
```

ì˜ˆë¥¼ ë“¤ì–´, nn.Moduleì„ ìƒì†í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆì„ ë§Œë“¤ê±°ë‚˜, ì´ì „ì— ìƒì„±ëœ ëª¨ë“ˆ ì¤‘ í•˜ë‚˜ì¸ ì„ í˜• ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ y = Wx + b ì‘ì—…ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.




```js
# norch/nn/modules/linear.py

from ..module import Module
from ..parameter import Parameter

class Linear(Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.weight = Parameter(shape=[self.output_dim, self.input_dim])
        self.bias = Parameter(shape=[self.output_dim, 1])

    def forward(self, x):
        z = self.weight @ x + self.bias
        return z

    def inner_repr(self):
        return f"input_dim={self.input_dim}, output_dim={self.output_dim}, " \
               f"bias={True if self.bias is not None else False}"
```

ì´ì œ ëª‡ ê°€ì§€ ì†ì‹¤ ë° í™œì„±í™” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í‰ê·  ì œê³± ì˜¤ì°¨ ì†ì‹¤ ë° ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜:

```js
# norch/nn/loss.py

from .module import Module
 
class MSELoss(Module):
    def __init__(self):
      pass

    def forward(self, predictions, labels):
        assert labels.shape == predictions.shape, \
            "Labels and predictions shape does not match: {} and {}".format(labels.shape, predictions.shape)
        
        return ((predictions - labels) ** 2).sum() / predictions.numel

    def __call__(self, *inputs):
        return self.forward(*inputs)
```

```js
# norch/nn/activation.py

from .module import Module
import math

class Sigmoid(Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 1.0 / (1.0 + (math.e) ** (-x)) 
```



ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤. ì˜ˆì‹œë¡œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent) ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤:

```js
# norch/optim/optimizer.py

from abc import ABC
from norch.tensor import Tensor

class Optimizer(ABC):
    """
    ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤
    """

    def __init__(self, parameters):
        if isinstance(parameters, Tensor):
            raise TypeError("parametersëŠ” ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ì´ì–´ì•¼ í•˜ì§€ë§Œ {} íƒ€ì…ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤".format(type(parameters)))
        elif isinstance(parameters, dict):
            parameters = parameters.values()

        self.parameters = list(parameters)

    def step(self):
        raise NotImplementedError
    
    def zero_grad(self):
        for module, name, parameter in self.parameters:
            parameter.zero_grad()


class SGD(Optimizer):
    def __init__(self, parameters, lr=1e-1, momentum=0):
        super().__init__(parameters)
        self.lr = lr
        self.momentum = momentum
        self._cache = {'velocity': [p.zeros_like() for (_, _, p) in self.parameters]}

    def step(self):
        for i, (module, name, _) in enumerate(self.parameters):
            parameter = getattr(module, name)

            velocity = self._cache['velocity'][i]

            velocity = self.momentum * velocity - self.lr * parameter.grad

            updated_parameter = parameter + velocity

            setattr(module, name, updated_parameter)

            self._cache['velocity'][i] = velocity

            parameter.detach()
            velocity.detach()
```

ê·¸ë¦¬ê³  ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤! ì´ì œ ìš°ë¦¬ë§Œì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ë§Œë“¤ì—ˆì–´ìš”! ğŸ¥³

ì´ì œ í•™ìŠµì„ ì‹œì‘í•´ë´…ì‹œë‹¤:



```js
import norch
import norch.nn as nn
import norch.optim as optim
import random
import math

random.seed(1)

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(1, 10)
        self.sigmoid = nn.Sigmoid()
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.sigmoid(out)
        out = self.fc2(out)
        
        return out

device = "cuda"
epochs = 10

model = MyModel().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)
loss_list = []

x_values = [0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ,
        4.4,  4.8,  5.2,  5.6,  6. ,  6.4,  6.8,  7.2,  7.6,  8. ,  8.4,
        8.8,  9.2,  9.6, 10. , 10.4, 10.8, 11.2, 11.6, 12. , 12.4, 12.8,
       13.2, 13.6, 14. , 14.4, 14.8, 15.2, 15.6, 16. , 16.4, 16.8, 17.2,
       17.6, 18. , 18.4, 18.8, 19.2, 19.6, 20.]

y_true = []
for x in x_values:
    y_true.append(math.pow(math.sin(x), 2))


for epoch in range(epochs):
    for x, target in zip(x_values, y_true):
        x = norch.Tensor([[x]]).T
        target = norch.Tensor([[target]]).T

        x = x.to(device)
        target = target.to(device)

        outputs = model(x)
        loss = criterion(outputs, target)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}')
    loss_list.append(loss[0])

# Epoch [1/10], Loss: 1.7035
# Epoch [2/10], Loss: 0.7193
# Epoch [3/10], Loss: 0.3068
# Epoch [4/10], Loss: 0.1742
# Epoch [5/10], Loss: 0.1342
# Epoch [6/10], Loss: 0.1232
# Epoch [7/10], Loss: 0.1220
# Epoch [8/10], Loss: 0.1241
# Epoch [9/10], Loss: 0.1270
# Epoch [10/10], Loss: 0.1297
```

<img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_23.png" />

ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±ë˜ê³  ì‚¬ìš©ì ì •ì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤!

ì „ì²´ ì½”ë“œëŠ” ì—¬ê¸°ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



# ê²°ë¡ 

ì´ ê²Œì‹œë¬¼ì—ì„œëŠ” í…ì„œì™€ ê°™ì€ ê¸°ë³¸ ê°œë…, ì–´ë–»ê²Œ ëª¨ë¸ë§ë˜ëŠ”ì§€, CUDA ë° Autogradì™€ ê°™ì€ ê³ ê¸‰ ì£¼ì œ ë“±ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ì´ ê°€ëŠ¥í•œ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ ê²Œì‹œë¬¼ì´ ì—¬ëŸ¬ë¶„ì´ PyTorchê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê°„ëµíˆ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.

ì•ìœ¼ë¡œì˜ ê²Œì‹œë¬¼ì—ì„œëŠ” ë¶„ì‚° í›ˆë ¨(ë‹¤ì¤‘ ë…¸ë“œ/ë‹¤ì¤‘ GPU) ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ê°™ì€ ê³ ê¸‰ ì£¼ì œë¥¼ ë‹¤ë£¨ë ¤ê³  í•  ê²ƒì…ë‹ˆë‹¤. ì˜ê²¬ì´ ìˆê±°ë‚˜ ë‹¤ìŒì— ì–´ë–¤ ë‚´ìš©ì„ ë‹¤ë£¨ê¸¸ ì›í•˜ì‹œëŠ”ì§€ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”! ì½ì–´ ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š

ë˜í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ë°›ì•„ë³´ê¸° ìœ„í•´ ì—¬ê¸°ì™€ ì œ LinkedIn í”„ë¡œí•„ì—ì„œ íŒ”ë¡œìš°í•´ ì£¼ì„¸ìš”!



# ì°¸ê³  ìë£Œ

- [PyNorch](https://github.com) - ì´ í”„ë¡œì íŠ¸ì˜ GitHub ì €ì¥ì†Œ 
- [CUDA íŠœí† ë¦¬ì–¼](https://www.example.com/tutorial-cuda) - CUDA ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ê°„ë‹¨í•œ ì†Œê°œ
- [PyTorch](https://pytorch.org/docs) - PyTorch ë¬¸ì„œ



# MartinLwx's ë¸”ë¡œê·¸ - ìŠ¤íŠ¸ë¼ì´ë“œì— ê´€í•œ íŠœí† ë¦¬ì–¼.

# ìŠ¤íŠ¸ë¼ì´ë“œ íŠœí† ë¦¬ì–¼ - ìŠ¤íŠ¸ë¼ì´ë“œì— ê´€í•œ ë˜ ë‹¤ë¥¸ íŠœí† ë¦¬ì–¼.

# PyTorch ë‚´ë¶€ êµ¬ì¡° - PyTorch êµ¬ì¡°ì— ëŒ€í•œ ê°€ì´ë“œ.

# ë„¤ì¸  - NumPyë¥¼ ì‚¬ìš©í•œ PyTorch ì¬êµ¬í˜„.



Markdownìœ¼ë¡œ í‘œ íƒœê·¸ë¥¼ ë³€ê²½í•˜ì‹­ì‹œì˜¤.