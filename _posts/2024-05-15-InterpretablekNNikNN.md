---
title: "해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN"
description: ""
coverImage: "/assets/img/2024-05-15-InterpretablekNNikNN_0.png"
date: 2024-05-15 16:35
ogImage: 
  url: /assets/img/2024-05-15-InterpretablekNNikNN_0.png
tag: Tech
originalTitle: "Interpretable kNN (ikNN)"
link: "https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc"
---


## 해석 가능한 분류기

머신 러닝에서 분류 또는 회귀 문제를 다룰 때 많은 경우에, 우리는 가능한 가장 정확한 모델을 얻기에 관심이 있습니다. 그러나 때로는 모델의 해석가능성에도 관심이 있습니다. XGBoost, CatBoost, LGBM과 같은 모델은 강력할 수 있지만, 왜 그런 예측을 했는지 또는 보이지 않는 데이터에서 어떻게 행동할지 이해하기 어려울 수 있습니다. 이러한 모델은 블랙박스 모델이라고 불립니다. 즉, 왜 그런 예측을 하는지를 구체적으로 이해할 수 없는 모델들을 의미합니다.

많은 상황에서 이는 괜찮습니다. 대체로 정확한 경우에는 큰 문제가 없고, 때로는 잘못된 예측을 할 수 있습니다. 예를 들어, 웹사이트에서 현재 사용자에게 특정 광고가 판매를 생성할 가능성이 가장 높다는 것을 예측하는 모델이 있을 수 있습니다. 모델이 가끔 잘못 동작하더라도 수익에 영향을 미칠 수 있지만, 큰 문제는 없을 것입니다. 단순히 최적화되지 않은 모델을 사용하고 있지만 일반적으로 유용합니다.

그러나 다른 상황에서는 모델이 왜 그런 예측을 하는지 알아야 하는 것이 매우 중요할 수 있습니다. 이는 의학 및 보안과 같은 고위험 환경, 인종, 성별 또는 기타 보호되는 집단과 관련된 모델의 편향이 없어야 하는 환경뿐만 아니라, 감사가 필요한 환경에서도 중요합니다. 모델이 예상대로 동작하는지 확인하기 위해 모델을 이해하는 것이 필요한 환경에서도 중요합니다.



이러한 경우에도 블랙박스 모델(예: 부스팅 모델, 신경망, 랜덤 포레스트 등)을 사용하고 그 후에 사후 분석을 수행하는 것이 가능할 때가 있습니다. 이를 통해 모델이 왜 그런 예측을 한 것으로 보이는지에 대한 설명을 제공할 수 있습니다. 이를 설명가능한 AI(XAI) 분야라고 합니다. 이는 프록시 모델, 특성 중요도(예: SHAP), 반사실적 사례 또는 ALE 플롯과 같은 기술을 사용합니다. 이들은 매우 유용한 도구이지만, 다른 모든 조건이 동일하다면, 가능하면 처음부터 해석 가능한 모델을 가지는 것이 선호됩니다. XAI 방법은 매우 유용하지만 한계가 있습니다.

프록시 모델을 사용하면, 해석 가능한 모델(예: 얕은 의사 결정 트리)을 훈련하여 블랙박스 모델의 동작을 학습합니다. 이는 어느 정도 설명을 제공할 수 있지만 항상 정확하지는 않으며 대략적인 설명만 제공할 수 있습니다.

특성 중요도 또한 매우 유용하지만 관련 특성만을 나타내며, 예측과 관련된 방식이나 이러한 특성이 어떻게 상호 작용하여 예측을 형성하는지에 대해 제공하지는 않습니다. 또한 보지 못한 데이터와 어떻게 작동하는지 결정할 수 있는 능력이 없습니다.

해석 가능한 모델을 사용하면 이러한 문제가 없습니다. 모델 자체가 이해하기 쉬우며 각 예측을 하는 이유를 정확히 알 수 있습니다. 그러나 문제는 해석 가능한 모델이 블랙박스 모델보다 정확도가 낮을 수 있다는 것입니다. 항상 그렇지는 않겠지만, 종종 정확도가 떨어질 수 있습니다. 대부분의 문제에 대해 대부분의 해석 가능한 모델은 부스팅 모델이나 신경망과 경쟁력을 갖지 못합니다. 각각의 문제에 대해 충분히 정확한 해석 가능한 모델을 찾기 위해 여러 해석 가능한 모델을 시도해야 할 수도 있습니다.



오늘은 해석 가능한 여러 모델이 사용 가능하지만 아쉽게도 매우 적습니다. 이 중에는 의사 결정 트리, 규칙 목록(및 규칙 세트), GAM(Generalized Additive Models, 예를 들어 해석 가능한 부스팅 머신) 및 선형/로지스틱 회귀가 있습니다. 각각은 잘 작동하는 경우 유용할 수 있지만 옵션은 제한적입니다. 이것의 함의는: 다수의 프로젝트에서 만족할만한 성능을 발휘하는 해석 가능한 모델을 찾는 것이 불가능할 수 있습니다. 더 많은 옵션이 제공되면 실제 혜택이 있을 수 있습니다.

여기서 소개하는 또 다른 해석 가능한 모델인 ikNN 또는 해석 가능한 k 최근접 이웃은 2차원 kNN 모델의 앙상블에 기반합니다. 아이디어는 간단하지만 놀랍도록 효과적입니다. 그리고 상당히 해석하기 쉽습니다. CatBoost와 같은 탭 데이터의 예측에 대한 최첨단 모델과 정확도 측면에서 경쟁력이 있지는 않지만, 종종 문제에 충분한 정확도를 제공할 수 있습니다. 또한 의사 결정 트리 및 기존 해석 가능한 모델들과도 경쟁력이 있습니다.

재미있는 점은 일반 kNN 모델보다 더 강한 정확도를 가지고 있다는 것입니다.

이 프로젝트의 주요 페이지는 다음과 같습니다: [https://github.com/Brett-Kennedy/ikNN](https://github.com/Brett-Kennedy/ikNN)



프로젝트는 iKNNClassifier라는 단일 클래스를 정의합니다. 이 클래스는 interpretable_knn.py 파일을 복사하고 불러와서 어떤 프로젝트에든 포함될 수 있습니다. 이 클래스는 scikit-learn 분류기들과 일관된 인터페이스를 제공합니다. 즉, 일반적으로 인스턴스를 생성하고 fit() 및 predict()를 호출하여 사용하는 것이 랜덤 포레스트나 다른 scikit-learn 모델을 사용하는 것과 유사합니다.

2D kNN 앙상블을 활용하는 것은 여러 이점을 제공합니다. 하나는 앙상블을 사용할 때 항상 보는 일반적인 이점으로, 단일 모델에 의존하는 것보다 더 신뢰할 수 있는 예측을 할 수 있다는 것입니다.

또 하나는 2D 공간을 시각화하는 것이 간단하다는 것입니다. 현재 이 모델은 숫자형 입력을 필요로 합니다 (kNN의 경우와 마찬가지로), 따라서 모든 범주형 특성은 인코딩되어야 합니다. 그러나 이 작업이 완료되면 모든 2D 공간을 산점도로 시각화할 수 있습니다. 이는 해석하기 쉬운 정도를 제공합니다.

또한, 각 예측에 대해 가장 관련성 높은 2D 공간을 결정할 수 있으므로 각 레코드에 대해 몇 개의 플롯을 제공할 수 있습니다. 이를 통해 각 레코드에 대해 꽤 간단하면서 완전한 시각적 설명을 제공할 수 있습니다.



ikNN은 흥미로운 모델입니다. 앙상블을 기반으로 하지만 실제로는 해석 가능성을 높이는 모델이기 때문에 흥미롭습니다. 일반적으로는 그 반대가 더 많이 나타나는데요.

# 표준 kNN 분류기

kNN 모델은 부스트 모델이나 신경망 모델만큼 정확하지 않거나 의사결정 나무처럼 해석하기 어렵지 않기 때문에 다른 모델들보다 사용 빈도가 적습니다. 그렇지만 여전히 널리 사용되고 있습니다. kNN은 직관적인 아이디어에 기반합니다: 항목의 클래스는 해당 항목과 가장 유사한 다른 항목의 클래스를 기반으로 예측할 수 있습니다.

예를 들어, 붓꽃 데이터셋(아래 예시에서 사용되는 데이터셋)을 보면 세 가지 클래스가 있습니다. 다른 샘플의 붓꽃을 수집하여 이 샘플이 어떤 세 가지 붓꽃 종류 중 어떤 것인지 예측하고 싶다고 해보겠습니다. 훈련 데이터에서 가장 유사한, 예를 들어 10개의 레코드를 살펴보고 그들의 클래스를 확인한 뒤 가장 일반적인 클래스를 취할 수 있습니다.



이 예제에서는 각 레코드의 클래스를 추정하는 데 사용할 인근 이웃의 수로 10을 선택했지만, 다른 값도 사용할 수 있습니다. kNN 및 ikNN 모델에서 하이퍼파라미터로 지정됩니다(k 매개변수). 우리는 유사한 레코드의 합리적인 수를 사용하기 위해 k를 설정하고 싶습니다. 너무 적게 사용하면 결과가 불안정할 수 있습니다(각 예측은 매우 적은 다른 레코드에 기반합니다). 너무 많이 사용하면 결과가 매우 비슷하지 않은 다른 레코드에 기반할 수 있습니다.

또한 가장 유사한 항목을 결정할 방법이 필요합니다. 이를 위해, 적어도 기본적으로 유클리드 거리를 사용합니다. 데이터셋이 20개의 기능을 가지고 있고 k=10을 사용하는 경우, 20차원 공간에서 유클리드 거리에 기반하여 가장 가까운 10 개의 점을 찾습니다.

하나의 레코드를 예측할 때, 훈련 데이터에서 가장 가까운 10개의 레코드를 찾고 그들의 클래스를 확인합니다. 10개 중 8개가 Setosa 클래스(붓꽃의 3가지 종류 중 하나)라면, 이 행도 아마 Setosa일 것으로 가정할 수 있거나 적어도 우리가 할 수 있는 최선의 추측이라고 할 수 있습니다.

이 방법의 문제점 중 하나는 많은 기능이 있는 경우에는 작동하지 않는다는 것인데, 이것을 차원의 저주라고 합니다. 고차원 공간의 흥미로운 특성 중 하나는 충분한 기능이 함께하면 점들간의 거리가 의미를 잃기 시작한다는 것입니다.



kNN은 모든 특징을 동등하게 사용하지만, 몇 가지는 다른 것보다 타겟을 예측하는 데 훨씬 더 중요할 수 있습니다. 점들 사이의 거리는 유클리드(또는 때로는 맨하탄 및 다른 거리 측정 방법)에 기반하여 계산되며 모든 특징을 동등하게 고려합니다. 이 방법은 간단하지만, 모든 특징이 타겟에 무관할 수 있는데 이것이 가장 효과적이지는 않습니다. 특정 특징 선택이 이루어졌다고 가정했을 때는 그 가능성이 적어지지만, 특징들의 중요성은 여전히 똑같지 않을 수 있습니다.

또한, kNN 예측기에 의한 예측은 해석하기 어렵습니다. 이 알고리즘은 이해하기 쉽지만, 예측을 이해하는 것은 어려울 수 있습니다. k 최근접 이웃을 나열하여 예측에 대한 어떤 통찰력을 제공할 수 있지만, 특히 많은 특징이 있는 경우에는 왜 주어진 레코드 세트가 가장 유사한지 파악하기 어려울 수 있습니다.

# The ikNN Algorithm

ikNN 모델은 먼저 모든 특징 쌍을 가져와 이러한 특징을 사용하여 표준 2차원 kNN 분류기를 생성합니다. 따라서, 테이블이 10개의 특징을 가지고 있으면, 10개 중 2개씩 선택하는 방식으로, 즉 45개의 모델을 생성하게 됩니다.



그런 다음 훈련 데이터를 사용하여 대상 열을 예측하는 데 대한 정확도를 평가합니다. 이렇게하면 ikNN 모델이 각 2차원 하위 공간의 예측 능력을 결정합니다. 45개의 2차원 모델의 경우 일부는 다른 것보다 더 예측력이 뛰어날 것입니다. 예측을 수행하기 위해 훈련 데이터에서 가장 예측력이 뛰어난 것으로 알려진 2차원 하위 공간이 선택되며, 선택적으로 훈련 데이터에서의 예측력에 따라 가중치가 부여될 수 있습니다.

또한 추론 시, 주어진 행 주변의 가장 가까운 이웃 세트의 순도를 고려할 수 있으며, 모델은 훈련 데이터에서 더 예측력이 뛰어난 하위 공간 및 현재 인스턴스와 관련하여 예측이 가장 일관된 것으로 보이는 하위 공간을 더 중요하게 고려할 수 있습니다.

별로 표시된 두 하위 공간과 한 점을 고려해 보겠습니다. 두 경우 모두 점에 가장 가까운 k개의 점 세트를 찾을 수 있습니다. 여기서 점 주변에 녹색 원을 그립니다. 그러나 이 점들이 실제로 원을 형성하지는 않습니다 (단, 효과적으로 이웃을 정의하는 k번째 가장 가까운 이웃의 반경이 있습니다).



이 플롯들은 각각 한 쌍의 피처를 나타냅니다. 왼쪽 플롯의 경우, 별 주변 이웃 사이에 매우 높은 일관성이 있습니다: 그들은 모두 빨간색입니다. 오른쪽 플롯에서는 이웃들 사이에 일관성이 거의 없습니다: 일부는 빨간색이고 일부는 파란색입니다. 첫 번째 피처 쌍은 레코드에 대해 더 예측력이 높아 보이며, ikNN이 이용합니다.

이 방식을 통해 모델은 모든 입력 피쳐의 영향을 고려하지만, 더 많은 예측력을 가진 피처의 영향을 강화하고, 덜 예측력이 있는 피처의 영향을 줄입니다.

# 예시

우리는 먼저 iris 데이터셋과 같은 장난감 데이터셋을 이용하여 ikNN을 시연합니다. 데이터를 불러오고, 훈련 및 테스트 세트로 분리하고, 테스트 세트에 대한 예측을 수행합니다.



```js
from sklearn.datasets import load_iris
from interpretable_knn import ikNNClassifier

iris = load_iris()
X, y = iris.data, iris.target

clf = ikNNClassifier()
X_train, X_test, y_train, y_test = train_test_split(
   X, y, test_size=0.33, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```

예측을 위해 이것으로 충분합니다. 하지만 ikNN은 모델을 이해하기 위한 도구도 제공합니다. 특히, graph_model()과 graph_predictions() API가 있습니다.

graph_model()의 예시는 다음과 같습니다:

```js
ikNN.graph_model(X.columns)
```




![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_1.png)

이 그림은 기본적으로 5개의 2D 공간을 나타내는 데이터 공간의 빠른 개요를 제공합니다. 점들은 훈련 데이터의 클래스를 보여줍니다. 배경 색상은 2D 공간의 각 영역에 대해 kNN이 한 예측을 보여줍니다.

그래프_예측() API는 특정 행을 설명합니다. 예를 들어:

![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_2.png)




여기서 설명되는 행은 빨간 별로 표시됩니다. 기본적으로 기본적으로 다섯 개의 플롯을 사용하지만 간단함을 위해 두 개만 사용됩니다. 두 그래프 모두에서 Row 0이 학습 데이터와 2D 공간에 대한 2D kNN의 예측과 관련하여 어디에 위치하는지 확인할 수 있습니다.

# 시각화

구성할 수는 있지만 기본적으로 각 ikNN 예측은 오직 다섯 개의 2D 공간만 사용합니다. 이는 예측 시간이 빠르고 시각화가 간단해지도록 합니다. 이것은 예측을 단순화하는 것이 아닌 실제 예측을 보여주므로 예측이 완전히 해석 가능하다는 것을 의미합니다.

대부분의 데이터 세트에서 대부분의 행에 대해 대부분 또는 거의 모든 2D 공간에서 예측이 일치합니다. 그러나 예측이 잘못된 경우, 현재 데이터 세트에 맞게 하이퍼파라미터를 더 잘 조정하기 위해 더 많은 2D 플롯을 조사하는 것이 유용할 수 있습니다.



# 정확도 테스트

OpenML에서 무작위로 선택한 100개의 분류 데이터셋을 사용하여 일련의 테스트를 수행했습니다. 표준 kNN 및 ikNN 모델의 F1 (macro) 점수를 비교한 결과, ikNN은 58개 데이터셋에서 더 높은 점수를 기록하고, kNN은 42개 데이터셋에서 우세했습니다.

최적의 하이퍼파라미터를 탐색하기 위해 그리드 검색을 수행하는 경우, ikNN이 더 나은 성과를 보입니다. 모든 100개 데이터셋에 대해 두 모델에 대해 이를 수행한 결과, ikNN은 100개 중 76개에서 가장 우수한 성과를 보였습니다. 또한 훈련 및 테스트 점수 사이의 간격이 작아져 표준 kNN 모델보다 더 안정적인 모델을 나타냅니다.

ikNN 모델은 다소 느릴 수 있지만, 강화 모델보다 여전히 상당히 빠르며 대개 훈련에 거의 1분 미만이 소요됩니다.通前합니다.



깃허브 페이지에서는 정확도에 대한 몇 가지 예제와 분석을 제공합니다.

# 결론

정확도가 주요 목표인 경우 ikNN이 가장 강력한 모델은 아닐 수 있지만 (어떤 모델이든 그렇듯이 때때로 그럴 수 있음), 해석 가능한 모델이 필요한 경우 시도해 볼만한 모델일 것으로 예상됩니다.

이 페이지에서는 도구를 사용하는 데 필요한 기본 정보를 제공했습니다. .py 파일(https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py)을 다운로드하고 코드에 가져와 인스턴스를 생성하고, 훈련하고 예측하며, (원하는 경우) graph_predictions()를 호출하여 원하는 레코드에 대한 설명을 확인하면 됩니다.



모든 이미지는 저자에 의해 제공됩니다.