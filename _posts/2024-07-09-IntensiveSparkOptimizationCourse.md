---
title: "ìŠ¤íŒŒí¬ ìµœì í™” ì§‘ì¤‘ ê°•ì¢Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œ"
description: ""
coverImage: "/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png"
date: 2024-07-09 20:14
ogImage: 
  url: /assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png
tag: Tech
originalTitle: "Intensive Spark Optimization Course"
link: "https://medium.com/@kevinchwong/intensive-spark-optimization-course-082bdd0592bc"
isUpdated: true
---



![Intensive Spark Optimization Course](/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png)

# ë¡œì»¬ì—ì„œ í”Œë ˆì´ê·¸ë¼ìš´ë“œ ì„¤ì •í•˜ê¸°

- Docker Desktopì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
- `docker run -p 8888:8888 jupyter/pyspark-notebook`ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
- ë‹¤ìŒ ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ì£¼í”¼í„° ë©ì„ ì—´ê¸° ìœ„í•´ URL ì¤‘ í•˜ë‚˜ë¥¼ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤.

```js
ì„œë²„ì— ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ì´ íŒŒì¼ì„ ì—½ë‹ˆë‹¤:
    file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html
ë˜ëŠ” ë‹¤ìŒ URL ì¤‘ í•˜ë‚˜ë¥¼ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤:
    http://3c331b638888:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72
    http://127.0.0.1:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72
```

<div class="content-ad"></div>

# SparkSession ì´ˆê¸°í™”í•˜ê¸°

```js
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Spark Test").getOrCreate()
```

# ë°ì´í„°í”„ë ˆì„

## 1. ë°ì´í„°í”„ë ˆì„ ìƒì„±í•˜ê¸°

<div class="content-ad"></div>

- ì»¬ëŸ¼ ì‚¬ìš©

```js
from pyspark.sql.types import StructType, IntegerType, StringType

# ë°ì´í„°ë¥¼ íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜
data = [("James", 34), ("Anna", 20), ("Lee", 30)]

# ì»¬ëŸ¼ ì‚¬ìš©
columns = ["Name", "Age"]
df = spark.createDataFrame(data, schema=columns)
```

- ìŠ¤í‚¤ë§ˆ ì‚¬ìš©

```js
from pyspark.sql.types import StructType, IntegerType, StringType

# ë°ì´í„°ë¥¼ íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜
data = [("James", 34), ("Anna", 20), ("Lee", 30)]

# ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True)
])
df = spark.createDataFrame(data, schema=schema)
```

<div class="content-ad"></div>

- RDD ì‚¬ìš©

```js
from pyspark.sql.types import StructType, IntegerType, StringType

# ë°ì´í„°ë¥¼ íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì¤€ë¹„
data = [("James", 34), ("Anna", 20), ("Lee", 30)]

# RDD ì‚¬ìš©
rdd = spark.sparkContext.parallelize(data)
schema = StructType([
    StructField("ì´ë¦„", StringType(), True),
    StructField("ë‚˜ì´", IntegerType(), True)
])
df = spark.createDataFrame(rdd, schema=schema)
```

```js
df.show()

# ì¶œë ¥
+-----+---+
| ì´ë¦„|ë‚˜ì´|
+-----+---+
|James| 34|
| Anna| 20|
|  Lee| 30|
+-----+---+
```

## 2. ë°ì´í„°í”„ë ˆì„ í‘œì‹œ

<div class="content-ad"></div>

```js
df.printSchema();
print(df.schema);
print(df.columns);
df.describe().show();
```

```js
#ê²°ê³¼

## df.printSchema()
root
 |-- Name: string (nullable = true)
 |-- Age: long (nullable = true)

## print(df.schema)
StructType([
  StructField(â€˜Nameâ€™, StringType(), True),
  StructField(â€˜Ageâ€™, LongType(), True)
])

## print(df.columns)
[â€˜Nameâ€™, â€˜Ageâ€™]

## df.describe().show()
+-------+----+-----------------+
|summary|Name|              Age|
+-------+----+-----------------+
|  count|   3|                3|
|   mean|NULL|             28.0|
| stddev|NULL|7.211102550927978|
|    min|Anna|               20|
|    max| Lee|               34|
+-------+----+-----------------+
```

## 3. ì»¬ëŸ¼ ì„ íƒ

```js
df.select(df[0]).show();
df.select(df.Name).show();
df.select(df["Name"]).show();
df.select("Name").show();
```

<div class="content-ad"></div>

```js
#Output
+-----+
| Name|
+-----+
|James|
| Anna|
|  Lee|
+-----+
```

## 4. ë°ì´í„° í•„í„°ë§

```js
# ë°ì´í„° í•„í„°ë§
df.filter(df[1] > 25).show()
df.filter(df.Age > 25).show()
df.filter(df["Age"] > 25).show()
```

```js
#Output
+-----+---+
| Name|Age|
+-----+---+
|James| 34|
|  Lee| 30|
+-----+---+
```

<div class="content-ad"></div>

## 5. íŒŒì¼ì— DataFrame ì‘ì„±í•˜ê¸°

```js
# JSON íŒŒì¼ ì‘ì„±
df.write.json("test123.json")

# Parquet íŒŒì¼ ì‘ì„±
df.write.parquet("test123.parquet")
```

## 6. íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°

```js
# JSON íŒŒì¼ ì½ê¸°
df_json = spark.read.json("test123.json")
# Parquet íŒŒì¼ ì½ê¸°
df_parquet = spark.read.parquet("test123.parquet")
```

<div class="content-ad"></div>

## 7. ìƒˆë¡œìš´ ë³µí•© ë°ì´í„° ìœ í˜•ì„ í¬í•¨í•œ ìƒˆë¡œìš´ ì—´ ì¶”ê°€

```js
from pyspark.sql.functions import struct
df2 = df.withColumn("NameAndAge", struct(df.Name, df.Age))
df2.show()
df2.printSchema()
```

```js
# ì¶œë ¥
+-----+---+-----------+
| Name|Age| NameAndAge|
+-----+---+-----------+
|James| 34|{James, 34}|
| Anna| 20| {Anna, 20}|
|  Lee| 30|  {Lee, 30}|
+-----+---+-----------+

# ìŠ¤í‚¤ë§ˆ ì¶œë ¥
root
 |-- Name: string (nullable = true)
 |-- Age: long (nullable = true)
 |-- NameAndAge: struct (nullable = false)
 |    |-- Name: string (nullable = true)
 |    |-- Age: long (nullable = true)
```

# ì¿¼ë¦¬: ê·¸ë£¹í™” ë° ì§‘ê³„

<div class="content-ad"></div>

## 1. count()

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import count

# ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™”
spark = SparkSession.builder.appName("ê·¸ë£¨í•‘ ë° ì§‘ê³„").getOrCreate()

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = [("James", "Sales", 3000),
        ("Michael", "Sales", 4600),
        ("Robert", "Sales", 4100),
        ("Maria", "Finance", 3000),
        ("James", "Sales", 3000),
        ("Scott", "Finance", 3300),
        ("Jen", "Finance", 3900),
        ("Jeff", "Marketing", 3000),
        ("Kumar", "Marketing", 2000),
        ("Saif", "Sales", 4100)]
columns = ["employee_name", "department", "salary"]
df = spark.createDataFrame(data, schema=columns)

# ê·¸ë£¹í™” ë° count ìˆ˜í–‰
grouped_df = df.groupBy("department").count()
grouped_df.show()
```

```python
# ê²°ê³¼

+----------+-----+
|department|count|
+----------+-----+
|     Sales|    5|
|   Finance|    3|
| Marketing|    2|
+----------+-----+
```

## 2. max(), min(), avg(), sum()

<div class="content-ad"></div>

```js
# ê·¸ë£¹ë³„ë¡œ ê·¸ë£¹í™” ë° ìµœëŒ“ê°’ ì°¾ê¸°
max_df = df.groupBy("department").max("salary").alias("max_salary")
max_df.show()
```

```js
# ê²°ê³¼

+----------+-----------+
|department|max(salary)|
+----------+-----------+
|     Sales|       4600|
|   Finance|       3900|
| Marketing|       3000|
+----------+-----------+
```

## 3. agg() + F.max(), F.count() ë“±â€¦

```js
from pyspark.sql import functions as F

# ì—¬ëŸ¬ ê°€ì§€ ì§‘ê³„ ë™ì‘ ìˆ˜í–‰
agg_df = df.groupBy("department").agg(
    F.count("salary").alias("count"),
    F.max("salary").alias("max_salary"),
    F.min("salary").alias("min_salary"),
    F.sum("salary").alias("total_salary"),
    F.avg("salary").alias("average_salary")
)
agg_df.show()
```

<div class="content-ad"></div>

```js
# ê²°ê³¼
+----------+-----+----------+----------+------------+--------------+
|department|count|max_salary|min_salary|total_salary|average_salary|
+----------+-----+----------+----------+------------+--------------+
|     Sales|    5|      4600|      3000|       18800|        3760.0|
|   Finance|    3|      3900|      3000|       10200|        3400.0|
| Marketing|    2|      3000|      2000|        5000|        2500.0|
+----------+-----+----------+----------+------------+--------------+
```

## 4. agg() + collect_list() ë° collect_set()

```js
from pyspark.sql.functions import collect_list

# GroupBy ë° ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ìˆ˜í–‰
collected_list_df = df.groupBy("department").agg(
  collect_list("salary"),
  collect_set("salary")
)
collected_list_df.show(truncate=False)
```

```js
# ê²°ê³¼
+----------+------------------------------+-------------------+
|department|collect_list(salary)          |collect_set(salary)|
+----------+------------------------------+-------------------+
|Sales     |[3000, 4600, 4100, 3000, 4100]|[4600, 3000, 4100] |
|Finance   |[3000, 3300, 3900]            |[3900, 3000, 3300] |
|Marketing |[3000, 2000]                  |[3000, 2000]       |
+----------+------------------------------+-------------------+
```

<div class="content-ad"></div>

## 5. agg() + ì‚¬ìš©ì ì •ì˜ ì§‘ê³„ í•¨ìˆ˜ (UDAF)

- ë•Œë¡œëŠ” ë‚´ì¥ í•¨ìˆ˜ë§Œìœ¼ë¡œ ë³µì¡í•œ ì§‘ê³„ë¥¼ ìˆ˜í–‰í•˜ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Sparkë¥¼ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©ì ì •ì˜ ì§‘ê³„ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
# ì´ˆê¸° ë°ì´í„°
|ë¶€ì„œ     |ì§ì› ì´ë¦„        |ê¸‰ì—¬    |
|----------|-------------|------|
|Sales    |James        |3000  |
|Sales    |Michael      |4600  |
|Sales    |Robert       |4100  |
|Finance  |Maria        |3000  |
|Sales    |James        |3000  |
|Finance  |Scott        |3300  |
|Finance  |Jen          |3900  |
|Marketing|Jeff         |3000  |
|Marketing|Kumar        |2000  |
|Sales    |Saif         |4100  |
```

```js
from pyspark.sql.functions import pandas_udf, PandasUDFType
from pandas import DataFrame

@pandas_udf("double")
def mean_salary(s: pd.Series) -> float:
 return s.mean()
udaf_df = df.groupBy("department").agg(
  mean_salary(df["salary"]).alias("average_salary")
)
udaf_df.show()
```

<div class="content-ad"></div>

```js
# ê²°ê³¼
+----------+--------------+
|department|average_salary|
+----------+--------------+
|   Finance|        3400.0|
| Marketing|        2500.0|
|     Sales|        3760.0|
+----------+--------------+

## 6. agg() + ë³µì¡í•œ ì¡°ê±´: when()

- ë•Œë¡œëŠ” ì¡°ê±´ì— ë”°ë¥¸ í•©ê³„ë‚˜ í‰ê· ê³¼ ê°™ì€ ë³µì¡í•œ ì¡°ê±´ì´ ì§‘ê³„ ì¤‘ì— í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ì´ˆê¸° ë°ì´í„°
+----------+-------------+------+
|department|employee_name|salary|
+----------+-------------+------+
|     Sales|        James|  3000|
|     Sales|      Michael|  4600|
|     Sales|       Robert|  4100|
|   Finance|        Maria|  3000|
|     Sales|        James|  3000|
|   Finance|        Scott|  3300|
|   Finance|          Jen|  3900|
| Marketing|         Jeff|  3000|
| Marketing|        Kumar|  2000|
|     Sales|         Saif|  4100|
+----------+-------------+------+

<div class="content-ad"></div>

from pyspark.sql.functions import when

# ì¡°ê±´ë¶€ ì§‘ê³„
conditional_agg_df = df.groupBy("department").agg(
    sum(when(df["salary"] > 3000, df["salary"])).alias("sum_high_salaries")
)
conditional_agg_df.show()

# ê²°ê³¼

+----------+-----------------+
|department|sum_high_salaries|
+----------+-----------------+
|     Sales|            12800|
|   Finance|             7200|
| Marketing|             NULL|
+----------+-----------------+

## 6. agg() ì´í›„ GroupByì—ì„œ RDD Map í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°

- ê²½ìš°ì— ë”°ë¼ ë§¤í•‘ í•¨ìˆ˜ë¥¼ GroupByì™€ ê²°í•©í•˜ì—¬ ì§‘ë‹¨í™”ëœ ë°ì´í„°ì— ëŒ€í•œ ì§ì ‘ì ì¸ ì§‘ê³„ê°€ ì•„ë‹Œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

```

# ë°ì´í„°

| department | employee_name | salary |
| ---------- | ------------- | ------ |
| Sales      | James         | 3000   |
| Sales      | Michael       | 4600   |
| Sales      | Robert        | 4100   |
| Finance    | Maria         | 3000   |
| Sales      | James         | 3000   |
| Finance    | Scott         | 3300   |
| Finance    | Jen           | 3900   |
| Marketing  | Jeff          | 3000   |
| Marketing  | Kumar         | 2000   |
| Sales      | Saif          | 4100   |

# GroupBy í›„ map ì‘ì—… ì ìš©

```python
result_rdd = df.groupBy("department").agg(
  collect_list("salary")
).rdd.map(
  lambda x: (x[0], max(x[1]))
)

result_df = spark.createDataFrame(result_rdd, ["department", "max_salary"])
result_df.show()
```

# ê²°ê³¼

| department | max_salary |
| ---------- | ---------- |
| Sales      | 4600       |
| Finance    | 3900       |
| Marketing  | 3000       |

# ì¡°íšŒ: ë‹¤ë¥¸ ê²ƒ

<div class="content-ad"></div>

## 1. rollup()ê³¼ cube()

- Rollup()ì€ ë‹¤ì°¨ì› ì§‘ê³„ë¥¼ ìƒì„±í•˜ê³  Excelì˜ ì†Œê³„ì™€ ìœ ì‚¬í•œ ê³„ì¸µì  ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

```js
# ì´ˆê¸° ë°ì´í„°
|department|employee_name|salary|
|----------|-------------|------|
| Sales    | James       | 3000 |
| Sales    | Michael     | 4600 |
| Sales    | Robert      | 4100 |
| Finance  | Maria       | 3000 |
| Sales    | James       | 3000 |
| Finance  | Scott       | 3300 |
| Finance  | Jen         | 3900 |
| Marketing| Jeff        | 3000 |
| Marketing| Kumar       | 2000 |
| Sales    | Saif        | 4100 |
```

```js
from pyspark.sql.functions import sum

# Rollup ì˜ˆì œ
rollup_df = df.rollup("department", "employee_name").sum("salary")
rollup_df.show()
```

<div class="content-ad"></div>

```js
# ê²°ê³¼
+----------+-------------+-----------+
|department|employee_name|sum(salary)|
+----------+-------------+-----------+
|     Sales|        James|       6000|
|      NULL|         NULL|      34000|
|     Sales|         NULL|      18800|
|     Sales|      Michael|       4600|
|     Sales|       Robert|       4100|
|   Finance|         NULL|      10200|
|   Finance|        Maria|       3000|
|   Finance|        Scott|       3300|
|   Finance|          Jen|       3900|
| Marketing|         NULL|       5000|
| Marketing|         Jeff|       3000|
| Marketing|        Kumar|       2000|
|     Sales|         Saif|       4100|
+----------+-------------+-----------+
```

- Cube(): CubeëŠ” ë‹¤ì°¨ì› ì§‘ê³„ë¥¼ ìƒì„±í•˜ê³  ì§€ì •ëœ ê·¸ë£¹í™” ì—´ì˜ ë‹¤ì¤‘ ì¡°í•©ì„ í†µí•´ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

```js
# ì´ˆê¸° ë°ì´í„°
+----------+-------------+------+
|department|employee_name|salary|
+----------+-------------+------+
|     Sales|        James|  3000|
|     Sales|      Michael|  4600|
|     Sales|       Robert|  4100|
|   Finance|        Maria|  3000|
|     Sales|        James|  3000|
|   Finance|        Scott|  3300|
|   Finance|          Jen|  3900|
| Marketing|         Jeff|  3000|
| Marketing|        Kumar|  2000|
|     Sales|         Saif|  4100|
+----------+-------------+------+
```

```js
from pyspark.sql.functions import sum

# Cube ì˜ˆì‹œ
cube_df = df.cube("department", "employee_name").sum("salary")
cube_df.show()
```

<div class="content-ad"></div>

```js
# ê²°ê³¼

+----------+-------------+-----------+
|ë¶€ì„œ      |ì§ì›ëª…         |ê¸‰ì—¬í•©ê³„   |
+----------+-------------+-----------+
|      NULL|        James|       6000|
|     íŒë§¤ |        James|       6000|
|      NULL|         NULL|      34000|
|     íŒë§¤ |         NULL|      18800|
|      NULL|      Michael|       4600|
|     íŒë§¤ |      Michael|       4600|
|     íŒë§¤ |       Robert|       4100|
|      NULL|       Robert|       4100|
|   ì¬ë¬´   |         NULL|      10200|
|   ì¬ë¬´   |        Maria|       3000|
|      NULL|        Maria|       3000|
|      NULL|        Scott|       3300|
|   ì¬ë¬´   |        Scott|       3300|
|      NULL|          Jen|       3900|
|   ì¬ë¬´   |          Jen|       3900|
| ë§ˆì¼€íŒ…  |         NULL|       5000|
| ë§ˆì¼€íŒ…  |         Jeff|       3000|
|      NULL|         Jeff|       3000|
| ë§ˆì¼€íŒ…  |        Kumar|       2000|
|      NULL|        Kumar|       2000|
+----------+-------------+-----------+
ìƒìœ„ 20ê°œ í–‰ë§Œ í‘œì‹œ
```

## 2. groupBy() + pivot()

- Pivotingì„ ì‚¬ìš©í•˜ë©´ í–‰ì„ ì—´ë¡œ ë³€í™˜í•˜ì—¬ í”¼ë²— í…Œì´ë¸”ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¢…ì¢… ë‘ ì—´ ê°„ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

```js
# ì´ˆê¸° ë°ì´í„°
+----------+-------------+------+
|ë¶€ì„œ     |ì§ì›ëª…        |ê¸‰ì—¬   |
+----------+-------------+------+
|     íŒë§¤ |        James|  3000|
|     íŒë§¤ |      Michael|  4600|
|     íŒë§¤ |       Robert|  4100|
|   ì¬ë¬´  |        Maria|  3000|
|     íŒë§¤ |        James|  3000|
|   ì¬ë¬´  |        Scott|  3300|
|   ì¬ë¬´  |          Jen|  3900|
| ë§ˆì¼€íŒ…  |         Jeff|  3000|
| ë§ˆì¼€íŒ…  |        Kumar|  2000|
|     íŒë§¤ |         Saif|  4100|
+----------+-------------+------+
```

<div class="content-ad"></div>

```js
# Pivot ì˜ˆì‹œ
pivot_df = df.groupBy("department").pivot("employee_name").sum("salary")
pivot_df.show()
```

```js
# ê²°ê³¼
+----------+-----+----+----+-----+-----+-------+------+----+-----+
|department|James|Jeff| Jen|Kumar|Maria|Michael|Robert|Saif|Scott|
+----------+-----+----+----+-----+-----+-------+------+----+-----+
|     Sales| 6000|NULL|NULL| NULL| NULL|   4600|  4100|4100| NULL|
|   Finance| NULL|NULL|3900| NULL| 3000|   NULL|  NULL|NULL| 3300|
| Marketing| NULL|3000|NULL| 2000| NULL|   NULL|  NULL|NULL| NULL|
+----------+-----+----+----+-----+-----+-------+------+----+-----+
```

## 3. ìœˆë„ìš° í•¨ìˆ˜: partitionBy() + row_number()/rank().over(w)

- ìœˆë„ìš° í•¨ìˆ˜ëŠ” í˜„ì¬ í–‰ê³¼ ê´€ë ¨ëœ "ìœˆë„ìš°"ì— ëŒ€í•´ ê³„ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ ì „í†µì ì¸ group-by ì‘ì—…ë³´ë‹¤ ë” ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ëŸ¬ë‹ í† íƒˆ, ì´ë™ í‰ê·  ë˜ëŠ” ì´ì „ ë° ë‹¤ìŒ í–‰ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë° íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.

<div class="content-ad"></div>

```js
# ì´ˆê¸° ë°ì´í„°
| ë¶€ì„œ      | ì§ì› ì´ë¦„       | ê¸‰ì—¬  |
|----------|---------------|------|
| Sales    | James         | 3000 |
| Sales    | Michael       | 4600 |
| Sales    | Robert        | 4100 |
| Finance  | Maria         | 3000 |
| Sales    | James         | 3000 |
| Finance  | Scott         | 3300 |
| Finance  | Jen           | 3900 |
| Marketing| Jeff          | 3000 |
| Marketing| Kumar         | 2000 |
| Sales    | Saif          | 4100 |

```

```js
from pyspark.sql.window import Window
from pyspark.sql.functions import col, row_number

windowSpec = Window.partitionBy("department").orderBy(col("salary").asc())
df_with_row_number = df.withColumn("row_number", row_number().over(windowSpec))
df_with_row_number.show()
```

```js
# ê²°ê³¼
| ì§ì› ì´ë¦„      | ë¶€ì„œ        | ê¸‰ì—¬  | row_number |
|---------------|------------|------|------------|
| Maria         | Finance    | 3000 |      1     |
| Scott         | Finance    | 3300 |      2     |
| Jen           | Finance    | 3900 |      3     |
| Kumar         | Marketing  | 2000 |      1     |
| Jeff          | Marketing  | 3000 |      2     |
| James         | Sales      | 3000 |      1     |
| James         | Sales      | 3000 |      2     |
| Robert        | Sales      | 4100 |      3     |
| Saif          | Sales      | 4100 |      4     |
| Michael       | Sales      | 4600 |      5     |

```

- Rank() í•¨ìˆ˜ë¥¼ ìœ„í•´

<div class="content-ad"></div>

```js
from pyspark.sql.window import Window
from pyspark.sql.functions import rank, col

windowSpec = Window.partitionBy("department").orderBy(col("salary").desc())
df_with_rank = df.withColumn("rank", rank().over(windowSpec))
df_with_rank.show()
```

```js
# ì¶œë ¥
+-------------+----------+------+----------+
|employee_name|department|salary|      rank|
+-------------+----------+------+----------+
|          Jen|   Finance|  3900|         1|
|        Scott|   Finance|  3300|         2|
|        Maria|   Finance|  3000|         3|
|         Jeff| Marketing|  3000|         1|
|        Kumar| Marketing|  2000|         2|
|      Michael|     Sales|  4600|         1|
|       Robert|     Sales|  4100|         2|
|         Saif|     Sales|  4100|         2|
|        James|     Sales|  3000|         3|
|        James|     Sales|  3000|         3|
+-------------+----------+------+----------+
```

# ìµœì í™” I: ë¬´ê²Œ ê°ì†Œ

## 0. ë¶ˆí•„ìš”í•œ ì›ì‹œ ë°ì´í„° ì œê±°

<div class="content-ad"></div>

## 1. DataFrameì´ ì—¬ëŸ¬ ë²ˆ ì•¡ì„¸ìŠ¤ ë  ë•Œ ìºì‹œí•©ë‹ˆë‹¤.

```js
df.cache();
df.count();
```

```js
#ì¶œë ¥
3
```

## 2. ì ì ˆí•œ íŒŒì¼ í˜•ì‹ ì‚¬ìš©í•˜ê¸°

<div class="content-ad"></div>

- ì••ì¶• íŒŒì¼ì€ íŒŒì¼ì˜ ì…ì¶œë ¥ ë° ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆì–´ìš”.
- ì••ì¶• í•´ì œëœ íŒŒì¼ì€ CPUë¥¼ ì ˆì•½í•  ìˆ˜ ìˆì–´ìš”.

```js
df.write.parquet("output.parquet");
```

## 3. ìŠ¤í‚¤ë§ˆ ìˆ˜ë™ ì§€ì •í•˜ê¸°

```js
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])
df = spark.read.schema(schema).csv("path/to/file.csv")
```

<div class="content-ad"></div>

## 4. ì¡°ê¸°ì— í•„ìš”í•œ ì—´ ì„ íƒí•˜ê¸°

ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ í•„ìš”í•œ ì—´ë§Œ ë¯¸ë¦¬ ì„ íƒí•˜ì„¸ìš”.

```js
df.select("dept_name", "name").filter("dept_id >= 102").show();
df.select("dept_name", "name")
  .filter(df.dept_id >= 102)
  .show();
```

```js
#ì¶œë ¥
+---------+----+
|dept_name|name|
+---------+----+
|Marketing|Jane|
|  Finance| Joe|
+---------+----+
```

<div class="content-ad"></div>

## 5. í•„í„°ë¥¼ ì¡°ì¸ì´ë‚˜ ì§‘ê³„ ì „ì— ë¹ ë¥´ê²Œ ì ìš©í•˜ì„¸ìš”.

```js
df.filter("age > 25").join(df_other, "id").show();
```

## 6. í° ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë°©ì§€ë¥¼ ìœ„í•´ limit() ì‚¬ìš©í•˜ê¸°

- í° ë°ì´í„°ì…‹ì— collect()ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì—¬ ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ì„¸ìš”.

<div class="content-ad"></div>

```js
df.filter("age > 30").limit(100).collect();
```

## 7. Using spark.sql(): Catalyst optimizer for Complex Queries

- Leverage Spark SQL for complex queries, which might be more readable and can benefit from the Catalyst optimizer.

```js
df.createOrReplaceTempView("table");
spark.sql("SELECT id, sum(value) FROM table GROUP BY id").show();
```

<div class="content-ad"></div>

## 8. RDD ì‚¬ìš©: reduceByKeyë¥¼ ì‚¬ìš©í•œ ì§‘ê³„

- ì§‘ê³„ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ reduceByKeyë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ groupByë³´ë‹¤ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
rdd = df.rdd.map(lambda x: (x[0], x[1]))
reduced = rdd.reduceByKey(lambda a, b: a + b)
reduced.toDF(["key", "value"]).show()
```

# ìµœì í™” II: íŒŒí‹°ì…˜ ì—†ì´ ë³‘ë ¬í™” ì—†ìŒ

<div class="content-ad"></div>

## 1. í…Œì´ë¸” ë¶„í• : partitionBy()

- ë°ì´í„°í”„ë ˆì„ì„ ë””ìŠ¤í¬ì— ì €ì¥í•  ë•Œ ë¹ ë¥¸ í›„ì† ì½ê¸°ë¥¼ ìœ„í•´ ë¶„í• ì„ ì‚¬ìš©í•˜ì„¸ìš”.

```js
df.write.partitionBy("year", "month").parquet("path/to/output");
```

## 2. ìŠ¤íŠœ ê´€ë¦¬ë¥¼ ìœ„í•œ Salting í‚¤

<div class="content-ad"></div>

- ì¡°ì¸ ì—°ì‚° ì¤‘ ë°ì´í„° ìŠ¤íê°€ ë°œìƒí•˜ëŠ” ê²½ìš°, í•˜ë‚˜ ì´ìƒì˜ í‚¤ ê°’ì´ ë‹¤ë¥¸ ê²ƒë“¤ë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.
- ì˜ˆë¥¼ ë“¤ì–´ "customer_id"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸ì„ ìˆ˜í–‰í•˜ê³ , ëŒ€ë¶€ë¶„ì˜ ê±°ë˜ê°€ ì†Œìˆ˜ì˜ ê³ ê°ì— ì†í•´ ìˆë‹¤ë©´ ì´ëŸ¬í•œ ì†Œìˆ˜ì˜ í‚¤ëŠ” ë‹¤ë¥¸ í‚¤ë“¤ì— ë¹„í•´ í›¨ì”¬ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì¼ë¶€ ì‘ì—…(í° í‚¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‘ì—…)ì´ í›¨ì”¬ ë” ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ê³  ë³‘ëª© í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì€ skewed ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ í‚¤ì— ì„ì˜ì˜ ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```js
from pyspark.sql.functions import monotonically_increasing_id, expr
df.withColumn("salted_key",
    expr("concat(name, '_', (monotonically_increasing_id() % 10))")
).groupBy("salted_key").count().select(sum("count")).show()
```

```js
# ê²°ê³¼
+----------+
|sum(count)|
+----------+
|         3|
+----------+
```

- ë°ì´í„° ë¡œë”©ì˜ ê· í˜•ì„ ì–´ë–»ê²Œ ë§ì¶œê¹Œìš”?

<div class="content-ad"></div>

```js
from pyspark.sql.functions import monotonically_increasing_id, expr
df.withColumn("salted_key",
    expr("concat(name, '_', (monotonically_increasing_id() % 10))")
).groupBy("salted_key").count().show()
```

```js
# ê²°ê³¼
+----------+-----+
|salted_key|count|
+----------+-----+
|   James_6|    1|
|   James_4|    1|
|   James_0|    1|
+----------+-----+
```

# ìµœì í™” III: Shufflingì„ ìµœì†Œí™”í•˜ëŠ” ì „ëµ

```js
### Shuffling ìµœì†Œí™” ì „ëµ
- **Broadcast ë³€ìˆ˜ ì‚¬ìš©**
  - ë°ì´í„°ì…‹ì´ ì‘ì€ ê²½ìš° Shufflingì„ í”¼í•˜ê¸° ìœ„í•´ ëª¨ë“  ë…¸ë“œì— ë¸Œë¡œë“œìºìŠ¤íŠ¸í•©ë‹ˆë‹¤.
- **íŒŒí‹°ì…˜ íŠœë‹**
  - ì‘ì—… ë° ë°ì´í„° ê·œëª¨ì— ë§ê²Œ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
- **ë³€í™˜ ìµœì í™”**
  - Shufflingì„ í•„ìš”ë¡œ í•˜ëŠ” ë„“ì€ ë³€í™˜ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì‘ì—…ì„ ê³„íší•©ë‹ˆë‹¤.
```

<div class="content-ad"></div>

## ì„ê¸°

- ì„ê¸°ëŠ” ë°ì´í„°ê°€ ì„œë¡œ ë‹¤ë¥¸ íŒŒí‹°ì…˜ì— ì¬ë¶„ë°°ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
- ì´ëŠ” ë°ì´í„°ë¥¼ ì‹¤í–‰ì ê°„ì´ë‚˜ ì‹¬ì§€ì–´ ê¸°ê³„ ê°„ì— ì´ë™í•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤.
- ë„¤íŠ¸ì›Œí¬ ë° ë””ìŠ¤í¬ I/O ì¸¡ë©´ì—ì„œ ê°€ì¥ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

## ì„ê¸°ì˜ ëª©ì 

- ë°ì´í„° ì¬ë¶„ë°°: ì¡°ì¸, ê·¸ë£¹í™”, ì§‘ê³„ ë° ì¬ë¶„í• ê³¼ ê°™ì€ ë„“ì€ ë³€í™˜ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.
- ë¶€í•˜ ë¶„ì‚°: í´ëŸ¬ìŠ¤í„° ì „ì²´ì— ë°ì´í„°ì™€ ì‘ì—… ë¶€ë‹´ì„ ê· ë“±í•˜ê²Œ ë¶„ë°°í•©ë‹ˆë‹¤.
- ë™ì‹œì„±: ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê°•í™”í•˜ê³  ë¦¬ì†ŒìŠ¤ í™œìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.
- ë°ì´í„° ì§€ì—­ì„± ìµœì í™”: ë°ì´í„°ê°€ ì²˜ë¦¬ë  ìœ„ì¹˜ì— ê°€ê¹Œì´ ì´ë™í•˜ë„ë¡ í•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ì„ ì¤„ì…ë‹ˆë‹¤.

<div class="content-ad"></div>

## ì…”í”Œë§ì˜ ê³ ë¯¼ê±°ë¦¬

- ìì› ì†Œëª¨ê°€ ë§ìŒ: ìƒë‹¹í•œ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ê³¼ ë””ìŠ¤í¬ I/Oë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì§€ì—° ì‹œê°„ ì¦ê°€: íŠ¹íˆ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì…‹ì¸ ê²½ìš° ì²˜ë¦¬ ì‹œê°„ì´ ìƒë‹¹íˆ ì¦ê°€í•©ë‹ˆë‹¤.
- ë³‘ëª© í˜„ìƒ ë°œìƒ ê°€ëŠ¥ì„±: ì ì ˆíˆ ê´€ë¦¬ë˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ëŠë¦¬ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 1. ì‘ì€ DataFrameê³¼ í° DataFrameì„ ì¡°ì¸í•  ë•Œ ë°ì´í„° ì…”í”Œë§ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```js
from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast

# ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™”
spark = SparkSession.builder.appName("ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ ì˜ˆì œ").getOrCreate()
# ì§ì›ìš© í° DataFrame ìƒì„±
data_employees = [(1, "John", 101),
                  (2, "Jane", 102),
                  (3, "Joe", 103),
                  (4, "Jill", 101),
                  # ë” ë§ì€ ë ˆì½”ë“œê°€ ìˆë‹¤ê³  ê°€ì •
                  ]
columns_employees = ["emp_id", "name", "dept_id"]
df_employees = spark.createDataFrame(data_employees, columns_employees)
# ë¶€ì„œìš© ì‘ì€ DataFrame ìƒì„±
data_departments = [(101, "ì¸ì‚¬"),
                    (102, "ë§ˆì¼€íŒ…"),
                    (103, "ê¸ˆìœµ"),
                    (104, "IT"),
                    (105, "ì§€ì›")
                    ]
columns_departments = ["dept_id", "dept_name"]
df_departments = spark.createDataFrame(data_departments, columns_departments)
# ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ ìˆ˜í–‰
df_joined = df_employees.join(broadcast(df_departments), "dept_id")
df_joined.show()
```

<div class="content-ad"></div>

```js
#ì¶œë ¥
+-------+------+----+---------+
|dept_id|emp_id|  ì´ë¦„|dept_name|
+-------+------+-----+--------+
|    101|     1| ì¡´|       ì¸ì‚¬|
|    102|     2| ì œì¸|     ë§ˆì¼€íŒ…|
|    103|     3| ì¡°|     ê¸ˆìœµ|
|    101|     4| ì§ˆ|       ì¸ì‚¬|
+-------+------+-----+--------+
```

- ì§ì› â€” ì§ì› ì„¸ë¶€ ì •ë³´ë¥¼ ë‹´ì€ ì‘ì€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
- ë¶€ì„œ â€” ë¶€ì„œ ì„¸ë¶€ ì •ë³´ë¥¼ ë‹´ì€ í° ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

ë‘ ë°ì´í„°ì…‹ì„ ë¶€ì„œ IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸í•˜ë˜, ë¶€ì„œ ë°ì´í„°ì…‹ì„ í¬ê²Œ ì„ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

## 2. íŒŒí‹°ì…˜ ì¡°ì •: ë³‘ë ¬ì„± ì¦ê°€ë¥¼ ìœ„í•œ ë‹¤ì‹œ ë¶„í• 

<div class="content-ad"></div>

- ë°ì´í„° í”„ë ˆì„ì˜ íŒŒí‹°ì…˜ì„ ì¬ë¶„í• í•˜ì—¬ ë³‘ë ¬ì„±ì„ ë†’ì´ê±°ë‚˜ ì…”í”Œ ë¹„ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ ì „ì²´ ì…”í”Œì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
# ë³‘ë ¬ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ì¬ë¶„í•  ì˜ˆì œ
df = spark.createDataFrame([
  (1, 'foo'), (2, 'bar'), (3, 'baz'), (4, 'qux')
], ["id", "value"])
df_repartitioned = df.repartition(10)  # íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€
```

## 3. íŒŒí‹°ì…˜ íŠœë‹: íŒŒí‹°ì…˜ ê°ì†Œë¥¼ ìœ„í•œ Coalesce

- ì „ì²´ ì…”í”Œ í”¼í•˜ê¸°: coalesceëŠ” ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ í•„í„°ë§í•œ í›„ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì´ê³  ì‹¶ì„ ë•Œ ì…”í”Œ ë¹„ìš©ì„ í”¼í•´ì•¼ í•  ë•Œ ìµœì ì…ë‹ˆë‹¤.
- ì „í˜•ì ì¸ ì‚¬ìš© ì‚¬ë¡€: ë§ì€ íŒŒí‹°ì…˜ì´ ë¶€ë¶„ì ìœ¼ë¡œ ì±„ì›Œì§€ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ìƒíƒœë¡œ ë‚¨ì„ ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ DataFrameì„ í•„í„°ë§í•œ í›„ ì‚¬ìš©ë©ë‹ˆë‹¤. coalesceëŠ” ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì´ê³  ë¹„ìš© íš¨ìœ¨ì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

<div class="content-ad"></div>

```js
# ëŒ€ê·œëª¨ ì…”í”Œë§ ì—†ì´ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì½”ì–´ìŠ¤ ì˜ˆì‹œ
df_filtered = df.filter("id > 1")
df_coalesced = df_filtered.coalesce(2)  # íŒŒí‹°ì…˜ ìˆ˜ ì¤„ì´ê¸°

```

## 4. ë³€í™˜ ìµœì í™”ë¥¼ í†µí•´ ë°ì´í„° ì…”í”Œë§ ìµœì†Œí™”í•˜ê¸°

- ìµœì í™”ëœ ë³€í™˜ì„ í†µí•´ Apache Sparkì—ì„œ ì…”í”Œë§ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ì¸¡ë©´ì…ë‹ˆë‹¤.
- ë³€í™˜ ìµœì í™”ëŠ” ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ì„ êµ¬ì¡°í™”í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì „ì²´ì—ì„œ ë¶ˆí•„ìš”í•œ ë°ì´í„° ì´ë™ì„ ì¤„ì´ëŠ” ê²ƒì„ í¬í•¨í•˜ë©°, ì´ëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ë§ì´ ì‚¬ìš©í•˜ê³  ì‹¤í–‰ ì†ë„ë¥¼ ëŠ¦ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ë¥¼ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ëª‡ ê°€ì§€ ì „ëµê³¼ ì½”ë“œ ì˜ˆì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

## 4â€“1. ì¼ì° í•„í„°ë§í•˜ê¸°

<div class="content-ad"></div>

- ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì¡°ì¸ ë˜ëŠ” ì§‘ê³„ì™€ ê°™ì€ í›„ì† ì‘ì—…ì—ì„œ ë‚˜ì¤‘ì— ì„ì–´ì•¼ í•˜ëŠ” ë°ì´í„° ì–‘ì„ ì¤„ì´ê¸° ìœ„í•´ í•„í„°ë¥¼ ê°€ëŠ¥í•œ í•œ ë¹¨ë¦¬ ì ìš©í•˜ì„¸ìš”.

```python
from pyspark.sql import SparkSession

# ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™”
spark = SparkSession.builder.appName("Shuffling ìµœì†Œí™”").getOrCreate()

# DataFrame ìƒì„±
data = [("John", "ê¸ˆìœµ", 3000), ("Jane", "ë§ˆì¼€íŒ…", 4000), ("Joe", "ë§ˆì¼€íŒ…", 2800), ("Jill", "ê¸ˆìœµ", 3900)]
columns = ["ì´ë¦„", "ë¶€ì„œ", "ì—°ë´‰"]
df = spark.createDataFrame(data, schema=columns)

# ë„“ì€ ë³€í™˜ ì „ì— ë¯¸ë¦¬ í•„í„°ë§
filtered_df = df.filter(df["ì—°ë´‰"] > 3000)

# ì´ì œ ì§‘ê³„ ìˆ˜í–‰
aggregated_df = filtered_df.groupBy("ë¶€ì„œ").avg("ì—°ë´‰")
aggregated_df.show()
```

```python
# ì¶œë ¥
+----------+-----------+
|ë¶€ì„œ      |avg(ì—°ë´‰)  |
+----------+-----------+
| ë§ˆì¼€íŒ…  |     4000.0|
| ê¸ˆìœµ    |     3900.0|
+----------+-----------+
```

## 4-2. ê°€ëŠ¥í•œ ê²½ìš° RDD/ë„“ì€ ë³€í™˜ ì‚¬ìš©í•˜ê¸°

<div class="content-ad"></div>

- ì¢ì€ ë³€í™˜, ì˜ˆë¥¼ ë“¤ì–´ `map` ë° `filter`ì™€ ê°™ì€ ì‘ì—…ì€ ê°œë³„ íŒŒí‹°ì…˜ì—ì„œ ì‘ë™í•˜ë©° ë°ì´í„° ì…”í”Œì„ í•„ìš”ë¡œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ ê²½ìš° ì´ëŸ¬í•œ ì‘ì—…ì„ ë„“ì€ ë³€í™˜ ëŒ€ì‹  ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.

```js
# ì…”í”Œì„ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ìƒˆë¡œìš´ ì—´ì„ ë§Œë“¤ê¸° ìœ„í•´ mapì„ ì‚¬ìš©í•©ë‹ˆë‹¤
rdd = df.rdd.map(lambda x: (x.Name, x.Salary * 1.1))
updated_salaries_df = spark.createDataFrame(
  rdd, schema=["Name", "UpdatedSalary"]
)
updated_salaries_df.show()
```

```js
# ê²°ê³¼
+----+------------------+
|Name|     UpdatedSalary|
+----+------------------+
|John|3300.0000000000005|
|Jane|            4400.0|
| Joe|3080.0000000000005|
|Jill|            4290.0|
+----+------------------+
```

## 4-3. Boardcasting joinìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ì…”í”Œì„ í”¼í•˜ì„¸ìš”

<div class="content-ad"></div>

- ì¡°ì¸ ì‘ì—…ì‹œ, í•œ ë°ì´í„°ì…‹ì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ë³´ë‹¤ í˜„ì €íˆ ì‘ì„ ë•Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ì„ ì‚¬ìš©í•˜ì—¬ í° ë°ì´í„°ì…‹ì„ ì…”í”Œë§í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.

```js
from pyspark.sql.functions import broadcast
# df_smallì´ df_largeë³´ë‹¤ í›¨ì”¬ ì‘ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤
df_small = spark.createDataFrame(
  [(1, "HR"), (2, "ë§ˆì¼€íŒ…")], ["id", "ë¶€ì„œ"]
)
df_large = spark.createDataFrame(
  [(1, "ì¡´"), (2, "ì œì¸"), (1, "ì¡°"), (2, "ì§ˆ")],
  ["ë¶€ì„œID", "ì´ë¦„"]
)
# ì¡°ì¸ ìµœì í™”ë¥¼ ìœ„í•´ ì‘ì€ DataFrameì„ ë¸Œë¡œë“œìºìŠ¤íŠ¸í•©ë‹ˆë‹¤
optimized_join_df = df_large.join(broadcast(df_small), df_large.ë¶€ì„œID == df_small.id)
optimized_join_df.show()
```

```js
# ê²°ê³¼

+------+----+---+------+
|ë¶€ì„œID|ì´ë¦„| id|  ë¶€ì„œ|
+------+----+---+------+
|     1| ì¡´|  1|   HR|
|     2|ì œì¸|  2|ë§ˆì¼€íŒ…|
|     1| ì¡°|  1|   HR|
|     2| ì§ˆ|  2|ë§ˆì¼€íŒ…|
+------+----+---+------+
```

## 4-4. ì „ëµì ìœ¼ë¡œ íŒŒí‹°ì…˜ ë‚˜ëˆ„ê¸°

<div class="content-ad"></div>

- ë§Œì•½ ë„“ì€ ë³€í™˜ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ë©´, ë‚˜ì¤‘ì— ì¡°ì¸ ë˜ëŠ” ì§‘ê³„í•  í‚¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”. ì´ ì „ëµì„ ì‚¬ìš©í•˜ë©´ ë™ì¼í•œ í‚¤ë¥¼ ê°€ì§„ í–‰ì„ ë™ì¼í•œ íŒŒí‹°ì…˜ì— í•¨ê»˜ ë‘ì–´ ì…”í”Œë§ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
# ì…”í”Œë§ ìµœì†Œí™”ë¥¼ ìœ„í•´ ì§‘ê³„ ì „ì— íŒŒí‹°ì…˜ ì¬ë¶„ë°°
repartitioned_df = df.repartition("Department")
aggregated_df = repartitioned_df.groupBy("Department").avg("Salary")
aggregated_df.show()
```

```js
# ê²°ê³¼
+----------+-----------+
|Department|avg(Salary)|
+----------+-----------+
|   Finance|     3450.0|
| Marketing|     3400.0|
+----------+-----------+
```

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì„¸ë¶€ ì¡°ì •

<div class="content-ad"></div>

## 1. ë©”ëª¨ë¦¬ ê´€ë¦¬

```js
spark.conf.set(â€œspark.executor.memoryâ€, â€œ4gâ€)
spark.conf.set(â€œspark.driver.memoryâ€, â€œ2gâ€)
```

## 2. ì‘ì—… ë° ìŠ¤í…Œì´ì§€ ëª¨ë‹ˆí„°ë§

- Spark UIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ìš© í”„ë¡œê·¸ë¨ ë‚´ì˜ ì‘ì—… ë° ìŠ¤í…Œì´ì§€ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.
- Spark UIì— ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´ ë‹¤ìŒìœ¼ë¡œ ì´ë™í•˜ì‹­ì‹œì˜¤: http://[your-spark-driver-host]:4040
- Executor ë©”íŠ¸ë¦­ ë¶„ì„: ê° executorì˜ ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©, ë””ìŠ¤í¬ ìŠ¤í”¼ë¦´ ë° ê°€ë¹„ì§€ ìˆ˜ì§‘ì— ëŒ€í•œ í†µì°°ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

```js
# ìì„¸í•œ ì‹¤í–‰ì ì§€í‘œë¥¼ ìˆ˜ì§‘í•˜ë„ë¡ Sparkë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤
spark.conf.set("spark.executor.metrics.pollingInterval", "5000")
```

## 3. SQL ì„±ëŠ¥ íŠœë‹

- SQL ì‹¤í–‰ ê³„íšì„ ì´í•´í•˜ê³  ìµœì í™”í•˜ê¸° ìœ„í•´ `EXPLAIN` ê³„íšì„ í™œìš©í•˜ì„¸ìš”.

```js
df.explain(â€œformattedâ€)
```

<div class="content-ad"></div>

```js
== ë¬¼ë¦¬ì ì¸ ê³„íš ==
* ê¸°ì¡´ RDD ìŠ¤ìº” (1)


(1) ê¸°ì¡´ RDD ìŠ¤ìº” [ì½”ë“œ ìƒì„± ID : 1]
ì¶œë ¥ [3]: [ì´ë¦„ #4628, ë¶€ì„œ #4629, ê¸‰ì—¬ #4630L]
ì¸ìˆ˜: [ì´ë¦„ #4628, ë¶€ì„œ #4629, ê¸‰ì—¬ #4630L],
 applySchemaToPythonRDDì— ìˆëŠ” MapPartitionsRDD[693]ì—ì„œ
                       at <ì•Œ ìˆ˜ ì—†ìŒ>:0, ExistingRDD, UnknownPartitioning(0)
```

## 4. ë™ì  í• ë‹¹

- ì›Œí¬ë¡œë“œì— ë”°ë¼ ìŠ¤íŒŒí¬ê°€ ì‹¤í–‰ì ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ ë™ì  í• ë‹¹ì„ í™œì„±í™”í•©ë‹ˆë‹¤.

```js
spark.conf.set("spark.dynamicAllocation.enabled", "true");
spark.conf.set("spark.dynamicAllocation.minExecutors", "1");
spark.conf.set("spark.dynamicAllocation.maxExecutors", "20");
spark.conf.set("spark.dynamicAllocation.executorIdleTimeout", "60s");
spark.conf.set("spark.shuffle.service.enabled", "true");
```

<div class="content-ad"></div>

## 5. ë°ì´í„° ì§€ì—­ì„±

- ì €ì¥ ë° ì²˜ë¦¬ ì¥ì¹˜ ê°„ì— ë°ì´í„°ê°€ ì´ë™í•´ì•¼ í•˜ëŠ” ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ì—¬ ë°ì´í„° ì§€ì—­ì„±ì„ ìµœì í™”í•©ë‹ˆë‹¤.

```js
spark.conf.set("spark.locality.wait", "300ms");
```

## 6. Garbage Collection Tuning

<div class="content-ad"></div>

```js
- ê°€ë¹„ì§€ ì»¬ë ‰í„° ì„¤ì •ì„ ì¡°ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìµœì í™”í•˜ê³  ì¼ì‹œ ì¤‘ì§€ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë” ë‚˜ì€ ì§€ì—° ì‹œê°„ì„ ìœ„í•´ G1GC ì‚¬ìš©
spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC")
# ì§§ì€ ì¼ì‹œ ì¤‘ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì ì¸ GC ì„¤ì • êµ¬ì„±
spark.conf.set("spark.executor.extraJavaOptions", "-XX:MaxGCPauseMillis=100")

## 7. ë°ì´í„° ì§ë ¬í™” ì„¸ë¶€ ì¡°ì •

- ë°ì´í„° ì§ë ¬í™”ëŠ” ë¶„ì‚° ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. SparkëŠ” ë‘ ê°€ì§€ ì§ë ¬í™” ë„êµ¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:
```

<div class="content-ad"></div>

```js
# ë” ë‚˜ì€ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ìœ„í•´ Kryo ì§ë ¬í™” í”„ë¡œê·¸ë¨ ì‚¬ìš©
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "true")

# Kryoì™€ ì‚¬ìš©ì ì •ì˜ í´ë˜ìŠ¤ ë“±ë¡
class MyClass:
    def __init__(self, name, id):
        self.name = name
        self.id = id
spark.sparkContext.getConf().registerKryoClasses([MyClass])
```

## 8. ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ìµœì í™”

- ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì€ íŠ¹íˆ ëŒ€ê·œëª¨ ë°°í¬ì—ì„œ ì„±ëŠ¥ì— ì¤‘ëŒ€í•œ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
# ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ì¡°ì •í•˜ì—¬ ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë¶ˆí•„ìš”í•œ ì‘ì—… ì‹¤íŒ¨ë¥¼ í”¼í•˜ì‹­ì‹œì˜¤
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.core.connection.ack.wait.timeout", "600s")
```

<div class="content-ad"></div>

## 9. ê³ ê¸‰ Spark SQL íŠœë‹

- Catalyst ì˜µí‹°ë§ˆì´ì € ë° Tungsten ì‹¤í–‰ ì—”ì§„ì„ í™œìš©í•˜ë©´ Spark SQLì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```js
# ì§ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì²´ ë‹¨ê³„ ì½”ë“œ ìƒì„± í™œì„±í™”
spark.conf.set("spark.sql.codegen.wholeStage", "true")

# ì¡°ì¸ ìµœì í™”ì— ìœ ìš©í•œ í…Œì´ë¸” ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìµœëŒ€ ë°”ì´íŠ¸ ìˆ˜ ì¦ê°€
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")  # 10 MB
```

## 10. ë°ì´í„° íŒŒí‹°ì…”ë‹ ìµœì í™”

<div class="content-ad"></div>

ë°ì´í„° ë¶„ë°°ë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•˜ì—¬ ì¿¼ë¦¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ì…”í”Œ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì¼ ìˆ˜ ìˆì–´ìš”:

```js
# ë°ì´í„° í¬ê¸° ë° ì‘ì—…ì„ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ë™ìœ¼ë¡œ ì…”í”Œ íŒŒí‹°ì…˜ ìˆ˜ ì„¤ì •

spark.conf.set("spark.sql.shuffle.partitions", "200")
# í´ëŸ¬ìŠ¤í„° í¬ê¸° ë° ë°ì´í„°ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”
```

## 11. ì ì‘í˜• ì¿¼ë¦¬ ì‹¤í–‰ í™œì„±í™”

- ì ì‘í˜• ì¿¼ë¦¬ ì‹¤í–‰ (AQE)ëŠ” ì‹¤í–‰ ì¤‘ì— ì¿¼ë¦¬ ê³„íšì„ ì¡°ì •í•¨ìœ¼ë¡œì¨ Spark SQL ì¿¼ë¦¬ë¥¼ ë” ë¹ ë¥´ê³  ë°ì´í„° ìŠ¤ì¿  ë° ê¸°íƒ€ ì´ìŠˆì— ë” ê°•ê±´í•˜ê²Œ ë§Œë“œëŠ” ê¸°ëŠ¥ì´ì—ìš”.

<div class="content-ad"></div>

```js
# ì¿¼ë¦¬ ì‹¤í–‰ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” AQEë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤. ì´ëŠ” êµ¬ì„±ì„ ê°„ì†Œí™”í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

- AQEëŠ” ì‹¤ì œ ë°ì´í„°ì— ì ì‘í•´ ì…”í”Œ íŒŒí‹°ì…”ë‹ì„ ì¡°ì •í•˜ê³ , ë¶ˆê· í˜• ì¡°ì¸ì„ ì²˜ë¦¬í•˜ë©°, ì •ë ¬ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 12. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì§€ì •

- ì ì ˆí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ëŠ” ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì‘ì—…ì—ì„œ íŠ¹íˆ íš¨ê³¼ì ì¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ìŠ¤íŒŒì´ì§€ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

# RDD ì €ì¥ì†Œì— ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ ë¶„ìˆ˜ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

spark.conf.set(â€œspark.memory.fractionâ€, â€œ0.6â€)
spark.conf.set(â€œspark.memory.storageFractionâ€, â€œ0.5â€)

ì´ëŸ¬í•œ ì„¤ì •ì€ ì‹¤í–‰ ë©”ëª¨ë¦¬ì™€ ì €ì¥ì†Œ ë©”ëª¨ë¦¬ ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶”ì–´ ì…”í”Œ ë° ìºì‹± ì¤‘ ë””ìŠ¤í¬ ìŠ¤íŒŒì¼ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

# ì½ì–´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤

ì´ ê¸€ì´ ë§ˆìŒì— ë“œì‹œë©´:

<div class="content-ad"></div>

- ğŸ‘ ì—¬ëŸ¬ ë²ˆ ë°•ìˆ˜ë¡œ ì§€ì§€ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”!
- ì´ ì•ˆë‚´ì„œë¥¼ ì¹œêµ¬ë“¤ê³¼ ê³µìœ í•´ë„ ì¢‹ì•„ìš”.
- ì—¬ëŸ¬ë¶„ì˜ í”¼ë“œë°±ì€ ì†Œì¤‘í•©ë‹ˆë‹¤. ì•ìœ¼ë¡œì˜ ê¸€ì— ì˜ê°ì„ ì£¼ê³  ì•ˆë‚´í•´ ì¤ë‹ˆë‹¤.
- ë˜ëŠ” ë©”ì‹œì§€ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”: https://www.linkedin.com/in/kevinchwong
