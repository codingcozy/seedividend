---
title: "친근한 톤으로 번역 GPT-4를 소개합니다"
description: ""
coverImage: "/assets/img/2024-05-15-HelloGPT-4o_0.png"
date: 2024-05-15 03:08
ogImage: 
  url: /assets/img/2024-05-15-HelloGPT-4o_0.png
tag: Tech
originalTitle: "Hello GPT-4o"
link: "https://medium.com/@Code_With_Ssn/hello-gpt-4o-0a3f29f03e79"
---


GPT-4o ("o" for "omni")은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.

# 모델 기능

두 대 GPT-4o가 상호작용하고 노래합니다.

인터뷰 준비요.



안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.


바위 가위 보 게임.

비꼼.

Sal과 Imran Khan과 함께 하는 수학.

둘의 GPT-4가 화음을 이루다.
 

더 궁금한 사항이 있으시면 언제든지 알려주세요!


GPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.

고객 서비스 프로토타입.

GPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.

GPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.



# 모델 평가

전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.

![이미지](/assets/img/2024-05-15-HelloGPT-4o_0.png)