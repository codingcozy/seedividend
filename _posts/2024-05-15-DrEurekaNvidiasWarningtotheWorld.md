---
title: "드라유레카, 세계에 대한 Nvidia의 경고"
description: ""
coverImage: "/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_0.png"
date: 2024-05-15 11:32
ogImage: 
  url: /assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_0.png
tag: Tech
originalTitle: "DrEureka, Nvidia’s Warning to the World"
link: "https://medium.com/@ignacio.de.gregorio.noblejas/dreureka-nvidias-warning-to-the-world-2e1ff4ef47fd"
isUpdated: true
---




지난 주 Nvidia는 AI 로봇이 얼마나 빠르게 발전하고 있는지를 다시 상기시키며, LLMs의 훌륭한 잠재력을 강조했습니다. 이는 교육과정에서 인간의 개입을 최소화하는 데 큰 기회를 제공합니다.

간단히 말해서, 그들은 AI로 훈련된 더 강력한 로봇의 출현을 보여주었습니다.

로봇 AIs를 훈련하는 AIs입니다.

이를 위해 그들은 몇 달 전에만으로는 불가능했던 다양한 복잡도의 시나리오에서 요가 볼 위에 균형을 유지하는 안드로이드를 훈련시켰습니다.



이미지: "/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_0.png"

확실히, 로봇이 이미 매우 복잡한 현실 과제를 아주 빠르게 수행하는 사실은 AI 로보틱스가 멈출 수 없는 추세임을 모두에게 알립니다.

하지만 그들은 어떻게 이것을 성취했을까요?

# 보상의 중요성



먼저, AI 로봇이 어떻게 훈련되는지 궁금한 적이 있나요?

우리는 그들이 주어진 보상을 극대화하는 환경에서 행동을 취할 수 있도록 도와주는 정책을 생성합니다.

예를 들어, 100m를 달리는 모델을 훈련하려면 일어서고 걷고 뛰고 결국은 질주하는 방법을 먼저 배워야 합니다. 모델이 그 작업을 이해할 때까지 '도약'마다 보상을 주면서 훈련시킵니다.

하지만 이것을 어떻게 가르칠까요? 보상을 부여함으로써요.



예를 들어, 모델이 세워지면 점수를 주는 것입니다. 그 반면에 모델이 넘어지면 처벌을 받습니다.

이를 보상 모델링이라고 합니다. 좋은 행동에 보상을, 나쁜 행동에 처벌을 하는 함수를 만드는 것이죠. 이렇게 하면 시간이 지남에 따라 모델은 누적 보상을 최대화하는 몸의 위치와 행동을 결정합니다.

이것은 매우 표준적인 방법이며, Google Deepmind과 같은 최첨단 연구소도 같은 원칙을 사용하여 일어서고, 뛰어들고, 슛을 차고, 심지어 수비를 하는 축구 로봇을 훈련시키고 있습니다.

하지만, 이러한 보상 함수를 정의하는 것은 굉장히 어렵습니다.



인간의 몸을 생각해보자면, 예를 들어 펜 회전이라는 전반적인 결과에 긍정적이거나 부정적인 영향을 미치는 손의 모든 근육과 관절의 움직임이 있습니다.

액션이 매우 복잡한 경우, 인간은 로봇이 그에 따라 행동하도록 돕는 가장 좋은 보상 함수를 작성하는데 어려움을 겪습니다.

이 복잡성을 감지하기 위해, 볼 걸음걸이 작업을 위해 최종적으로 결정된 보상 함수는 다음과 같습니다:

![이미지](/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_1.png)



그리고 이 방정식을 찾는 것은 보이는 대로 어렵습니다. 하지만 여기에 중요한 점이 있습니다: 그 함수는 인간이 아닌 AI가 작성했습니다.

# AI의 보상

몇 달 전, DrEureka 뒤의 연구자들이 Eureka를 출시했습니다. 이는 AI 기반 보상 함수 설계 알고리즘입니다.

아래에서 보듯이, 아이디어는 Large Language Model (GPT-4)을 사용하여 반복적 루프를 사용하여 환경(로봇이 행동할 장면을 설명하는 코드) 및 수행해야할 작업을 주면, 이러한 보상 함수를 생성하여 해당 내용을 시뮬레이션된 환경에서 평가하는 것이었습니다.



시뮬레이션 결과에 따라 피드백이 생성되었고, LLM은 이를 사용하여 품질 기준이 충족될 때까지 새 보상 함수를 생성했습니다.

![image](/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_2.png)

유레카와 팀은 놀라운 결과를 달성했습니다: 로봇을 펜 회전 트릭을 수행하도록 훈련시켰으며, 최고의 CGI 전문가조차 고전할 것입니다.

그 업적은 놀라운 것이었으며, LLM을 반복적으로 사용하여 인간이 설계할 수 없는 복잡한 보상 함수를 만들 수 있음을 증명했습니다. 그리고 거의 인간 개입이 없었습니다.



이제, 그림을 그리듯이 손가락, 관절, 근육 및 관절 점수를 모델링하는 보상 함수를 쓰고 있다고 상상해보세요.

의심의 여지없이, Eureka는 Nvidia가 AI가 AI를 훈련하는 것이 로봇 공학의 미래임을 세계에 입증했습니다.

그러나 안타깝게도 문제가 하나 있었습니다. 만일 이러한 결과를 현실 세계로 가져가고 싶다면 어떻게 해야 할까요?

그럴 때 DrEureka가 등장합니다.



# 개방적인 세상 속 인공지능

로봇 공학에서 가장 어려운 문제는 비용과 불확실성을 다루는 것입니다.

실제로 로봇을 훈련하는 것은 매우 비싸고 실제 제약(마찰, 바람, 온도 등)을 다뤄야 하며 물론 고장의 위험도 있습니다.

오늘 논의하는 경우와 같이, 일부 경우에는 영점(Zero-shot) 모드로 작동합니다. 다시 말해서, 모든 훈련은 시뮬레이션에서 수행되며 실제 세계에서는 최선을 바랍니다.



상상할 수 있는 대로, 이 전환은 매우 중요하며 오류 발생 가능성이 높습니다. 실제 환경에는 높은 불확실성이 포함되어 있습니다. 따라서 로봇을 길에 내려놓기 전에 프로세스에 한 가지 추가 단계를 추가해야 했습니다: 도메인 랜덤화.

## 견고한 로봇 훈련

실제 세계를 생각해보면 불확실성이 가득합니다.

- 온도는 매초 변화합니다.
- 로봇은 울퉁불퉁한 지형에 부딪힐 수 있습니다.
- 바람이 다양한 방향으로 불 수 있습니다.
- 로봇의 관절 운동은 사용으로 인해 덜 부드러워질 수 있습니다.
- 그리고 물체들이 길에 나타날 수 있습니다.



수천 개의 다른 예기치 않은 변화 중 하나입니다.

따라서 성공 확률을 극대화하기 위해 연구자들은 시뮬레이션에서 모델을 훈련시키기 위해 제약 조건(품질이나 마찰력과 같은)에 무작위 변화를 도입하여 다양한 환경 시나리오에서 모델을 강화합니다.

그러나 각 제약 조건에 대해 정의할 값이 무엇인지 결정하는 것은 굉장히 어렵습니다. 그래서 연구자들이 생각한 것은 LLMs가 여기에서도 우리를 도울 수 있을까요?

네, 하지만 문제가 있습니다: 도메인 랜덤화 검색 공간 또는 각 환경 제약 조건이 가질 수 있는 가능한 값의 수는 무한하므로, 이 문제는 LLMs에게 매우 어렵습니다.



이를 완화하기 위해, 그들은 먼저 RAPP (보상 인식 물리 사전)라고 알려진 경량 검색을 수행하여 가능한 제약 값 범위를 물리적으로 가능한 값으로 좁혔습니다 (예: 음의 마찰 값은 불가능합니다).

이는 LLM의 검색 공간을 좁혀 다양한 도메인 조건을 생성하는 데 도움이 됩니다.

RAPP 경계가 설정되면, 그들은 다시 GPT-4를 사용하여 각 제약에 대한 타당한 값을 생성하여 새로운 시나리오 (도메인 무작위화)를 만들었으며, 우리에게 전체 DrEureka 프레임워크를 제공했습니다:

![DrEureka Framework](/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_3.png)



따라서 전체 프로세스는 다음과 같습니다:

- 작업 및 안전 지침이 정의되며 환경 조건도 정의됩니다.
- RAPP를 사용하여 각 조건의 값 범위를 제한합니다. 이는 각 환경 제약 조건의 값 범위를 좁힙니다.
- 그런 다음 LLM은 각 환경에 대한 가능하지만 현실적인 시나리오를 생성합니다(도메인 랜덤화), 로봇을 테스트하기 위한 '새로운' 시뮬레이션 환경 세트를 생성합니다.
- 병렬로 다른 LLM은 각 특정 환경의 조건을 기반으로 보상 함수를 생성합니다.
- 그다음 로봇은 해당 보상을 최대화하는 정책을 사용하여 훈련되어 실제 세계에서 예상대로 행동할 수 있도록 학습됩니다. 품질 기준이 충족될 때까지 세 번째 및 네 번째 단계를 반복하는 피드백이 생성됩니다. 목표는 로봇이 실제 세계에서 예상대로 행동한다는 가능성을 극대화하는 정책을 학습하는 보상 함수를 얻는 것입니다.

결국 로봇은 실제로 투입되었고 결과는 스스로 이야기합니다.

# 로봇 시대



Nvidia가 로보틱스에 대해 매우 많은 것을 건너뛰고 있다고 말하는 것은 저평가이에요.

사실, 이 논문의 연구자 가운데 하나인 Jim Fan은 Nvidia의 랜드마크 프로젝트인 프로젝트 Gr00t를 이끄는 중이에요. 이 프로젝트는 현실 세계 에이전트를 위한 기초 모델을 개발하는 프로젝트입니다.

그러나 여전히 명백한 제약이 있으므로 바닥에 발을 디딘 채로 있어야 해요. 예를 들어, 한 번 도메인 매개변수가 설정되면 해당 환경에서의 훈련 실행 동안 고정되는 한계가 있어요.

다시 말해, 마찰력이 일정하게 유지된 채로 남아있는 것처럼, 이는 실제 세계에서는 사실이 아니라는 것이에요. 왜냐하면 새로운 표면마다 변화하기 때문이죠.



그 말은 그들의 결과가 인공지능 로봇 분야의 발전 속도에 대한 경고 신호로 작용한다는 것을 명확히 보여줍니다. 이 분야는 여전히 빠르게 가속화되고 있는 것으로 보입니다.